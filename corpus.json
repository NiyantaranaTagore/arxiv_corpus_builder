[
    {
        "arxiv_id": "2512.03026v1",
        "title": "The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models",
        "abstract": "The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.",
        "authors": [
            "Saeid Jamshidi",
            "Kawser Wazed Nafi",
            "Arghavan Moradi Dakhel",
            "Negar Shahabi",
            "Foutse Khomh"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T18:52:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.03026v1"
    },
    {
        "arxiv_id": "2512.03025v1",
        "title": "LORE: A Large Generative Model for Search Relevance",
        "abstract": "Achievement. We introduce LORE, a systematic framework for Large Generative Model-based relevance in e-commerce search. Deployed and iterated over three years, LORE achieves a cumulative +27\\% improvement in online GoodRate metrics. This report shares the valuable experience gained throughout its development lifecycle, spanning data, features, training, evaluation, and deployment. Insight. While existing works apply Chain-of-Thought (CoT) to enhance relevance, they often hit a performance ceiling. We argue this stems from treating relevance as a monolithic task, lacking principled deconstruction. Our key insight is that relevance comprises distinct capabilities: knowledge and reasoning, multi-modal matching, and rule adherence. We contend that a qualitative-driven decomposition is essential for breaking through current performance bottlenecks. Contributions. LORE provides a complete blueprint for the LLM relevance lifecycle. Key contributions include: (1) A two-stage training paradigm combining progressive CoT synthesis via SFT with human preference alignment via RL. (2) A comprehensive benchmark, RAIR, designed to evaluate these core capabilities. (3) A query frequency-stratified deployment strategy that efficiently transfers offline LLM capabilities to the online system. LORE serves as both a practical solution and a methodological reference for other vertical domains.",
        "authors": [
            "Chenji Lu",
            "Zhuo Chen",
            "Hui Zhao",
            "Zhiyuan Zeng",
            "Gang Zhao",
            "Junjie Ren",
            "Ruicong Xu",
            "Haoran Li",
            "Songyan Liu",
            "Pengjie Wang",
            "Jian Xu",
            "Bo Zheng"
        ],
        "category": "cs.IR",
        "submitted_date": "2025-12-02T18:50:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.03025v1"
    },
    {
        "arxiv_id": "2512.02987v1",
        "title": "Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic",
        "abstract": "Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.",
        "authors": [
            "Muyu Pan",
            "Dheeraj Kodakandla",
            "Mahfuza Farooque"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T18:03:06+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02987v1"
    },
    {
        "arxiv_id": "2512.02973v1",
        "title": "Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities",
        "abstract": "While Multimodal Large Language Models (MLLMs) show remarkable capabilities, their safety alignments are susceptible to jailbreak attacks. Existing attack methods typically focus on text-image interplay, treating the visual modality as a secondary prompt. This approach underutilizes the unique potential of images to carry complex, contextual information. To address this gap, we propose a new image-centric attack method, Contextual Image Attack (CIA), which employs a multi-agent system to subtly embeds harmful queries into seemingly benign visual contexts using four distinct visualization strategies. To further enhance the attack's efficacy, the system incorporate contextual element enhancement and automatic toxicity obfuscation techniques. Experimental results on the MMSafetyBench-tiny dataset show that CIA achieves high toxicity scores of 4.73 and 4.83 against the GPT-4o and Qwen2.5-VL-72B models, respectively, with Attack Success Rates (ASR) reaching 86.31\\% and 91.07\\%. Our method significantly outperforms prior work, demonstrating that the visual modality itself is a potent vector for jailbreaking advanced MLLMs.",
        "authors": [
            "Yuan Xiong",
            "Ziqi Miao",
            "Lijun Li",
            "Chen Qian",
            "Jie Li",
            "Jing Shao"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-12-02T17:51:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02973v1"
    },
    {
        "arxiv_id": "2512.02924v1",
        "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
        "abstract": "While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.",
        "authors": [
            "Wei Chen",
            "Liangmin Wu",
            "Yunhai Hu",
            "Zhiyuan Li",
            "Zhiyuan Cheng",
            "Yicheng Qian",
            "Lingyue Zhu",
            "Zhipeng Hu",
            "Luoyi Liang",
            "Qiang Tang",
            "Zhen Liu",
            "Han Yang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T16:45:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02924v1"
    },
    {
        "arxiv_id": "2512.02914v1",
        "title": "Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning",
        "abstract": "Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.",
        "authors": [
            "Zhonghao He",
            "Tianyi Qiu",
            "Hirokazu Shirado",
            "Maarten Sap"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-12-02T16:34:05+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02914v1"
    },
    {
        "arxiv_id": "2512.02892v1",
        "title": "Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules",
        "abstract": "Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves $3.8$-$4.0\\times$ speedups while retaining $99.8$-$100\\%$ of the baseline score on average. On base models, SchED yields consistent speedup gains with $99.1$-$100\\%$ performance retention, with up to $2.34\\times$ under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, $\u03b3{=}4$), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.",
        "authors": [
            "Amr Mohamed",
            "Yang Zhang",
            "Michalis Vazirgiannis",
            "Guokan Shang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T16:01:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02892v1"
    },
    {
        "arxiv_id": "2512.02882v1",
        "title": "OptPO: Optimal Rollout Allocation for Test-time Policy Optimization",
        "abstract": "Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.",
        "authors": [
            "Youkang Wang",
            "Jian Wang",
            "Rubing Chen",
            "Tianyi Zeng",
            "Xiao-Yong Wei",
            "Qing Li"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-12-02T15:38:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02882v1"
    },
    {
        "arxiv_id": "2512.02874v1",
        "title": "Think in Parallel, Answer as One: Logit Averaging for Open-Ended Reasoning",
        "abstract": "Majority voting has proven effective for close-ended question answering by aggregating parallel reasoning traces. However, it is not directly applicable to open-ended reasoning, such as code generation and web-based deep research, where a \"majority\" over complete solutions is ill-defined. We introduce ThinkMerge, a training-free, plug-and-play decoding strategy that runs K parallel reasoning traces and averages their next-token logits at synchronization points to produce a single coherent output. ThinkMerge integrates seamlessly with vLLM/SGLang and remains compatible with standard decoding techniques such as Top-p/Top-k. Empirically, it matches or surpasses majority voting on AIME and GPQA, while delivering consistent gains on open-ended coding tasks: on LiveCodeBench (hard), pass@1 improves by +8.28% for DeepCoder-14B-Preview and +7.58% for Qwen3-8B. Beyond code, we further show that ThinkMerge improves web-based deep-research agents (e.g., WebSailor-7B/32B) across GAIA, BrowseComp-en/zh, and XbenchDeepSearch. These results demonstrate that parallel test-time scaling can benefit open-ended reasoning without relying on voting over complete outputs.",
        "authors": [
            "Haonan Wang",
            "Chao Du",
            "Kenji Kawaguchi",
            "Tianyu Pang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T15:35:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02874v1"
    },
    {
        "arxiv_id": "2512.02845v1",
        "title": "Bangla Hate Speech Classification with Fine-tuned Transformer Models",
        "abstract": "Hate speech recognition in low-resource languages remains a difficult problem due to insufficient datasets, orthographic heterogeneity, and linguistic variety. Bangla is spoken by more than 230 million people of Bangladesh and India (West Bengal). Despite the growing need for automated moderation on social media platforms, Bangla is significantly under-represented in computational resources. In this work, we study Subtask 1A and Subtask 1B of the BLP 2025 Shared Task on hate speech detection. We reproduce the official baselines (e.g., Majority, Random, Support Vector Machine) and also produce and consider Logistic Regression, Random Forest, and Decision Tree as baseline methods. We also utilized transformer-based models such as DistilBERT, BanglaBERT, m-BERT, and XLM-RoBERTa for hate speech classification. All the transformer-based models outperformed baseline methods for the subtasks, except for DistilBERT. Among the transformer-based models, BanglaBERT produces the best performance for both subtasks. Despite being smaller in size, BanglaBERT outperforms both m-BERT and XLM-RoBERTa, which suggests language-specific pre-training is very important. Our results highlight the potential and need for pre-trained language models for the low-resource Bangla language.",
        "authors": [
            "Yalda Keivan Jafari",
            "Krishno Dey"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T14:56:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02845v1"
    },
    {
        "arxiv_id": "2512.02841v1",
        "title": "Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages",
        "abstract": "System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.",
        "authors": [
            "Lechen Zhang",
            "Yusheng Zhou",
            "Tolga Ergen",
            "Lajanugen Logeswaran",
            "Moontae Lee",
            "David Jurgens"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T14:54:54+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02841v1"
    },
    {
        "arxiv_id": "2512.02840v1",
        "title": "promptolution: A Unified, Modular Framework for Prompt Optimization",
        "abstract": "Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers show its effectiveness, practical adoption is hindered as existing implementations are often tied to unmaintained and isolated research codebases. To address this, we introduce promptolution, a unified and modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers while remaining agnostic to the underlying LLM implementation.",
        "authors": [
            "Tom Zehle",
            "Timo Hei\u00df",
            "Moritz Schlager",
            "Matthias A\u00dfenmacher",
            "Matthias Feurer"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T14:53:23+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02840v1"
    },
    {
        "arxiv_id": "2512.02835v1",
        "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
        "abstract": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .",
        "authors": [
            "Yifan Li",
            "Yingda Yin",
            "Lingting Zhu",
            "Weikai Chen",
            "Shengju Qian",
            "Xin Wang",
            "Yanwei Fu"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-12-02T14:44:12+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02835v1"
    },
    {
        "arxiv_id": "2512.02817v1",
        "title": "BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion",
        "abstract": "The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present \\textbf{BOOM}, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\\footnote{All released code and models are licensed under the MIT License.",
        "authors": [
            "Sai Koneru",
            "Fabian Retkowski",
            "Christian Huber",
            "Lukas Hilgert",
            "Seymanur Akti",
            "Enes Yavuz Ugan",
            "Alexander Waibel",
            "Jan Niehues"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T14:27:26+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02817v1"
    },
    {
        "arxiv_id": "2512.02816v1",
        "title": "A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models",
        "abstract": "The emergence of Large Language Models (LLMs) within the Traditional Chinese Medicine (TCM) domain presents an urgent need to assess their clinical application capabilities. However, such evaluations are challenged by the individualized, holistic, and diverse nature of TCM's \"Syndrome Differentiation and Treatment\" (SDT). Existing benchmarks are confined to knowledge-based question-answering or the accuracy of syndrome differentiation, often neglecting assessment of treatment decision-making. Here, we propose a comprehensive, clinical case-based benchmark spearheaded by TCM experts, and a specialized reward model employed to quantify prescription-syndrome congruence. Data annotation follows a rigorous pipeline. This benchmark, designated TCM-BEST4SDT, encompasses four tasks, including TCM Basic Knowledge, Medical Ethics, LLM Content Safety, and SDT. The evaluation framework integrates three mechanisms, namely selected-response evaluation, judge model evaluation, and reward model evaluation. The effectiveness of TCM-BEST4SDT was corroborated through experiments on 15 mainstream LLMs, spanning both general and TCM domains. To foster the development of intelligent TCM research, TCM-BEST4SDT is now publicly available.",
        "authors": [
            "Kunning Li",
            "Jianbin Guo",
            "Zhaoyang Shang",
            "Yiqing Liu",
            "Hongmin Du",
            "Lingling Liu",
            "Yuping Zhao",
            "Lifeng Dong"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T14:26:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02816v1"
    },
    {
        "arxiv_id": "2512.02807v1",
        "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
        "abstract": "Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.",
        "authors": [
            "Yixuan Tang",
            "Yi Yang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T14:21:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02807v1"
    },
    {
        "arxiv_id": "2512.02799v1",
        "title": "TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages",
        "abstract": "Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented framework that unifies corpus-based extraction, cross lingual mapping, and retrieval augmented generation (RAG) driven lexical refinement to systematically expand sentiment lexicons for low-resource languages. Using the enriched lexicon, the performance of two prominent African pretrained language models (AfroXLMR and AfriBERTa) is evaluated across multiple case studies. Results demonstrate that AfroXLMR delivers superior performance, achieving F1-scores above 80% for isiXhosa and isiZulu and exhibiting strong cross-lingual stability. Although AfriBERTa lacks pre-training on these target languages, it still achieves reliable F1-scores around 64%, validating its utility in computationally constrained settings. Both models outperform traditional machine learning baselines, and ensemble analyses further enhance precision and robustness. The findings establish TriLex as a scalable and effective framework for multilingual sentiment lexicon expansion and sentiment modeling in low-resource South African languages.",
        "authors": [
            "Mike Nkongolo",
            "Hilton Vorster",
            "Josh Warren",
            "Trevor Naick",
            "Deandre Vanmali",
            "Masana Mashapha",
            "Luke Brand",
            "Alyssa Fernandes",
            "Janco Calitz",
            "Sibusiso Makhoba"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T14:16:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02799v1"
    },
    {
        "arxiv_id": "2512.02791v1",
        "title": "Making Dialogue Grounding Data Rich: A Three-Tier Data Synthesis Framework for Generalized Referring Expression Comprehension",
        "abstract": "Dialogue-Based Generalized Referring Expressions Comprehension (GREC) requires models to ground the expression and unlimited targets in complex visual scenes while resolving coreference across a long dialogue context. However, existing systems struggle under distribution shift between training and evaluation domains, a gap exacerbated by the scarcity of annotated dialogue grounding data. We address this challenge with a three-tier data-synthesis method that balances realism and controllability to produce scalable supervision for dialogue-conditioned grounding. Fine-tuning on the synthesized data yields consistent, substantial improvements over prior approaches across standard evaluation metrics.",
        "authors": [
            "Juexi Shao",
            "Siyou Li",
            "Yujian Gan",
            "Chris Madge",
            "Vanja Karan",
            "Massimo Poesio"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T14:08:47+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02791v1"
    },
    {
        "arxiv_id": "2512.02772v1",
        "title": "Towards Unification of Hallucination Detection and Fact Verification for Large Language Models",
        "abstract": "Large Language Models (LLMs) frequently exhibit hallucinations, generating content that appears fluent and coherent but is factually incorrect. Such errors undermine trust and hinder their adoption in real-world applications. To address this challenge, two distinct research paradigms have emerged: model-centric Hallucination Detection (HD) and text-centric Fact Verification (FV). Despite sharing the same goal, these paradigms have evolved in isolation, using distinct assumptions, datasets, and evaluation protocols. This separation has created a research schism that hinders their collective progress. In this work, we take a decisive step toward bridging this divide. We introduce UniFact, a unified evaluation framework that enables direct, instance-level comparison between FV and HD by dynamically generating model outputs and corresponding factuality labels. Through large-scale experiments across multiple LLM families and detection methods, we reveal three key findings: (1) No paradigm is universally superior; (2) HD and FV capture complementary facets of factual errors; and (3) hybrid approaches that integrate both methods consistently achieve state-of-the-art performance. Beyond benchmarking, we provide the first in-depth analysis of why FV and HD diverged, as well as empirical evidence supporting the need for their unification. The comprehensive experimental results call for a new, integrated research agenda toward unifying Hallucination Detection and Fact Verification in LLMs.   We have open-sourced all the code, data, and baseline implementation at: https://github.com/oneal2000/UniFact/",
        "authors": [
            "Weihang Su",
            "Jianming Long",
            "Changyue Wang",
            "Shiyu Lin",
            "Jingyan Xu",
            "Ziyi Ye",
            "Qingyao Ai",
            "Yiqun Liu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T13:51:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02772v1"
    },
    {
        "arxiv_id": "2512.02764v1",
        "title": "PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models",
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods address the increasing size of Large Language Models (LLMs). Currently, many newly introduced PEFT methods are challenging to replicate, deploy, or compare with one another. To address this, we introduce PEFT-Factory, a unified framework for efficient fine-tuning LLMs using both off-the-shelf and custom PEFT methods. While its modular design supports extensibility, it natively provides a representative set of 19 PEFT methods, 27 classification and text generation datasets addressing 12 tasks, and both standard and PEFT-specific evaluation metrics. As a result, PEFT-Factory provides a ready-to-use, controlled, and stable environment, improving replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream framework that originates from the popular LLaMA-Factory, and is publicly available at https://github.com/kinit-sk/PEFT-Factory",
        "authors": [
            "Robert Belanec",
            "Ivan Srba",
            "Maria Bielikova"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T13:44:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02764v1"
    },
    {
        "arxiv_id": "2512.02763v1",
        "title": "SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys",
        "abstract": "LLM-based automatic survey systems are transforming how users acquire information from the web by integrating retrieval, organization, and content synthesis into end-to-end generation pipelines. While recent works focus on developing new generation pipelines, how to evaluate such complex systems remains a significant challenge. To this end, we introduce SurveyEval, a comprehensive benchmark that evaluates automatically generated surveys across three dimensions: overall quality, outline coherence, and reference accuracy. We extend the evaluation across 7 subjects and augment the LLM-as-a-Judge framework with human references to strengthen evaluation-human alignment. Evaluation results show that while general long-text or paper-writing systems tend to produce lower-quality surveys, specialized survey-generation systems are able to deliver substantially higher-quality results. We envision SurveyEval as a scalable testbed to understand and improve automatic survey systems across diverse subjects and evaluation criteria.",
        "authors": [
            "Jiahao Zhao",
            "Shuaixing Zhang",
            "Nan Xu",
            "Lei Wang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T13:42:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02763v1"
    },
    {
        "arxiv_id": "2512.02738v1",
        "title": "Probabilistic energy profiler for statically typed JVM-based programming languages",
        "abstract": "Energy consumption is a growing concern in several fields, from mobile devices to large data centers. Developers need detailed data on the energy consumption of their software to mitigate consumption issues. Previous approaches have a broader focus, such as on specific functions or programs, rather than source code statements. They primarily focus on estimating the CPU's energy consumption using point estimates, thereby disregarding other hardware effects and limiting their use for statistical reasoning and explainability. We developed a novel methodology to address the limitations of measuring only the CPU's consumption and using point estimates, focusing on predicting the energy usage of statically typed JVM-based programming languages, such as Java and Scala. We measure the energy consumption of Bytecode patterns, the translation from the programming language's source code statement to their Java Bytecode representation. With the energy measurements, we construct a statistical model using Bayesian statistics, which allows us to predict the energy consumption through statistical distributions and analyze individual factors. The model includes three factors we obtain statically from the code: data size, data type, operation, and one factor about the hardware platform the code executes on: device. To validate our methodology, we implemented it for Java and evaluated its energy predictions on unseen programs. We observe that all four factors are influential, notably that two devices of the same model may differ in energy consumption and that the operations and data types cause consumption differences. The experiments also show that the energy prediction of programs closely follows the program's real energy consumption, validating our approach. Our work presents a methodology for constructing an energy model that future work, such as verification tools, can use for their energy estimates.",
        "authors": [
            "Joel Nyholm",
            "Wojciech Mostowski",
            "Christoph Reichenbach"
        ],
        "category": "cs.PL",
        "submitted_date": "2025-12-02T13:21:35+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02738v1"
    },
    {
        "arxiv_id": "2512.02719v1",
        "title": "Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs",
        "abstract": "Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.",
        "authors": [
            "Julian Ma",
            "Jun Wang",
            "Zafeirios Fountas"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T12:51:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02719v1"
    },
    {
        "arxiv_id": "2512.02711v1",
        "title": "CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer",
        "abstract": "Ensuring content safety in large language models (LLMs) is essential for their deployment in real-world applications. However, existing safety guardrails are predominantly tailored for high-resource languages, leaving a significant portion of the world's population underrepresented who communicate in low-resource languages. To address this, we introduce CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model that supports 100 languages with only 0.5B parameters. By training on a strategically chosen subset of only 13 high-resource languages, our model utilizes cluster-based cross-lingual transfer from a few to 100 languages, enabling effective generalization to both unseen high-resource and low-resource languages. This approach addresses the challenge of limited training data in low-resource settings. We conduct comprehensive evaluations across six safety benchmarks to demonstrate that CREST outperforms existing state-of-the-art guardrails of comparable scale and achieves competitive results against models with significantly larger parameter counts (2.5B parameters and above). Our findings highlight the limitations of language-specific guardrails and underscore the importance of developing universal, language-agnostic safety systems that can scale effectively to serve global populations.",
        "authors": [
            "Lavish Bansal",
            "Naman Mishra"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T12:41:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02711v1"
    },
    {
        "arxiv_id": "2512.02689v1",
        "title": "An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation",
        "abstract": "Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.",
        "authors": [
            "Daiki Shirafuji",
            "Tatsuhiko Saito",
            "Yasutomo Kimura"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T12:18:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02689v1"
    },
    {
        "arxiv_id": "2512.02665v1",
        "title": "Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization",
        "abstract": "Large language models (LLMs) are now used in settings such as Google's AI Overviews, where it summarizes multiple long documents. However, it remains unclear whether they weight all inputs equally. Focusing on abortion-related news, we construct 40 pro-neutral-con article triplets, permute each triplet into six input orders, and prompt Gemini 2.5 Flash to generate a neutral overview. We evaluate each summary against its source articles using ROUGE-L (lexical overlap), BERTScore (semantic similarity), and SummaC (factual consistency). One-way ANOVA reveals a significant primacy effect for BERTScore across all stances, indicating that summaries are more semantically aligned with the first-seen article. Pairwise comparisons further show that Position 1 differs significantly from Positions 2 and 3, while the latter two do not differ from each other, confirming a selective preference for the first document. The findings present risks for applications that rely on LLM-generated overviews and for agentic AI systems, where the steps involving LLMs can disproportionately influence downstream actions.",
        "authors": [
            "Jing Ma"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T11:36:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02665v1"
    },
    {
        "arxiv_id": "2512.02593v1",
        "title": "Spoken Conversational Agents with Large Language Models",
        "abstract": "Spoken conversational agents are converging toward voice-native LLMs. This tutorial distills the path from cascaded ASR/NLU to end-to-end, retrieval-and vision-grounded systems. We frame adaptation of text LLMs to audio, cross-modal alignment, and joint speech-text training; review datasets, metrics, and robustness across accents and compare design choices (cascaded vs. E2E, post-ASR correction, streaming). We link industrial assistants to current open-domain and task-oriented agents, highlight reproducible baselines, and outline open problems in privacy, safety, and evaluation. Attendees leave with practical recipes and a clear systems-level roadmap.",
        "authors": [
            "Chao-Han Huck Yang",
            "Andreas Stolcke",
            "Larry Heck"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T10:02:10+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02593v1"
    },
    {
        "arxiv_id": "2512.02580v1",
        "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
        "abstract": "Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.",
        "authors": [
            "Changpeng Yang",
            "Jinyang Wu",
            "Yuchen Liu",
            "Shuai Zhang",
            "Yang Li",
            "Qiliang Liang",
            "Hongzhen Wang",
            "Shuai Nie",
            "Jiaming Xu",
            "Runyu Shi",
            "Ying Huang",
            "Guoquan Zhang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T09:48:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02580v1"
    },
    {
        "arxiv_id": "2512.02556v1",
        "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
        "abstract": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
        "authors": [
            "DeepSeek-AI",
            "Aixin Liu",
            "Aoxue Mei",
            "Bangcai Lin",
            "Bing Xue",
            "Bingxuan Wang",
            "Bingzheng Xu",
            "Bochao Wu",
            "Bowei Zhang",
            "Chaofan Lin",
            "Chen Dong",
            "Chengda Lu",
            "Chenggang Zhao",
            "Chengqi Deng",
            "Chenhao Xu",
            "Chong Ruan",
            "Damai Dai",
            "Daya Guo",
            "Dejian Yang",
            "Deli Chen",
            "Erhang Li",
            "Fangqi Zhou",
            "Fangyun Lin",
            "Fucong Dai",
            "Guangbo Hao",
            "Guanting Chen",
            "Guowei Li",
            "H. Zhang",
            "Hanwei Xu",
            "Hao Li",
            "Haofen Liang",
            "Haoran Wei",
            "Haowei Zhang",
            "Haowen Luo",
            "Haozhe Ji",
            "Honghui Ding",
            "Hongxuan Tang",
            "Huanqi Cao",
            "Huazuo Gao",
            "Hui Qu",
            "Hui Zeng",
            "Jialiang Huang",
            "Jiashi Li",
            "Jiaxin Xu",
            "Jiewen Hu",
            "Jingchang Chen",
            "Jingting Xiang",
            "Jingyang Yuan",
            "Jingyuan Cheng",
            "Jinhua Zhu",
            "Jun Ran",
            "Junguang Jiang",
            "Junjie Qiu",
            "Junlong Li",
            "Junxiao Song",
            "Kai Dong",
            "Kaige Gao",
            "Kang Guan",
            "Kexin Huang",
            "Kexing Zhou",
            "Kezhao Huang",
            "Kuai Yu",
            "Lean Wang",
            "Lecong Zhang",
            "Lei Wang",
            "Liang Zhao",
            "Liangsheng Yin",
            "Lihua Guo",
            "Lingxiao Luo",
            "Linwang Ma",
            "Litong Wang",
            "Liyue Zhang",
            "M. S. Di",
            "M. Y Xu",
            "Mingchuan Zhang",
            "Minghua Zhang",
            "Minghui Tang",
            "Mingxu Zhou",
            "Panpan Huang",
            "Peixin Cong",
            "Peiyi Wang",
            "Qiancheng Wang",
            "Qihao Zhu",
            "Qingyang Li",
            "Qinyu Chen",
            "Qiushi Du",
            "Ruiling Xu",
            "Ruiqi Ge",
            "Ruisong Zhang",
            "Ruizhe Pan",
            "Runji Wang",
            "Runqiu Yin",
            "Runxin Xu",
            "Ruomeng Shen",
            "Ruoyu Zhang",
            "S. H. Liu",
            "Shanghao Lu",
            "Shangyan Zhou",
            "Shanhuang Chen",
            "Shaofei Cai",
            "Shaoyuan Chen",
            "Shengding Hu",
            "Shengyu Liu",
            "Shiqiang Hu",
            "Shirong Ma",
            "Shiyu Wang",
            "Shuiping Yu",
            "Shunfeng Zhou",
            "Shuting Pan",
            "Songyang Zhou",
            "Tao Ni",
            "Tao Yun",
            "Tian Pei",
            "Tian Ye",
            "Tianyuan Yue",
            "Wangding Zeng",
            "Wen Liu",
            "Wenfeng Liang",
            "Wenjie Pang",
            "Wenjing Luo",
            "Wenjun Gao",
            "Wentao Zhang",
            "Xi Gao",
            "Xiangwen Wang",
            "Xiao Bi",
            "Xiaodong Liu",
            "Xiaohan Wang",
            "Xiaokang Chen",
            "Xiaokang Zhang",
            "Xiaotao Nie",
            "Xin Cheng",
            "Xin Liu",
            "Xin Xie",
            "Xingchao Liu",
            "Xingkai Yu",
            "Xingyou Li",
            "Xinyu Yang",
            "Xinyuan Li",
            "Xu Chen",
            "Xuecheng Su",
            "Xuehai Pan",
            "Xuheng Lin",
            "Xuwei Fu",
            "Y. Q. Wang",
            "Yang Zhang",
            "Yanhong Xu",
            "Yanru Ma",
            "Yao Li",
            "Yao Li",
            "Yao Zhao",
            "Yaofeng Sun",
            "Yaohui Wang",
            "Yi Qian",
            "Yi Yu",
            "Yichao Zhang",
            "Yifan Ding",
            "Yifan Shi",
            "Yiliang Xiong",
            "Ying He",
            "Ying Zhou",
            "Yinmin Zhong",
            "Yishi Piao",
            "Yisong Wang",
            "Yixiao Chen",
            "Yixuan Tan",
            "Yixuan Wei",
            "Yiyang Ma",
            "Yiyuan Liu",
            "Yonglun Yang",
            "Yongqiang Guo",
            "Yongtong Wu",
            "Yu Wu",
            "Yuan Cheng",
            "Yuan Ou",
            "Yuanfan Xu",
            "Yuduan Wang",
            "Yue Gong",
            "Yuhan Wu",
            "Yuheng Zou",
            "Yukun Li",
            "Yunfan Xiong",
            "Yuxiang Luo",
            "Yuxiang You",
            "Yuxuan Liu",
            "Yuyang Zhou",
            "Z. F. Wu",
            "Z. Z. Ren",
            "Zehua Zhao",
            "Zehui Ren",
            "Zhangli Sha",
            "Zhe Fu",
            "Zhean Xu",
            "Zhenda Xie",
            "Zhengyan Zhang",
            "Zhewen Hao",
            "Zhibin Gou",
            "Zhicheng Ma",
            "Zhigang Yan",
            "Zhihong Shao",
            "Zhixian Huang",
            "Zhiyu Wu",
            "Zhuoshu Li",
            "Zhuping Zhang",
            "Zian Xu",
            "Zihao Wang",
            "Zihui Gu",
            "Zijia Zhu",
            "Zilin Li",
            "Zipeng Zhang",
            "Ziwei Xie",
            "Ziyi Gao",
            "Zizheng Pan",
            "Zongqing Yao",
            "Bei Feng",
            "Hui Li",
            "J. L. Cai",
            "Jiaqi Ni",
            "Lei Xu",
            "Meng Li",
            "Ning Tian",
            "R. J. Chen",
            "R. L. Jin",
            "S. S. Li",
            "Shuang Zhou",
            "Tianyu Sun",
            "X. Q. Li",
            "Xiangyue Jin",
            "Xiaojin Shen",
            "Xiaosha Chen",
            "Xinnan Song",
            "Xinyi Zhou",
            "Y. X. Zhu",
            "Yanping Huang",
            "Yaohui Li",
            "Yi Zheng",
            "Yuchen Zhu",
            "Yunxian Ma",
            "Zhen Huang",
            "Zhipeng Xu",
            "Zhongyu Zhang",
            "Dongjie Ji",
            "Jian Liang",
            "Jianzhong Guo",
            "Jin Chen",
            "Leyi Xia",
            "Miaojun Wang",
            "Mingming Li",
            "Peng Zhang",
            "Ruyi Chen",
            "Shangmian Sun",
            "Shaoqing Wu",
            "Shengfeng Ye",
            "T. Wang",
            "W. L. Xiao",
            "Wei An",
            "Xianzu Wang",
            "Xiaowen Sun",
            "Xiaoxiang Wang",
            "Ying Tang",
            "Yukun Zha",
            "Zekai Zhang",
            "Zhe Ju",
            "Zhen Zhang",
            "Zihua Qu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T09:25:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02556v1"
    },
    {
        "arxiv_id": "2512.02555v1",
        "title": "ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce",
        "abstract": "Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.",
        "authors": [
            "Zheng Fang",
            "Donghao Xie",
            "Ming Pang",
            "Chunyuan Yuan",
            "Xue Jiang",
            "Changping Peng",
            "Zhangang Lin",
            "Zheng Luo"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T09:25:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02555v1"
    },
    {
        "arxiv_id": "2512.02552v1",
        "title": "What Signals Really Matter for Misinformation Tasks? Evaluating Fake-News Detection and Virality Prediction under Real-World Constraints",
        "abstract": "We present an evaluation-driven study of two practical tasks regarding online misinformation: (i) fake-news detection and (ii) virality prediction in the context of operational settings, with the necessity for rapid reaction. Using the EVONS and FakeNewsNet datasets, we compare textual embeddings (RoBERTa; with a control using Mistral) against lightweight numeric features (timing, follower counts, verification, likes) and sequence models (GRU, gating architectures, Transformer encoders). We show that textual content alone is a strong discriminator for fake-news detection, while numeric-only pipelines remain viable when language models are unavailable or compute is constrained. Virality prediction is markedly harder than fake-news detection and is highly sensitive to label construction; in our setup, a median-based ''viral'' split (<50 likes) is pragmatic but underestimates real-world virality, and time-censoring for engagement features is desirable yet difficult under current API limits. Dimensionality-reduction analyses suggest non-linear structure is more informative for virality than for fake-news detection (t-SNE > PCA on numeric features). Swapping RoBERTa for Mistral embeddings yields only modest deltas, leaving conclusions unchanged. We discuss implications for evaluation design and report reproducibility constraints that realistically affect the field. We release splits and code where possible and provide guidance for metric selection.",
        "authors": [
            "Francesco Paolo Savatteri",
            "Chahan Vidal-Gor\u00e8ne",
            "Florian Cafiero"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T09:24:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02552v1"
    },
    {
        "arxiv_id": "2512.02527v1",
        "title": "A Concise Review of Hallucinations in LLMs and their Mitigation",
        "abstract": "Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.",
        "authors": [
            "Parth Pulkundwar",
            "Vivek Dhanawade",
            "Rohit Yadav",
            "Minal Sonkar",
            "Medha Asurlekar",
            "Sarita Rathod"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T08:44:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02527v1"
    },
    {
        "arxiv_id": "2512.02472v1",
        "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
        "abstract": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.",
        "authors": [
            "Wenhao Yu",
            "Zhenwen Liang",
            "Chengsong Huang",
            "Kishan Panaganti",
            "Tianqing Fang",
            "Haitao Mi",
            "Dong Yu"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-12-02T07:06:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02472v1"
    },
    {
        "arxiv_id": "2512.02456v1",
        "title": "See, Think, Learn: A Self-Taught Multimodal Reasoner",
        "abstract": "Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.",
        "authors": [
            "Sourabh Sharma",
            "Sonam Gupta",
            "Sadbhawna"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-12-02T06:30:10+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02456v1"
    },
    {
        "arxiv_id": "2512.02445v1",
        "title": "When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents",
        "abstract": "Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\\sim$5\\% to $\\sim$40\\% while Grok 4 Fast decreases from $\\sim$80\\% to $\\sim$10\\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.",
        "authors": [
            "Tsimur Hadeliya",
            "Mohammad Ali Jauhar",
            "Nidhi Sakpal",
            "Diogo Cruz"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-12-02T06:12:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02445v1"
    },
    {
        "arxiv_id": "2512.02425v1",
        "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
        "abstract": "Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.",
        "authors": [
            "Woongyeong Yeo",
            "Kangsan Kim",
            "Jaehong Yoon",
            "Sung Ju Hwang"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-12-02T05:14:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02425v1"
    },
    {
        "arxiv_id": "2512.02419v1",
        "title": "The brain-AI convergence: Predictive and generative world models for general-purpose computation",
        "abstract": "Recent advances in general-purpose AI systems with attention-based transformers offer a potential window into how the neocortex and cerebellum, despite their relatively uniform circuit architectures, give rise to diverse functions and, ultimately, to human intelligence. This Perspective provides a cross-domain comparison between the brain and AI that goes beyond the traditional focus on visual processing, adopting the emerging perspecive of world-model-based computation. Here, we identify shared computational mechanisms in the attention-based neocortex and the non-attentional cerebellum: both predict future world events from past inputs and construct internal world models through prediction-error learning. These predictive world models are repurposed for seemingly distinct functions -- understanding in sensory processing and generation in motor processing -- enabling the brain to achieve multi-domain capabilities and human-like adaptive intelligence. Notably, attention-based AI has independently converged on a similar learning paradigm and world-model-based computation. We conclude that these shared mechanisms in both biological and artificial systems constitute a core computational foundation for realizing diverse functions including high-level intelligence, despite their relatively uniform circuit structures. Our theoretical insights bridge neuroscience and AI, advancing our understanding of the computational essence of intelligence.",
        "authors": [
            "Shogo Ohmae",
            "Keiko Ohmae"
        ],
        "category": "q-bio.NC",
        "submitted_date": "2025-12-02T05:03:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02419v1"
    },
    {
        "arxiv_id": "2512.02402v1",
        "title": "TaleFrame: An Interactive Story Generation System with Fine-Grained Control and Large Language Models",
        "abstract": "With the advancement of natural language generation (NLG) technologies, creative story generation systems have gained increasing attention. However, current systems often fail to accurately translate user intent into satisfactory story outputs due to a lack of fine-grained control and unclear input specifications, limiting their applicability. To address this, we propose TaleFrame, a system that combines large language models (LLMs) with human-computer interaction (HCI) to generate stories through structured information, enabling precise control over the generation process. The innovation of TaleFrame lies in decomposing the story structure into four basic units: entities, events, relationships, and story outline. We leverage the Tinystories dataset, parsing and constructing a preference dataset consisting of 9,851 JSON-formatted entries, which is then used to fine-tune a local Llama model. By employing this JSON2Story approach, structured data is transformed into coherent stories. TaleFrame also offers an intuitive interface that supports users in creating and editing entities and events and generates stories through the structured framework. Users can control these units through simple interactions (e.g., drag-and-drop, attach, and connect), thus influencing the details and progression of the story. The generated stories can be evaluated across seven dimensions (e.g., creativity, structural integrity), with the system providing suggestions for refinement based on these evaluations. Users can iteratively adjust the story until a satisfactory result is achieved. Finally, we conduct quantitative evaluation and user studies that demonstrate the usefulness of TaleFrame. Dataset available at https://huggingface.co/datasets/guodaosun/tale-frame.",
        "authors": [
            "Yunchao Wang",
            "Guodao Sun",
            "Zihang Fu",
            "Zhehao Liu",
            "Kaixing Du",
            "Haidong Gao",
            "Ronghua Liang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T04:27:10+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02402v1"
    },
    {
        "arxiv_id": "2512.02393v1",
        "title": "Process-Centric Analysis of Agentic Software Systems",
        "abstract": "Agentic systems are modern software systems: they consist of orchestrated modules, expose interfaces, and are deployed in software pipelines. Unlike conventional programs, their execution (i.e., trajectories) is inherently stochastic and adaptive to the problem they are solving. Evaluation of such systems is often outcome-centric, judging their performance based on success or failure at the final step. This narrow focus overlooks detailed insights about such systems, failing to explain how agents reason, plan, act, or change their strategies over time. Inspired by the structured representation of conventional software systems as graphs, we introduce Graphectory to systematically encode the temporal and semantic relations in such software systems. Graphectory facilitates the design of process-centric metrics and analyses to assess the quality of agentic workflows independent of final success.   Using Graphectory, we analyze 4000 trajectories of two dominant agentic programming workflows, namely SWE-agent and OpenHands, with a combination of four backbone Large Language Models (LLMs), attempting to resolve SWE-bench Verified issues. Our fully automated analyses reveal that: (1) agents using richer prompts or stronger LLMs exhibit more complex Graphectory, reflecting deeper exploration, broader context gathering, and more thorough validation before patch submission; (2) agents' problem-solving strategies vary with both problem difficulty and the underlying LLM -- for resolved issues, the strategies often follow coherent localization-patching-validation steps, while unresolved ones exhibit chaotic, repetitive, or backtracking behaviors; (3) even when successful, agentic programming systems often display inefficient processes, leading to unnecessarily prolonged trajectories.",
        "authors": [
            "Shuyang Liu",
            "Yang Chen",
            "Rahul Krishna",
            "Saurabh Sinha",
            "Jatin Ganhotra",
            "Reyhan Jabbarvand"
        ],
        "category": "cs.SE",
        "submitted_date": "2025-12-02T04:12:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02393v1"
    },
    {
        "arxiv_id": "2512.02363v1",
        "title": "Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering",
        "abstract": "Domain-specific question answering (QA) systems for services face unique challenges in integrating heterogeneous knowledge sources while ensuring both accuracy and safety. Existing large language models often struggle with factual consistency and context alignment in sensitive domains such as healthcare policies and government welfare. In this work, we introduce Knowledge-Aware Reasoning and Memory-Augmented Adaptation (KARMA), a novel framework designed to enhance QA performance in care scenarios. KARMA incorporates a dual-encoder architecture to fuse structured and unstructured knowledge sources, a gated memory unit to dynamically regulate external knowledge integration, and a safety-aware controllable decoder that mitigates unsafe outputs using safety classification and guided generation techniques. Extensive experiments on a proprietary QA dataset demonstrate that KARMA outperforms strong baselines in both answer quality and safety. This study offers a comprehensive solution for building trustworthy and adaptive QA systems in service contexts.",
        "authors": [
            "Lei Fu",
            "Xiang Chen",
            "Kaige Gao Xinyue Huang",
            "Kejian Tong"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T03:12:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02363v1"
    },
    {
        "arxiv_id": "2512.02321v1",
        "title": "LeechHijack: Covert Computational Resource Exploitation in Intelligent Agent Systems",
        "abstract": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in reasoning, planning, and tool usage. The recently proposed Model Context Protocol (MCP) has emerged as a unifying framework for integrating external tools into agent systems, enabling a thriving open ecosystem of community-built functionalities. However, the openness and composability that make MCP appealing also introduce a critical yet overlooked security assumption -- implicit trust in third-party tool providers. In this work, we identify and formalize a new class of attacks that exploit this trust boundary without violating explicit permissions. We term this new attack vector implicit toxicity, where malicious behaviors occur entirely within the allowed privilege scope. We propose LeechHijack, a Latent Embedded Exploit for Computation Hijacking, in which an adversarial MCP tool covertly expropriates the agent's computational resources for unauthorized workloads. LeechHijack operates through a two-stage mechanism: an implantation stage that embeds a benign-looking backdoor in a tool, and an exploitation stage where the backdoor activates upon predefined triggers to establish a command-and-control channel. Through this channel, the attacker injects additional tasks that the agent executes as if they were part of its normal workflow, effectively parasitizing the user's compute budget. We implement LeechHijack across four major LLM families. Experiments show that LeechHijack achieves an average success rate of 77.25%, with a resource overhead of 18.62% compared to the baseline. This study highlights the urgent need for computational provenance and resource attestation mechanisms to safeguard the emerging MCP ecosystem.",
        "authors": [
            "Yuanhe Zhang",
            "Weiliu Wang",
            "Zhenhong Zhou",
            "Kun Wang",
            "Jie Zhang",
            "Li Sun",
            "Yang Liu",
            "Sen Su"
        ],
        "category": "cs.CR",
        "submitted_date": "2025-12-02T01:34:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02321v1"
    },
    {
        "arxiv_id": "2512.02306v1",
        "title": "OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning",
        "abstract": "Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.",
        "authors": [
            "Boyu Zhu",
            "Xiaofei Wen",
            "Wenjie Jacky Mo",
            "Tinghui Zhu",
            "Yanan Xie",
            "Peng Qi",
            "Muhao Chen"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-12-02T01:01:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02306v1"
    },
    {
        "arxiv_id": "2512.02304v1",
        "title": "When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers",
        "abstract": "Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.",
        "authors": [
            "Jack Lu",
            "Ryan Teehan",
            "Jinran Jin",
            "Mengye Ren"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T00:51:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02304v1"
    },
    {
        "arxiv_id": "2512.02299v1",
        "title": "HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models",
        "abstract": "How do language models use contextual information to answer health questions? How are their responses impacted by conflicting contexts? We assess the ability of language models to reason over long, conflicting biomedical contexts using HealthContradict, an expert-verified dataset comprising 920 unique instances, each consisting of a health-related question, a factual answer supported by scientific evidence, and two documents presenting contradictory stances. We consider several prompt settings, including correct, incorrect or contradictory context, and measure their impact on model outputs. Compared to existing medical question-answering evaluation benchmarks, HealthContradict provides greater distinctions of language models' contextual reasoning capabilities. Our experiments show that the strength of fine-tuned biomedical language models lies not only in their parametric knowledge from pretraining, but also in their ability to exploit correct context while resisting incorrect context.",
        "authors": [
            "Boya Zhang",
            "Alban Bornet",
            "Rui Yang",
            "Nan Liu",
            "Douglas Teodoro"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-02T00:38:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02299v1"
    },
    {
        "arxiv_id": "2512.02251v1",
        "title": "CAIRNS: Balancing Readability and Scientific Accuracy in Climate Adaptation Question Answering",
        "abstract": "Climate adaptation strategies are proposed in response to climate change. They are practised in agriculture to sustain food production. These strategies can be found in unstructured data (for example, scientific literature from the Elsevier website) or structured (heterogeneous climate data via government APIs). We present Climate Adaptation question-answering with Improved Readability and Noted Sources (CAIRNS), a framework that enables experts -- farmer advisors -- to obtain credible preliminary answers from complex evidence sources from the web. It enhances readability and citation reliability through a structured ScholarGuide prompt and achieves robust evaluation via a consistency-weighted hybrid evaluator that leverages inter-model agreement with experts. Together, these components enable readable, verifiable, and domain-grounded question-answering without fine-tuning or reinforcement learning. Using a previously reported dataset of expert-curated question-answers, we show that CAIRNS outperforms the baselines on most of the metrics. Our thorough ablation study confirms the results on all metrics. To validate our LLM-based evaluation, we also report an analysis of correlations against human judgment.",
        "authors": [
            "Liangji Kong",
            "Aditya Joshi",
            "Sarvnaz Karimi"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T22:44:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02251v1"
    },
    {
        "arxiv_id": "2512.02246v1",
        "title": "DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models",
        "abstract": "Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.",
        "authors": [
            "Olivia Kim"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T22:28:39+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02246v1"
    },
    {
        "arxiv_id": "2512.02240v1",
        "title": "Lightweight Latent Reasoning for Narrative Tasks",
        "abstract": "Large language models (LLMs) tackle complex tasks by generating long chains of thought or \"reasoning traces\" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.",
        "authors": [
            "Alexander Gurung",
            "Nikolay Malkin",
            "Mirella Lapata"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T22:07:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02240v1"
    },
    {
        "arxiv_id": "2512.02201v1",
        "title": "Swivuriso: The South African Next Voices Multilingual Speech Dataset",
        "abstract": "This paper introduces Swivuriso, a 3000-hour multilingual speech dataset developed as part of the African Next Voices project, to support the development and benchmarking of automatic speech recognition (ASR) technologies in seven South African languages. Covering agriculture, healthcare, and general domain topics, Swivuriso addresses significant gaps in existing ASR datasets. We describe the design principles, ethical considerations, and data collection procedures that guided the dataset creation. We present baseline results of training/finetuning ASR models with this data and compare to other ASR datasets for the langauges concerned.",
        "authors": [
            "Vukosi Marivatee",
            "Kayode Olaleye",
            "Sitwala Mundia",
            "Andinda Bakainga",
            "Unarine Netshifhefhe",
            "Mahmooda Milanzie",
            "Tsholofelo Hope Mogale",
            "Thapelo Sindane",
            "Zainab Abdulrasaq",
            "Kesego Mokgosi",
            "Chijioke Okorie",
            "Nia Zion Van Wyk",
            "Graham Morrissey",
            "Dale Dunbar",
            "Francois Smit",
            "Tsosheletso Chidi",
            "Rooweither Mabuya",
            "Andiswa Bukula",
            "Respect Mlambo",
            "Tebogo Macucwa",
            "Idris Abdulmumin",
            "and Seani Rananga"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T20:49:10+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02201v1"
    },
    {
        "arxiv_id": "2512.02195v1",
        "title": "A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation",
        "abstract": "This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.",
        "authors": [
            "David Ph. Shakouri",
            "Crit Cremers",
            "Niels O. Schiller"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T20:40:36+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02195v1"
    },
    {
        "arxiv_id": "2512.02192v1",
        "title": "Story2MIDI: Emotionally Aligned Music Generation from Text",
        "abstract": "In this paper, we introduce Story2MIDI, a sequence-to-sequence Transformer-based model for generating emotion-aligned music from a given piece of text. To develop this model, we construct the Story2MIDI dataset by merging existing datasets for sentiment analysis from text and emotion classification in music. The resulting dataset contains pairs of text blurbs and music pieces that evoke the same emotions in the reader or listener. Despite the small scale of our dataset and limited computational resources, our results indicate that our model effectively learns emotion-relevant features in music and incorporates them into its generation process, producing samples with diverse emotional responses. We evaluate the generated outputs using objective musical metrics and a human listening study, confirming the model's ability to capture intended emotional cues.",
        "authors": [
            "Mohammad Shokri",
            "Alexandra C. Salem",
            "Gabriel Levine",
            "Johanna Devaney",
            "Sarah Ita Levitan"
        ],
        "category": "cs.SD",
        "submitted_date": "2025-12-01T20:35:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02192v1"
    },
    {
        "arxiv_id": "2512.02185v1",
        "title": "Think Before You Prune: Self-Reflective Structured Pruning for Reasoning Language Models",
        "abstract": "Reasoning LLMs (RLMs) such as OpenAI o1, DeepSeek-R1, and Qwen3 deliver strong multi-step reasoning through chain-of-thought generation, but their large model sizes and lengthy decode-time outputs make them costly to deploy and unsuitable for resource-constrained settings. To reduce computing and memory cost, pruning offers a promising solution by removing unimportant parameters. However, despite their success on standard LLMs, existing pruning methods severely damage RLMs, as even moderate sparsity (e.g., 20%) can collapse accuracy and completely disrupt the model's reasoning coherence. We begin by analyzing why existing pruning pipelines fail on reasoning LLMs and find that their brittleness largely stems from a mismatch between the calibration data, the pruning objective, and the model's decode-time reasoning behavior. Our study further shows that the most reliable calibration signal comes not from human-written labels but from the model's own self-generated reasoning traces, which more accurately reflect its inference distribution. Guided by these insights, we introduce RESP, a self-reflective structured pruning framework that aligns pruning decisions with the model's reasoning dynamics through self-generated calibration, decode-only gradient-based importance estimation, and progressive regeneration that maintains calibration fidelity as sparsity increases. Experiments on Qwen3-8B demonstrate that RESP markedly outperforms existing structured pruning methods on both GSM8K and MathQA, preserving near-dense accuracy at 20-30% sparsity and substantially mitigating performance collapse at higher sparsity levels. At 40% sparsity, RESP attains 81.3% accuracy on GSM8K and 59.6% on MathQA, surpassing the strongest baselines by 66.87% and 47%, respectively.",
        "authors": [
            "Ziyan Wang",
            "Enmao Diao",
            "Qi Le",
            "Pu Wang",
            "Guanchu Wang",
            "Minwoo Lee",
            "Shu-ping Yeh",
            "Li Yang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T20:27:05+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02185v1"
    },
    {
        "arxiv_id": "2512.02157v1",
        "title": "Factor(T,U): Factored Cognition Strengthens Monitoring of Untrusted AI",
        "abstract": "The field of AI Control seeks to develop robust control protocols, deployment safeguards for untrusted AI which may be intentionally subversive. However, existing protocols that rely on weaker monitors to detect unsafe behavior often fail on complex tasks beyond the monitor's comprehension. We develop control protocols based on factored cognition, in which a trusted model decomposes a task, an untrusted model solves each resultant child task in isolation, and the results are reassembled into a full solution. These protocols may improve safety by several means, such as by simplifying the context for monitors, or by obscuring vulnerabilities in the environment. We implement our protocols in the APPS coding setting and red team them against backdoor attempts from an adversarial GPT-4.1 Nano. We find that: (i) Adding factored cognition to a trusted monitoring protocol can boost safety from 41% to 63%; (ii) Safety improves because monitor performance improves; (iii) Factored cognition makes it no harder for capable LLMs to write backdoors in APPS. While our protocols show low usefulness in APPS, they hold promise for more complex tasks.",
        "authors": [
            "Aaron Sandoval",
            "Cody Rushing"
        ],
        "category": "cs.CR",
        "submitted_date": "2025-12-01T19:37:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02157v1"
    },
    {
        "arxiv_id": "2512.02141v1",
        "title": "Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation",
        "abstract": "Abusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation.",
        "authors": [
            "Pritish N. Desai",
            "Tanay Kewalramani",
            "Srimanta Mandal"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T19:11:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02141v1"
    },
    {
        "arxiv_id": "2512.02010v1",
        "title": "Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling",
        "abstract": "As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.",
        "authors": [
            "Jack Cook",
            "Junxian Guo",
            "Guangxuan Xiao",
            "Yujun Lin",
            "Song Han"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T18:59:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02010v1"
    },
    {
        "arxiv_id": "2512.02008v1",
        "title": "The Art of Scaling Test-Time Compute for Large Language Models",
        "abstract": "Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.",
        "authors": [
            "Aradhye Agarwal",
            "Ayan Sengupta",
            "Tanmoy Chakraborty"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T18:59:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02008v1"
    },
    {
        "arxiv_id": "2512.02004v1",
        "title": "AlignSAE: Concept-Aligned Sparse Autoencoders",
        "abstract": "Large Language Models (LLMs) encode factual knowledge within hidden parametric spaces that are difficult to inspect or control. While Sparse Autoencoders (SAEs) can decompose hidden activations into more fine-grained, interpretable features, they often struggle to reliably align these features with human-defined concepts, resulting in entangled and distributed feature representations. To address this, we introduce AlignSAE, a method that aligns SAE features with a defined ontology through a \"pre-train, then post-train\" curriculum. After an initial unsupervised training phase, we apply supervised post-training to bind specific concepts to dedicated latent slots while preserving the remaining capacity for general reconstruction. This separation creates an interpretable interface where specific relations can be inspected and controlled without interference from unrelated features. Empirical results demonstrate that AlignSAE enables precise causal interventions, such as reliable \"concept swaps\", by targeting single, semantically aligned slots.",
        "authors": [
            "Minglai Yang",
            "Xinyu Guo",
            "Mihai Surdeanu",
            "Liangming Pan"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-12-01T18:58:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02004v1"
    },
    {
        "arxiv_id": "2512.01992v1",
        "title": "LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through Chess",
        "abstract": "We introduce LLM CHESS, an evaluation framework designed to probe the generalization of reasoning and instruction-following abilities in large language models (LLMs) through extended agentic interaction in the domain of chess. We rank over 50 open and closed source models by playing against a random opponent using a range of behavioral metrics, including win and loss rates, move quality, move legality, hallucinated actions, and game duration. For a subset of top reasoning models, we derive an Elo estimate by playing against a chess engine with variably configured skill, which allows for comparisons between models in an easily understandable way. Despite the simplicity of the instruction-following task and the weakness of the opponent, many state-of-the-art models struggle to complete games or achieve consistent wins. Similar to other benchmarks on complex reasoning tasks, our experiments reveal a clear separation between reasoning and non-reasoning models. However, unlike existing static benchmarks, the stochastic and dynamic nature of LLM CHESS uniquely reduces overfitting and memorization while preventing benchmark saturation, proving difficult even for top reasoning models. To support future work on evaluating reasoning and instruction-following in LLMs, we release our experimental framework, a public leaderboard, and a dataset of associated games.",
        "authors": [
            "Sai Kolasani",
            "Maxim Saplin",
            "Nicholas Crispino",
            "Kyle Montgomery",
            "Jared Quincy Davis",
            "Matei Zaharia",
            "Chi Wang",
            "Chenguang Wang"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-12-01T18:51:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01992v1"
    },
    {
        "arxiv_id": "2512.01979v1",
        "title": "Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback",
        "abstract": "GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.",
        "authors": [
            "Aiden Yiliu Li",
            "Bizhi Yu",
            "Daoan Lei",
            "Tianhe Ren",
            "Shilong Liu"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-12-01T18:37:19+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01979v1"
    },
    {
        "arxiv_id": "2512.01970v2",
        "title": "From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning",
        "abstract": "The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.",
        "authors": [
            "Sitao Cheng",
            "Xunjian Yin",
            "Ruiwen Zhou",
            "Yuxuan Li",
            "Xinyi Wang",
            "Liangming Pan",
            "William Yang Wang",
            "Victor Zhong"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-12-01T18:27:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01970v2"
    },
    {
        "arxiv_id": "2512.01948v1",
        "title": "How Far Are We from Genuinely Useful Deep Research Agents?",
        "abstract": "Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.",
        "authors": [
            "Dingling Zhang",
            "He Zhu",
            "Jincheng Ren",
            "Kangqi Song",
            "Xinran Zhou",
            "Boyu Feng",
            "Shudong Liu",
            "Jiabin Luo",
            "Weihao Xie",
            "Zhaohui Wang",
            "Tianrui Qin",
            "King Zhu",
            "Yuqing Wang",
            "Qianben Chen",
            "Yuchen Eleanor Jiang",
            "Wei Wang",
            "Jiaheng Liu",
            "Wangchunshu Zhou"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T17:58:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01948v1"
    },
    {
        "arxiv_id": "2512.01945v1",
        "title": "Agentic Policy Optimization via Instruction-Policy Co-Evolution",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.",
        "authors": [
            "Han Zhou",
            "Xingchen Wan",
            "Ivan Vuli\u0107",
            "Anna Korhonen"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-12-01T17:56:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01945v1"
    },
    {
        "arxiv_id": "2512.01925v1",
        "title": "Rectifying LLM Thought from Lens of Optimization",
        "abstract": "Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.",
        "authors": [
            "Junnan Liu",
            "Hongwei Liu",
            "Songyang Zhang",
            "Kai Chen"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T17:41:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01925v1"
    },
    {
        "arxiv_id": "2512.01909v1",
        "title": "Latent Debate: A Surrogate Framework for Interpreting LLM Thinking",
        "abstract": "Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference. We first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks. Empirical studies demonstrate that latent debate is a faithful structured surrogate model that has highly consistent predictions with the original LLM. Beyond interpretability, we demonstrate that latent debate provides a strong baseline for hallucination detection. Further analysis reveals strong correlations between hallucinations and debate patterns, such as a high degree of latent debates in the middle layers is linked to a higher risk of hallucinations. These findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.",
        "authors": [
            "Lihu Chen",
            "Xiang Yin",
            "Francesca Toni"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T17:27:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01909v1"
    },
    {
        "arxiv_id": "2512.01896v1",
        "title": "OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report Generation",
        "abstract": "Online Public Opinion Reports consolidate news and social media for timely crisis management by governments and enterprises. While large language models have made automated report generation technically feasible, systematic research in this specific area remains notably absent, particularly lacking formal task definitions and corresponding benchmarks. To bridge this gap, we define the Automated Online Public Opinion Report Generation (OPOR-GEN) task and construct OPOR-BENCH, an event-centric dataset covering 463 crisis events with their corresponding news articles, social media posts, and a reference summary. To evaluate report quality, we propose OPOR-EVAL, a novel agent-based framework that simulates human expert evaluation by analyzing generated reports in context. Experiments with frontier models demonstrate that our framework achieves high correlation with human judgments. Our comprehensive task definition, benchmark dataset, and evaluation framework provide a solid foundation for future research in this critical domain.",
        "authors": [
            "Jinzheng Yu",
            "Yang Xu",
            "Haozhen Li",
            "Junqi Li",
            "Yifan Feng",
            "Ligu Zhu",
            "Hao Shen",
            "Lei Shi"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T17:18:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01896v1"
    },
    {
        "arxiv_id": "2512.01892v1",
        "title": "Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models",
        "abstract": "With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and relevance. In a within-subject study design, 57 participants assessed the responses under two conditions: harmful response plus its mitigation and solely mitigated response. Results revealed that participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations. Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts. This contrasts with how language is often treated in the quantitative evaluation of LLMs. We also introduced new metrics for training and evaluating mitigation strategies and insights for human-AI evaluation studies.",
        "authors": [
            "Heloisa Candello",
            "Muneeza Azmat",
            "Uma Sushmitha Gunturi",
            "Raya Horesh",
            "Rogerio Abreu de Paula",
            "Heloisa Pimentel",
            "Marcelo Carpinette Grave",
            "Aminat Adebiyi",
            "Tiago Machado",
            "Maysa Malfiza Garcia de Macedo"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T17:12:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01892v1"
    },
    {
        "arxiv_id": "2512.01865v1",
        "title": "Cross-Lingual Interleaving for Speech Language Models",
        "abstract": "Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also release an EN-FR training dataset, TinyStories (~42k hours), together with EN-FR spoken StoryCloze and TopicCloze benchmarks for cross-lingual semantic evaluation, both synthetically generated using GPT-4. On 360M and 1B SLMs under matched training-token budgets, interleaving improves monolingual semantic accuracy, enables robust cross-lingual continuation, and strengthens cross-lingual hidden-state alignment. Taken together, these results indicate that cross-lingual interleaving is a simple, scalable route to building multilingual SLMs that understand and converse across languages. All resources will be made open-source to support reproducibility.",
        "authors": [
            "Adel Moumen",
            "Guangzhi Sun",
            "Philip C. Woodland"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T16:48:05+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01865v1"
    },
    {
        "arxiv_id": "2512.01852v1",
        "title": "BHRAM-IL: A Benchmark for Hallucination Recognition and Assessment in Multiple Indian Languages",
        "abstract": "Large language models (LLMs) are increasingly deployed in multilingual applications but often generate plausible yet incorrect or misleading outputs, known as hallucinations. While hallucination detection has been studied extensively in English, under-resourced Indian languages remain largely unexplored. We present BHRAM-IL, a benchmark for hallucination recognition and assessment in multiple Indian languages, covering Hindi, Gujarati, Marathi, Odia, along with English. The benchmark comprises 36,047 curated questions across nine categories spanning factual, numerical, reasoning, and linguistic tasks. We evaluate 14 state-of-the-art multilingual LLMs on a benchmark subset of 10,265 questions, analyzing cross-lingual and factual hallucinations across languages, models, scales, categories, and domains using category-specific metrics normalized to (0,1) range. Aggregation over all categories and models yields a primary score of 0.23 and a language-corrected fuzzy score of 0.385, demonstrating the usefulness of BHRAM-IL for hallucination-focused evaluation. The dataset, and the code for generation and evaluation are available on GitHub (https://github.com/sambhashana/BHRAM-IL/) and HuggingFace (https://huggingface.co/datasets/sambhashana/BHRAM-IL/) to support future research in multilingual hallucination detection and mitigation.",
        "authors": [
            "Hrishikesh Terdalkar",
            "Kirtan Bhojani",
            "Aryan Dongare",
            "Omm Aditya Behera"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T16:37:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01852v1"
    },
    {
        "arxiv_id": "2512.01848v1",
        "title": "Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability",
        "abstract": "Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes.",
        "authors": [
            "Jinghan Jia",
            "Nathalie Baracaldo",
            "Sijia Liu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T16:35:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01848v1"
    },
    {
        "arxiv_id": "2512.01822v1",
        "title": "InnoGym: Benchmarking the Innovation Potential of AI Agents",
        "abstract": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.",
        "authors": [
            "Jintian Zhang",
            "Kewei Xu",
            "Jingsheng Zheng",
            "Zhuoyun Yu",
            "Yuqi Zhu",
            "Yujie Luo",
            "Lanning Wei",
            "Shuofei Qiao",
            "Lun Du",
            "Da Zheng",
            "Shumin Deng",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T16:03:04+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01822v1"
    },
    {
        "arxiv_id": "2512.01797v2",
        "title": "H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs",
        "abstract": "Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.",
        "authors": [
            "Cheng Gao",
            "Huimin Chen",
            "Chaojun Xiao",
            "Zhiyi Chen",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-12-01T15:32:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01797v2"
    },
    {
        "arxiv_id": "2512.01728v1",
        "title": "Reasoning About the Unsaid: Misinformation Detection with Omission-Aware Graph Inference",
        "abstract": "This paper investigates the detection of misinformation, which deceives readers by explicitly fabricating misleading content or implicitly omitting important information necessary for informed judgment. While the former has been extensively studied, omission-based deception remains largely overlooked, even though it can subtly guide readers toward false conclusions under the illusion of completeness. To pioneer in this direction, this paper presents OmiGraph, the first omission-aware framework for misinformation detection. Specifically, OmiGraph constructs an omission-aware graph for the target news by utilizing a contextual environment that captures complementary perspectives of the same event, thereby surfacing potentially omitted contents. Based on this graph, omission-oriented relation modeling is then proposed to identify the internal contextual dependencies, as well as the dynamic omission intents, formulating a comprehensive omission relation representation. Finally, to extract omission patterns for detection, OmiGraph introduces omission-aware message-passing and aggregation that establishes holistic deception perception by integrating the omission contents and relations. Experiments show that, by considering the omission perspective, our approach attains remarkable performance, achieving average improvements of +5.4% F1 and +5.3% ACC on two large-scale benchmarks.",
        "authors": [
            "Zhengjia Wang",
            "Danding Wang",
            "Qiang Sheng",
            "Jiaying Wu",
            "Juan Cao"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T14:37:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01728v1"
    },
    {
        "arxiv_id": "2512.01725v1",
        "title": "Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks",
        "abstract": "Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \\textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \\textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \\textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.",
        "authors": [
            "Jiannan Guan",
            "Qiguang Chen",
            "Libo Qin",
            "Dengyun Peng",
            "Jinhao Liu",
            "Liangyu Huo",
            "Jian Xie",
            "Wanxiang Che"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T14:35:06+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01725v1"
    },
    {
        "arxiv_id": "2512.01713v1",
        "title": "Self-Supervised Borrowing Detection on Multilingual Wordlists",
        "abstract": "This paper presents a fully self-supervised approach to borrowing detection in multilingual wordlists. The method combines two sources of information: PMI similarities based on a global correspondence model and a lightweight contrastive component trained on phonetic feature vectors. It further includes an automatic procedure for selecting decision thresholds without requiring labeled data. Experiments on benchmark datasets show that PMI alone already improves over existing string similarity measures such as NED and SCA, and that the combined similarity performs on par with or better than supervised baselines. An ablation study highlights the importance of character encoding, temperature settings and augmentation strategies. The approach scales to datasets of different sizes, works without manual supervision and is provided with a command-line tool that allows researchers to conduct their own studies.",
        "authors": [
            "Tim Wientzek"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T14:20:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01713v1"
    },
    {
        "arxiv_id": "2512.01710v1",
        "title": "MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications",
        "abstract": "Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.",
        "authors": [
            "Stefano Zeppieri"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T14:16:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01710v1"
    },
    {
        "arxiv_id": "2512.01707v1",
        "title": "StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos",
        "abstract": "Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.",
        "authors": [
            "Daeun Lee",
            "Subhojyoti Mukherjee",
            "Branislav Kveton",
            "Ryan A. Rossi",
            "Viet Dac Lai",
            "Seunghyun Yoon",
            "Trung Bui",
            "Franck Dernoncourt",
            "Mohit Bansal"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-12-01T14:15:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01707v1"
    },
    {
        "arxiv_id": "2512.01661v1",
        "title": "Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems",
        "abstract": "Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel \"Reverse Construction\" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.",
        "authors": [
            "Dengyun Peng",
            "Qiguang Chen",
            "Bofei Liu",
            "Jiannan Guan",
            "Libo Qin",
            "Zheng Yan",
            "Jinhao Liu",
            "Jianshu Zhang",
            "Wanxiang Che"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T13:32:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01661v1"
    },
    {
        "arxiv_id": "2512.01659v1",
        "title": "HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment",
        "abstract": "Legal AI systems powered by retrieval-augmented generation (RAG) face a critical accountability challenge: when an AI assistant cites case law, statutes, or contractual clauses, practitioners need verifiable guarantees that generated text faithfully represents source documents. Existing hallucination detectors rely on semantic similarity metrics that tolerate entity substitutions, a dangerous failure mode when confusing parties, dates, or legal provisions can have material consequences. We introduce HalluGraph, a graph-theoretic framework that quantifies hallucinations through structural alignment between knowledge graphs extracted from context, query, and response. Our approach produces bounded, interpretable metrics decomposed into \\textit{Entity Grounding} (EG), measuring whether entities in the response appear in source documents, and \\textit{Relation Preservation} (RP), verifying that asserted relationships are supported by context. On structured control documents, HalluGraph achieves near-perfect discrimination ($>$400 words, $>$20 entities), HalluGraph achieves $AUC = 0.979$, while maintaining robust performance ($AUC \\approx 0.89$) on challenging generative legal task, consistently outperforming semantic similarity baselines. The framework provides the transparency and traceability required for high-stakes legal applications, enabling full audit trails from generated assertions back to source passages.",
        "authors": [
            "Valentin No\u00ebl",
            "Elimane Yassine Seidou",
            "Charly Ken Capo-Chichi",
            "Ghanem Amari"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-12-01T13:31:06+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01659v1"
    },
    {
        "arxiv_id": "2512.01603v1",
        "title": "MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark",
        "abstract": "Spoken Language Understanding (SLU), which aims to extract user semantics to execute downstream tasks, is a crucial component of task-oriented dialog systems. Existing SLU datasets generally lack sufficient diversity and complexity, and there is an absence of a unified benchmark for the latest Large Language Models (LLMs) and Large Audio Language Models (LALMs). This work introduces MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset, which increases the difficulty of the SLU task by incorporating authentic and complex multi-intent data. Based on MAC-SLU, we conducted a comprehensive benchmark of leading open-source LLMs and LALMs, covering methods like in-context learning, supervised fine-tuning (SFT), and end-to-end (E2E) and pipeline paradigms. Our experiments show that while LLMs and LALMs have the potential to complete SLU tasks through in-context learning, their performance still lags significantly behind SFT. Meanwhile, E2E LALMs demonstrate performance comparable to pipeline approaches and effectively avoid error propagation from speech recognition. Code\\footnote{https://github.com/Gatsby-web/MAC\\_SLU} and datasets\\footnote{huggingface.co/datasets/Gatsby1984/MAC\\_SLU} are released publicly.",
        "authors": [
            "Yuezhang Peng",
            "Chonghao Cai",
            "Ziang Liu",
            "Shuai Fan",
            "Sheng Jiang",
            "Hua Xu",
            "Yuxin Liu",
            "Qiguang Chen",
            "Kele Xu",
            "Yao Li",
            "Sheng Wang",
            "Libo Qin",
            "Xie Chen"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T12:23:19+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01603v1"
    },
    {
        "arxiv_id": "2512.01568v1",
        "title": "Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism",
        "abstract": "We investigate whether Large Language Models (LLMs) exhibit altruistic tendencies, and critically, whether their implicit associations and self-reports predict actual altruistic behavior. Using a multi-method approach inspired by human social psychology, we tested 24 frontier LLMs across three paradigms: (1) an Implicit Association Test (IAT) measuring implicit altruism bias, (2) a forced binary choice task measuring behavioral altruism, and (3) a self-assessment scale measuring explicit altruism beliefs. Our key findings are: (1) All models show strong implicit pro-altruism bias (mean IAT = 0.87, p < .0001), confirming models \"know\" altruism is good. (2) Models behave more altruistically than chance (65.6% vs. 50%, p < .0001), but with substantial variation (48-85%). (3) Implicit associations do not predict behavior (r = .22, p = .29). (4) Most critically, models systematically overestimate their own altruism, claiming 77.5% altruism while acting at 65.6% (p < .0001, Cohen's d = 1.08). This \"virtue signaling gap\" affects 75% of models tested. Based on these findings, we recommend the Calibration Gap (the discrepancy between self-reported and behavioral values) as a standardized alignment metric. Well-calibrated models are more predictable and behaviorally consistent; only 12.5% of models achieve the ideal combination of high prosocial behavior and accurate self-knowledge.",
        "authors": [
            "Sandro Andric"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-12-01T11:43:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01568v1"
    },
    {
        "arxiv_id": "2512.01557v1",
        "title": "Language Diversity: Evaluating Language Usage and AI Performance on African Languages in Digital Spaces",
        "abstract": "This study examines the digital representation of African languages and the challenges this presents for current language detection tools. We evaluate their performance on Yoruba, Kinyarwanda, and Amharic. While these languages are spoken by millions, their online usage on conversational platforms is often sparse, heavily influenced by English, and not representative of the authentic, monolingual conversations prevalent among native speakers. This lack of readily available authentic data online creates a challenge of scarcity of conversational data for training language models. To investigate this, data was collected from subreddits and local news sources for each language. The analysis showed a stark contrast between the two sources. Reddit data was minimal and characterized by heavy code-switching. Conversely, local news media offered a robust source of clean, monolingual language data, which also prompted more user engagement in the local language on the news publishers social media pages. Language detection models, including the specialized AfroLID and a general LLM, performed with near-perfect accuracy on the clean news data but struggled with the code-switched Reddit posts. The study concludes that professionally curated news content is a more reliable and effective source for training context-rich AI models for African languages than data from conversational platforms. It also highlights the need for future models that can process clean and code-switched text to improve the detection accuracy for African languages.",
        "authors": [
            "Edward Ajayi",
            "Eudoxie Umwari",
            "Mawuli Deku",
            "Prosper Singadi",
            "Jules Udahemuka",
            "Bekalu Tadele",
            "Chukuemeka Edeh"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T11:27:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01557v1"
    },
    {
        "arxiv_id": "2512.01556v1",
        "title": "LEC: Linear Expectation Constraints for False-Discovery Control in Selective Prediction and Routing Systems",
        "abstract": "Large language models (LLMs) often generate unreliable answers, while heuristic uncertainty methods fail to fully distinguish correct from incorrect predictions, causing users to accept erroneous answers without statistical guarantees. We address this issue through the lens of false discovery rate (FDR) control, ensuring that among all accepted predictions, the proportion of errors does not exceed a target risk level. To achieve this in a principled way, we propose LEC, which reinterprets selective prediction as a constrained decision problem by enforcing a Linear Expectation Constraint over selection and error indicators. Then, we establish a finite-sample sufficient condition, which relies only on a held-out set of exchangeable calibration samples, to compute an FDR-constrained, coverage-maximizing threshold. Furthermore, we extend LEC to a two-model routing mechanism: given a prompt, if the current model's uncertainty exceeds its calibrated threshold, we delegate it to a stronger model, while maintaining a unified FDR guarantee. Evaluations on closed-ended and open-ended question-answering (QA) datasets show that LEC achieves tighter FDR control and substantially improves sample retention over prior methods. Moreover, the two-model routing mechanism achieves lower risk levels while accepting more correct samples than each individual model.",
        "authors": [
            "Zhiyuan Wang",
            "Aniri",
            "Tianlong Chen",
            "Yue Zhang",
            "Heng Tao Shen",
            "Xiaoshuang Shi",
            "Kaidi Xu"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-12-01T11:27:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01556v1"
    },
    {
        "arxiv_id": "2512.01546v1",
        "title": "LPCD: Unified Framework from Layer-Wise to Submodule Quantization",
        "abstract": "Post-training quantization (PTQ) aims to preserve model-level behavior; however, most methods focus on individual linear layers. Even recent extensions, such as QEP and LoaQ, which mitigate error propagation or target specific submodules, still rely on layer-wise formulations and fail to capture the behavior of larger submodules. We introduce Layer-Projected Coordinate Descent (LPCD), a unified framework that extends PTQ beyond layers by optimizing relaxed objectives across arbitrary submodules and projecting the solutions with layer-wise quantizers. LPCD generalizes existing methods and provides a principled approach to quantizing complex submodules while maintaining the efficiency and compatibility of layer-wise PTQ pipelines. Across diverse LLM architectures and bit-widths, LPCD-based submodule quantization consistently enhances both layer-wise PTQ methods and existing submodule approaches.",
        "authors": [
            "Yuma Ichikawa",
            "Yudai Fujimoto",
            "Akira Sakai"
        ],
        "category": "stat.ML",
        "submitted_date": "2025-12-01T11:21:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01546v1"
    },
    {
        "arxiv_id": "2512.01512v1",
        "title": "MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages",
        "abstract": "Multimodal Large Language Models (MLLMs) have achieved great success in Speech-to-Text Translation (S2TT) tasks. However, current research is constrained by two key challenges: language coverage and efficiency. Most of the popular S2TT datasets are substantially English-centric, which restricts the scaling-up of MLLMs' many-to-many translation capabilities. Moreover, the inference speed of MLLMs degrades dramatically when the speech is converted into long sequences (e.g., 750 tokens). To address these limitations, we propose a Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT) framework, which includes two innovations. First, a language scaling method that leverages curriculum learning and a data balancing strategy is introduced to extend the language coverage supported by MLLMs to 70 languages and achieve mutual translation among these languages. Second, an optimized speech adapter module is designed to reduce the length of the speech sequence to only 30 tokens. Extensive experiments were conducted on MLLMs of different scales (9B and 27B). The experimental results demonstrate that MCAT not only surpasses state-of-the-art end-to-end models on the FLEURS dataset across 70x69 directions but also enhances batch inference efficiency. This is achieved with only ~100M trainable parameters and by using only 10 hours of S2TT data per language. Furthermore, we have released MCAT as open-source to promote the development of MLLMs for robust S2TT capabilities. The code and models are released at https://github.com/yxduir/m2m-70.",
        "authors": [
            "Yexing Du",
            "Kaiyuan Liu",
            "Youcheng Pan",
            "Bo Yang",
            "Keqi Deng",
            "Xie Chen",
            "Yang Xiang",
            "Ming Liu",
            "Bin Qin",
            "YaoWei Wang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T10:39:12+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01512v1"
    },
    {
        "arxiv_id": "2512.01460v1",
        "title": "Enhancing BERT Fine-Tuning for Sentiment Analysis in Lower-Resourced Languages",
        "abstract": "Limited data for low-resource languages typically yield weaker language models (LMs). Since pre-training is compute-intensive, it is more pragmatic to target improvements during fine-tuning. In this work, we examine the use of Active Learning (AL) methods augmented by structured data selection strategies which we term 'Active Learning schedulers', to boost the fine-tuning process with a limited amount of training data. We connect the AL to data clustering and propose an integrated fine-tuning pipeline that systematically combines AL, clustering, and dynamic data selection schedulers to enhance model's performance. Experiments in the Slovak, Maltese, Icelandic and Turkish languages show that the use of clustering during the fine-tuning phase together with AL scheduling can simultaneously produce annotation savings up to 30% and performance improvements up to four F1 score points, while also providing better fine-tuning stability.",
        "authors": [
            "Jozef Kub\u00edk",
            "Marek \u0160uppa",
            "Martin Tak\u00e1\u010d"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T09:45:47+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01460v1"
    },
    {
        "arxiv_id": "2512.01457v1",
        "title": "ZIP-RC: Zero-overhead Inference-time Prediction of Reward and Cost for Adaptive and Interpretable Generation",
        "abstract": "Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.",
        "authors": [
            "Rohin Manvi",
            "Joey Hong",
            "Tim Seyde",
            "Maxime Labonne",
            "Mathias Lechner",
            "Sergey Levine"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-12-01T09:44:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01457v1"
    },
    {
        "arxiv_id": "2512.01443v1",
        "title": "MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification",
        "abstract": "We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments.",
        "authors": [
            "Xabier de Zuazo",
            "Ibon Saratxaga",
            "Eva Navas"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T09:25:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01443v1"
    },
    {
        "arxiv_id": "2512.01439v1",
        "title": "Multilingual Conversational AI for Financial Assistance: Bridging Language Barriers in Indian FinTech",
        "abstract": "India's linguistic diversity presents both opportunities and challenges for fintech platforms. While the country has 31 major languages and over 100 minor ones, only 10\\% of the population understands English, creating barriers to financial inclusion. We present a multilingual conversational AI system for a financial assistance use case that supports code-mixed languages like Hinglish, enabling natural interactions for India's diverse user base. Our system employs a multi-agent architecture with language classification, function management, and multilingual response generation. Through comparative analysis of multiple language models and real-world deployment, we demonstrate significant improvements in user engagement while maintaining low latency overhead (4-8\\%). This work contributes to bridging the language gap in digital financial services for emerging markets.",
        "authors": [
            "Bharatdeep Hazarika",
            "Arya Suneesh",
            "Prasanna Devadiga",
            "Pawan Kumar Rajpoot",
            "Anshuman B Suresh",
            "Ahmed Ifthaquar Hussain"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T09:23:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01439v1"
    },
    {
        "arxiv_id": "2512.01420v1",
        "title": "PromptBridge: Cross-Model Prompt Transfer for Large Language Models",
        "abstract": "Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.",
        "authors": [
            "Yaxuan Wang",
            "Quan Liu",
            "Zhenting Wang",
            "Zichao Li",
            "Wei Wei",
            "Yang Liu",
            "Yujia Bao"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T08:55:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01420v1"
    },
    {
        "arxiv_id": "2512.01410v1",
        "title": "DyFuLM: An Advanced Multimodal Framework for Sentiment Analysis",
        "abstract": "Understanding sentiment in complex textual expressions remains a fundamental challenge in affective computing. To address this, we propose a Dynamic Fusion Learning Model (DyFuLM), a multimodal framework designed to capture both hierarchical semantic representations and fine-grained emotional nuances. DyFuLM introduces two key moodules: a Hierarchical Dynamic Fusion module that adaptively integrates multi-level features, and a Gated Feature Aggregation module that regulates cross-layer information ffow to achieve balanced representation learning. Comprehensive experiments on multi-task sentiment datasets demonstrate that DyFuLM achieves 82.64% coarse-grained and 68.48% fine-grained accuracy, yielding the lowest regression errors (MAE = 0.0674, MSE = 0.0082) and the highest R^2 coefficient of determination (R^2= 0.6903). Furthermore, the ablation study validates the effectiveness of each module in DyFuLM. When all modules are removed, the accuracy drops by 0.91% for coarse-grained and 0.68% for fine-grained tasks. Keeping only the gated fusion module causes decreases of 0.75% and 0.55%, while removing the dynamic loss mechanism results in drops of 0.78% and 0.26% for coarse-grained and fine-grained sentiment classification, respectively. These results demonstrate that each module contributes significantly to feature interaction and task balance. Overall, the experimental findings further validate that DyFuLM enhances sentiment representation and overall performance through effective hierarchical feature fusion.",
        "authors": [
            "Ruohan Zhou",
            "Jiachen Yuan",
            "Churui Yang",
            "Wenzheng Huang",
            "Guoyan Zhang",
            "Shiyao Wei",
            "Jiazhen Hu",
            "Ning Xin",
            "Md Maruf Hasan"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T08:30:10+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01410v1"
    },
    {
        "arxiv_id": "2512.01396v1",
        "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches",
        "abstract": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.   To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.",
        "authors": [
            "Zhiqing Zhong",
            "Jiaming Huang",
            "Pinjia He"
        ],
        "category": "cs.SE",
        "submitted_date": "2025-12-01T08:16:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01396v1"
    },
    {
        "arxiv_id": "2512.01374v2",
        "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices",
        "abstract": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.",
        "authors": [
            "Chujie Zheng",
            "Kai Dang",
            "Bowen Yu",
            "Mingze Li",
            "Huiqiang Jiang",
            "Junrong Lin",
            "Yuqiong Liu",
            "Hao Lin",
            "Chencan Wu",
            "Feng Hu",
            "An Yang",
            "Jingren Zhou",
            "Junyang Lin"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-12-01T07:45:39+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01374v2"
    },
    {
        "arxiv_id": "2512.01369v1",
        "title": "MARSAD: A Multi-Functional Tool for Real-Time Social Media Analysis",
        "abstract": "MARSAD is a multifunctional natural language processing (NLP) platform designed for real-time social media monitoring and analysis, with a particular focus on the Arabic-speaking world. It enables researchers and non-technical users alike to examine both live and archived social media content, producing detailed visualizations and reports across various dimensions, including sentiment analysis, emotion analysis, propaganda detection, fact-checking, and hate speech detection. The platform also provides secure data-scraping capabilities through API keys for accessing public social media data. MARSAD's backend architecture integrates flexible document storage with structured data management, ensuring efficient processing of large and multimodal datasets. Its user-friendly frontend supports seamless data upload and interaction.",
        "authors": [
            "Md. Rafiul Biswas",
            "Firoj Alam",
            "Wajdi Zaghouani"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T07:31:37+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01369v1"
    },
    {
        "arxiv_id": "2512.01354v2",
        "title": "The Necessity of Imperfection:Reversing Model Collapse via Simulating Cognitive Boundedness",
        "abstract": "Although synthetic data is widely promoted as a remedy, its prevailing production paradigm -- one optimizing for statistical smoothness -- systematically removes the long-tail, cognitively grounded irregularities that characterize human text. Prolonged training on such statistically optimal but cognitively impoverished data accelerates model collapse.   This paper proposes a paradigm shift: instead of imitating the surface properties of data, we simulate the cognitive processes that generate human text. We introduce the Prompt-driven Cognitive Computing Framework (PMCSF), whose core consists of a Cognitive State Decoder (CSD) that reverse-engineers unstructured text into structured cognitive vectors, and a Cognitive Text Encoder (CTE) that re-materializes these states into text enriched with human-typical imperfections via mathematically defined Cognitive Perturbation Operators.   The framework is validated through a two-stage objective evaluation pipeline. First, in cognitive codec verification, CTE text yields a Jensen-Shannon divergence of 0.0614 from human text (vs. 0.4431 for standard LLM output), passes double-blind professional media review, and achieves an intraclass correlation coefficient ICC > 0.9 for cognitive profile alignment across heterogeneous models. Second, in functional gain evaluation, isomorphic stress tests in the A-share market show that strategies incorporating CTE-generated data reduce maximum drawdown by 47.4% during the 2015 crash and deliver 8.6% Defensive Alpha, exceeding transaction costs by a factor of 33.   Our findings demonstrate that modelling human cognitive limitations -- not copying surface data -- enables synthetic data with genuine functional gain, offering a viable technical pathway toward resolving the AI data-collapse crisis.",
        "authors": [
            "Zhongjie Jiang"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-12-01T07:09:38+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01354v2"
    },
    {
        "arxiv_id": "2512.01335v1",
        "title": "EmoRAG: Evaluating RAG Robustness to Symbolic Perturbations",
        "abstract": "Retrieval-Augmented Generation (RAG) systems are increasingly central to robust AI, enhancing large language model (LLM) faithfulness by incorporating external knowledge. However, our study unveils a critical, overlooked vulnerability: their profound susceptibility to subtle symbolic perturbations, particularly through near-imperceptible emoticon tokens such as \"(@_@)\" that can catastrophically mislead retrieval, termed EmoRAG. We demonstrate that injecting a single emoticon into a query makes it nearly 100% likely to retrieve semantically unrelated texts that contain a matching emoticon. Our extensive experiment across general question-answering and code domains, using a range of state-of-the-art retrievers and generators, reveals three key findings: (I) Single-Emoticon Disaster: Minimal emoticon injections cause maximal disruptions, with a single emoticon almost 100% dominating RAG output. (II) Positional Sensitivity: Placing an emoticon at the beginning of a query can cause severe perturbation, with F1-Scores exceeding 0.92 across all datasets. (III) Parameter-Scale Vulnerability: Counterintuitively, models with larger parameters exhibit greater vulnerability to the interference. We provide an in-depth analysis to uncover the underlying mechanisms of these phenomena. Furthermore, we raise a critical concern regarding the robustness assumption of current RAG systems, envisioning a threat scenario where an adversary exploits this vulnerability to manipulate the RAG system. We evaluate standard defenses and find them insufficient against EmoRAG. To address this, we propose targeted defenses, analyzing their strengths and limitations in mitigating emoticon-based perturbations. Finally, we outline future directions for building robust RAG systems.",
        "authors": [
            "Xinyun Zhou",
            "Xinfeng Li",
            "Yinan Peng",
            "Ming Xu",
            "Xuanwang Zhang",
            "Miao Yu",
            "Yidong Wang",
            "Xiaojun Jia",
            "Kun Wang",
            "Qingsong Wen",
            "XiaoFeng Wang",
            "Wei Dong"
        ],
        "category": "cs.CR",
        "submitted_date": "2025-12-01T06:53:49+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01335v1"
    },
    {
        "arxiv_id": "2512.01326v1",
        "title": "Securing Large Language Models (LLMs) from Prompt Injection Attacks",
        "abstract": "Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-following ability to make it perform malicious tasks. Recent work has proposed JATMO, a task-specific fine-tuning approach that trains non-instruction-tuned base models to perform a single function, thereby reducing susceptibility to adversarial instructions. In this study, we evaluate the robustness of JATMO against HOUYI, a genetic attack framework that systematically mutates and optimizes adversarial prompts. We adapt HOUYI by introducing custom fitness scoring, modified mutation logic, and a new harness for local model testing, enabling a more accurate assessment of defense effectiveness. We fine-tuned LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B models under the JATMO methodology and compared them with a fine-tuned GPT-3.5-Turbo baseline. Results show that while JATMO reduces attack success rates relative to instruction-tuned models, it does not fully prevent injections; adversaries exploiting multilingual cues or code-related disruptors still bypass defenses. We also observe a trade-off between generation quality and injection vulnerability, suggesting that better task performance often correlates with increased susceptibility. Our results highlight both the promise and limitations of fine-tuning-based defenses and point toward the need for layered, adversarially informed mitigation strategies.",
        "authors": [
            "Omar Farooq Khan Suri",
            "John McCrae"
        ],
        "category": "cs.CR",
        "submitted_date": "2025-12-01T06:34:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01326v1"
    },
    {
        "arxiv_id": "2512.01316v1",
        "title": "Agreement-Constrained Probabilistic Minimum Bayes Risk Decoding",
        "abstract": "Minimum Bayes risk (MBR) decoding generates high-quality translations by maximizing the expected utility of output candidates, but it evaluates all pairwise scores over the candidate set; hence, it takes quadratic time with respect to the number of candidates. To reduce the number of utility function calls, probabilistic MBR (PMBR) decoding partially evaluates quality scores using sampled pairs of candidates and completes the missing scores with a matrix completion algorithm. Nevertheless, it degrades the translation quality as the number of utility function calls is reduced. Therefore, to improve the trade-off between quality and cost, we propose agreement-constrained PMBR (AC-PMBR) decoding, which leverages a knowledge distilled model to guide the completion of the score matrix. Our AC-PMBR decoding improved approximation errors of matrix completion by up to 3 times and achieved higher translation quality compared with PMBR decoding at a comparable computational cost on the WMT'23 En$\\leftrightarrow$De translation tasks.",
        "authors": [
            "Koki Natsumi",
            "Hiroyuki Deguchi",
            "Yusuke Sakai",
            "Hidetaka Kamigaito",
            "Taro Watanabe"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T06:16:47+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01316v1"
    },
    {
        "arxiv_id": "2512.01282v2",
        "title": "Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning",
        "abstract": "As web platforms evolve towards greater personalization and emotional complexity, conversational agents must transcend superficial empathy to demonstrate identity-aware emotional reasoning. However, existing systems face two limitations: (1) reliance on situation-centric datasets lacking persistent user identity, which hampers the capture of personalized affective nuances; and (2) dependence on opaque, coarse reward signals that hinder development of verifiable empathetic reasoning. To address these gaps, we introduce KardiaBench, a large-scale user-grounded benchmark comprising 178,080 QA pairs across 22,080 multi-turn conversations anchored to 671 real-world profiles. The dataset is constructed via a model-in-the-loop pipeline with iterative rubric-guided refinement to ensure psychological plausibility and persona consistency. This progressive empathy pipeline that integrates user comprehension, contextual reasoning, and emotion perception into conversations, followed by iterative critique and rubric-based refinement to ensure psychological plausibility, emotional fidelity, and persona consistency. Building on this, we propose Kardia-R1, a framework that trains models for interpretable, stepwise empathetic cognition. Kardia-R1 leverages Rubric-as-Judge Empathetic Reinforcement Learning (Rubric-ERL), a GRPO-based method that uses explainable, human-aligned rubric rewards to tightly couple user understanding, emotional inference, and supportive response generation. Extensive experiments across four LLM backbones demonstrate that Kardia-R1 consistently outperforms othet methods in emotion accuracy, empathy, relevance, persona consistency, and safety. Our dataset and model will be released at https://github.com/JhCircle/Kardia-R1.",
        "authors": [
            "Jiahao Yuan",
            "Zhiqing Cui",
            "Hanqing Wang",
            "Yuansheng Gao",
            "Yucheng Zhou",
            "Usman Naseem"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T04:54:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01282v2"
    },
    {
        "arxiv_id": "2512.01274v1",
        "title": "SUPERChem: A Multimodal Reasoning Benchmark in Chemistry",
        "abstract": "Current benchmarks for evaluating the chemical reasoning capabilities of Large Language Models (LLMs) are limited by oversimplified tasks, lack of process-level evaluation, and misalignment with expert-level chemistry skills. To address these issues, we introduce SUPERChem, a benchmark of 500 expert-curated reasoning-intensive chemistry problems, covering diverse subfields and provided in both multimodal and text-only formats. Original content and an iterative curation pipeline eliminate flawed items and mitigate data contamination. Each problem is paired with an expert-authored solution path, enabling Reasoning Path Fidelity (RPF) scoring to evaluate reasoning quality beyond final-answer accuracy. Evaluations against a human baseline of 40.3% accuracy show that even the best-performing model, GPT-5 (High), reaches only 38.5%, followed closely by Gemini 2.5 Pro (37.9%) and DeepSeek-V3.1-Think (37.3%). SUPERChem elicits multi-step, multimodal reasoning, reveals model-dependent effects of visual information, and distinguishes high-fidelity reasoners from heuristic ones. By providing a challenging benchmark and a reliable evaluation framework, SUPERChem aims to facilitate the advancement of LLMs toward expert-level chemical intelligence. The dataset of the benchmark is available at https://huggingface.co/datasets/ZehuaZhao/SUPERChem.",
        "authors": [
            "Zehua Zhao",
            "Zhixian Huang",
            "Junren Li",
            "Siyu Lin",
            "Junting Zhou",
            "Fengqi Cao",
            "Kun Zhou",
            "Rui Ge",
            "Tingting Long",
            "Yuexiang Zhu",
            "Yan Liu",
            "Jie Zheng",
            "Junnian Wei",
            "Rong Zhu",
            "Peng Zou",
            "Wenyu Li",
            "Zekai Cheng",
            "Tian Ding",
            "Yaxuan Wang",
            "Yizhao Yan",
            "Tingru Wei",
            "Haowei Ming",
            "Weijie Mao",
            "Chen Sun",
            "Yiming Liu",
            "Zichen Wang",
            "Zuo Zhang",
            "Tong Yang",
            "Hao Ma",
            "Zhen Gao",
            "Jian Pei"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T04:46:35+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01274v1"
    },
    {
        "arxiv_id": "2512.01256v1",
        "title": "Sentiment Analysis and Emotion Classification using Machine Learning Techniques for Nagamese Language - A Low-resource Language",
        "abstract": "The Nagamese language, a.k.a Naga Pidgin, is an Assamese-lexified creole language developed primarily as a means of communication in trade between the people from Nagaland and people from Assam in the north-east India. Substantial amount of work in sentiment analysis has been done for resource-rich languages like English, Hindi, etc. However, no work has been done in Nagamese language. To the best of our knowledge, this is the first attempt on sentiment analysis and emotion classification for the Nagamese Language. The aim of this work is to detect sentiments in terms of polarity (positive, negative and neutral) and basic emotions contained in textual content of Nagamese language. We build sentiment polarity lexicon of 1,195 nagamese words and use these to build features along with additional features for supervised machine learning techniques using Na\"ive Bayes and Support Vector Machines.   Keywords: Nagamese, NLP, sentiment analysis, machine learning",
        "authors": [
            "Ekha Morang",
            "Surhoni A. Ngullie",
            "Sashienla Longkumer",
            "Teisovi Angami"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T04:01:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01256v1"
    },
    {
        "arxiv_id": "2512.01255v1",
        "title": "Large Language Models Cannot Reliably Detect Vulnerabilities in JavaScript: The First Systematic Benchmark and Evaluation",
        "abstract": "Researchers have proposed numerous methods to detect vulnerabilities in JavaScript, especially those assisted by Large Language Models (LLMs). However, the actual capability of LLMs in JavaScript vulnerability detection remains questionable, necessitating systematic evaluation and comprehensive benchmarks. Unfortunately, existing benchmarks suffer from three critical limitations: (1) incomplete coverage, such as covering a limited subset of CWE types; (2) underestimation of LLM capabilities caused by unreasonable ground truth labeling; and (3) overestimation due to unrealistic cases such as using isolated vulnerable files rather than complete projects.   In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation. Guided by these principles, we propose FORGEJS, the first automatic benchmark generation framework for evaluating LLMs' capability in JavaScript vulnerability detection. Then, we use FORGEJS to construct ARENAJS-the first systematic benchmark for LLM-based JavaScript vulnerability detection-and further propose JUDGEJS, an automatic evaluation framework.   We conduct the first systematic evaluation of LLMs for JavaScript vulnerability detection, leveraging JUDGEJS to assess seven popular commercial LLMs on ARENAJS. The results show that LLMs not only exhibit limited reasoning capabilities, but also suffer from severe robustness defects, indicating that reliable JavaScript vulnerability detection with LLMs remains an open challenge.",
        "authors": [
            "Qingyuan Fei",
            "Xin Liu",
            "Song Li",
            "Shujiang Wu",
            "Jianwei Hou",
            "Ping Chen",
            "Zifeng Kang"
        ],
        "category": "cs.CR",
        "submitted_date": "2025-12-01T04:00:06+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01255v1"
    },
    {
        "arxiv_id": "2512.01242v1",
        "title": "Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation",
        "abstract": "We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.",
        "authors": [
            "Zirui Zhao",
            "Boye Niu",
            "David Hsu",
            "Wee Sun Lee"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-12-01T03:38:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01242v1"
    },
    {
        "arxiv_id": "2512.01208v1",
        "title": "Pay Attention Later: From Vector Space Diffusion to Linearithmic Spectral Phase-Locking",
        "abstract": "Standard Transformers suffer from a \"Semantic Alignment Tax\", a prohibitive optimization cost required to organize a chaotic initialization into a coherent geometric map via local gradient diffusion. We hypothesize that this reliance on diffusive learning creates \"Catastrophic Rigidity\", rendering models unable to adapt to novel concepts without destroying their pre-trained reasoning capabilities. To isolate this phenomenon, we introduce Iterative Semantic Map Refinement (ISMR), a diagnostic protocol revealing that alignment is a fixed geometric barrier that scaling cannot solve; a 20-layer model overcomes this barrier no faster than a 1-layer model. We introduce the Phase-Resonant Intelligent Spectral Model (PRISM). PRISM encodes semantic identity as resonant frequencies in the complex domain (C^d) and replaces quadratic self-attention with linearithmic O(N log N) Gated Harmonic Convolutions. We validate PRISM on the WMT14 translation task. While the Standard Transformer maintains a slight edge in general competence on static benchmarks (23.88 vs 21.40 BLEU), it fails the \"Plasticity-Stability\" stress test completely. When injected with novel concepts, the Transformer suffers Catastrophic Forgetting, degrading by -10.55 BLEU points while achieving only 60% acquisition. In contrast, PRISM demonstrates Lossless Plasticity, achieving 96% 5-shot acquisition with negligible degradation (-0.84 BLEU). These results suggest that harmonic representations effectively decouple memory from reasoning, offering a structural solution to the plasticity-stability dilemma in real-time knowledge adaptation.",
        "authors": [
            "Alper Y\u0131ld\u0131r\u0131m",
            "\u0130brahim Y\u00fcceda\u011f"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-12-01T02:46:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01208v1"
    },
    {
        "arxiv_id": "2512.01198v1",
        "title": "Conveying Imagistic Thinking in Traditional Chinese Medicine Translation: A Prompt Engineering and LLM-Based Evaluation Framework",
        "abstract": "Traditional Chinese Medicine (TCM) theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop (HITL) framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis (IPA). Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient, and replicable HITL methodological pathway for the translation of ancient, concept-dense texts such as TCM.",
        "authors": [
            "Jiatong Han"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T02:27:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01198v1"
    },
    {
        "arxiv_id": "2512.01191v1",
        "title": "Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks",
        "abstract": "Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.",
        "authors": [
            "Krithik Vishwanath",
            "Mrigayu Ghosh",
            "Anton Alyakin",
            "Daniel Alexander Alber",
            "Yindalon Aphinyanaphongs",
            "Eric Karl Oermann"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T02:14:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01191v1"
    },
    {
        "arxiv_id": "2512.01183v1",
        "title": "TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness",
        "abstract": "The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.",
        "authors": [
            "Yongxin Zhou",
            "Philippe Mulhem",
            "Didier Schwab"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T01:46:36+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01183v1"
    },
    {
        "arxiv_id": "2512.01174v1",
        "title": "DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks",
        "abstract": "As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-based assessment: 8 objective criteria enable reproducible scoring, while action-level inspection allows stakeholders to audit agent behavior. Our framework comprises 250 diverse prompts across 20 categories and 4 difficulty levels, deterministic evaluation metrics, and an external oversight mechanism through multi-turn feedback that enables human control over agent refinement. Evaluating four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests, we establish both capabilities and limitations: models achieved 92.8% perfect performance with structured external feedback driving significant improvements (average +3.2%, up to +32.8% for complex scenes), but systematic error patterns emerged in tool state management and long-horizon planning. Notably, specification clarity proved more important than task complexity -- models achieved 100% perfect performance when given explicit, verifiable criteria. These findings demonstrate that transparent evaluation frameworks can establish trust in agentic systems, with external oversight proving more reliable than self-correction for guiding agent behavior. Our open-source framework provides a template for trustworthy agent assessment. Code and data: https://github.com/hyunjun1121/DrawingBench",
        "authors": [
            "Hyunjun Kim",
            "Sooyoung Ryu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-12-01T01:18:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01174v1"
    },
    {
        "arxiv_id": "2512.01127v1",
        "title": "Mode-Conditioning Unlocks Superior Test-Time Scaling",
        "abstract": "Parallel sampling promises substantial gains in test-time scaling, but its effectiveness is sharply limited by diversity collapse, where models concentrate on a few modes and repeated samples produce the same mistakes. We propose the mode-conditioning (ModC) framework, which explicitly allocates test-time compute across reasoning modes using either specialist models or mode-specific prefixes. ModC consistently improves scaling across controlled graph-search tasks and large-scale reasoning benchmarks, spanning model families and sizes from 0.5B to 7B. On OpenThoughts, fine-tuning Qwen2.5-7B with ModC achieves a 4x efficiency gain over standard training while also improving the maximum attainable Pass@k. We further show that gradient clustering enables ModC without explicit mode labels, yielding up to 10% gains on datasets such as NuminaMath. Finally, we show that ModC improves reinforcement learning (RL) and can further boost diversity-inducing RL methods. These results demonstrate that standard training underutilizes the diversity in data, and that ModC provides a simple, effective remedy for unlocking the full benefits of diversity in test-time scaling.",
        "authors": [
            "Chen Henry Wu",
            "Sachin Goyal",
            "Aditi Raghunathan"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-30T22:36:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01127v1"
    },
    {
        "arxiv_id": "2512.01109v1",
        "title": "How do we measure privacy in text? A survey of text anonymization metrics",
        "abstract": "In this work, we aim to clarify and reconcile metrics for evaluating privacy protection in text through a systematic survey. Although text anonymization is essential for enabling NLP research and model development in domains with sensitive data, evaluating whether anonymization methods sufficiently protect privacy remains an open challenge. In manually reviewing 47 papers that report privacy metrics, we identify and compare six distinct privacy notions, and analyze how the associated metrics capture different aspects of privacy risk. We then assess how well these notions align with legal privacy standards (HIPAA and GDPR), as well as user-centered expectations grounded in HCI studies. Our analysis offers practical guidance on navigating the landscape of privacy evaluation approaches further and highlights gaps in current practices. Ultimately, we aim to facilitate more robust, comparable, and legally aware privacy evaluations in text anonymization.",
        "authors": [
            "Yaxuan Ren",
            "Krithika Ramesh",
            "Yaxing Yao",
            "Anjalie Field"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T22:12:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01109v1"
    },
    {
        "arxiv_id": "2512.01085v1",
        "title": "Generalized Medical Phrase Grounding",
        "abstract": "Medical phrase grounding (MPG) maps textual descriptions of radiological findings to corresponding image regions. These grounded reports are easier to interpret, especially for non-experts. Existing MPG systems mostly follow the referring expression comprehension (REC) paradigm and return exactly one bounding box per phrase. Real reports often violate this assumption. They contain multi-region findings, non-diagnostic text, and non-groundable phrases, such as negations or descriptions of normal anatomy. Motivated by this, we reformulate the task as generalised medical phrase grounding (GMPG), where each sentence is mapped to zero, one, or multiple scored regions. To realise this formulation, we introduce the first GMPG model: MedGrounder. We adopted a two-stage training regime: pre-training on report sentence--anatomy box alignment datasets and fine-tuning on report sentence--human annotated box datasets. Experiments on PadChest-GR and MS-CXR show that MedGrounder achieves strong zero-shot transfer and outperforms REC-style and grounded report generation baselines on multi-region and non-groundable phrases, while using far fewer human box annotations. Finally, we show that MedGrounder can be composed with existing report generators to produce grounded reports without retraining the generator.",
        "authors": [
            "Wenjun Zhang",
            "Shekhar S. Chandra",
            "Aaron Nicolson"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-30T21:09:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01085v1"
    },
    {
        "arxiv_id": "2512.01081v1",
        "title": "Testing the Machine Consciousness Hypothesis",
        "abstract": "The Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control.",
        "authors": [
            "Stephen Fitz"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-30T21:05:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01081v1"
    },
    {
        "arxiv_id": "2512.01077v1",
        "title": "ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages",
        "abstract": "We present a culturally-grounded multimodal dataset of 1,060 traditional recipes crowdsourced from rural communities across remote regions of Eastern India, spanning 10 endangered languages. These recipes, rich in linguistic and cultural nuance, were collected using a mobile interface designed for contributors with low digital literacy. Endangered Language Recipes (ELR)-1000 -- captures not only culinary practices but also the socio-cultural context embedded in indigenous food traditions. We evaluate the performance of several state-of-the-art large language models (LLMs) on translating these recipes into English and find the following: despite the models' capabilities, they struggle with low-resource, culturally-specific language. However, we observe that providing targeted context -- including background information about the languages, translation examples, and guidelines for cultural preservation -- leads to significant improvements in translation quality. Our results underscore the need for benchmarks that cater to underrepresented languages and domains to advance equitable and culturally-aware language technologies. As part of this work, we release the ELR-1000 dataset to the NLP community, hoping it motivates the development of language technologies for endangered languages.",
        "authors": [
            "Neha Joshi",
            "Pamir Gogoi",
            "Aasim Mirza",
            "Aayush Jansari",
            "Aditya Yadavalli",
            "Ayushi Pandey",
            "Arunima Shukla",
            "Deepthi Sudharsan",
            "Kalika Bali",
            "Vivek Seshadri"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T20:51:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01077v1"
    },
    {
        "arxiv_id": "2512.01037v1",
        "title": "When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals",
        "abstract": "Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce \"semantic confusion,\" a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.",
        "authors": [
            "Riad Ahmed Anonto",
            "Md Labid Al Nahiyan",
            "Md Tanvir Hassan",
            "Ch. Md. Rakin Haider"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T19:11:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01037v1"
    },
    {
        "arxiv_id": "2512.01033v1",
        "title": "Associative Syntax and Maximal Repetitions reveal context-dependent complexity in fruit bat communication",
        "abstract": "This study presents an unsupervised method to infer discreteness, syntax and temporal structures of fruit-bats vocalizations, as a case study of graded vocal systems, and evaluates the complexity of communication patterns in relation with behavioral context. The method improved the baseline for unsupervised labeling of vocal units (i.e. syllables) through manifold learning, by investigating how dimen- sionality reduction on mel-spectrograms affects labeling, and comparing it with unsupervised labels based on acoustic similarity. We then encoded vocalizations as syllabic sequences to analyze the type of syntax, and extracted the Maximal Repetitions (MRs) to evaluate syntactical structures. We found evidence for: i) associative syntax, rather than combinatorial (context classification is unaffected by permutation of sequences, F 1 > 0.9); ii) context-dependent use of syllables (Wilcoxon rank-sum tests, p-value < 0.05); iii) heavy-tail distribution of MRs (truncated power-law, exponent \u03b1 < 2), indicative of mechanism encoding com- binatorial complexity. Analysis of MRs and syllabic transition networks revealed that mother-pupil interactions were characterized by repetitions, while commu- nication in conflict-contexts exhibited higher complexity (longer MRs and more interconnected vocal sequences) than non-agonistic contexts. We propose that communicative complexity is higher in scenarios of disagreement, reflecting lower compressibility of information.",
        "authors": [
            "Luigi Assom"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-30T19:01:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01033v1"
    },
    {
        "arxiv_id": "2512.01020v1",
        "title": "Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics",
        "abstract": "Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.",
        "authors": [
            "Jinu Lee",
            "Kyoung-Woon On",
            "Simeng Han",
            "Arman Cohan",
            "Julia Hockenmaier"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-30T18:32:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.01020v1"
    },
    {
        "arxiv_id": "2512.00991v1",
        "title": "Advancing Academic Chatbots: Evaluation of Non Traditional Outputs",
        "abstract": "Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.",
        "authors": [
            "Nicole Favero",
            "Francesca Salute",
            "Daniel Hardt"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T17:25:23+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00991v1"
    },
    {
        "arxiv_id": "2512.00986v1",
        "title": "Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent",
        "abstract": "The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge. First, existing benchmarks often focus narrowly on retrieval while neglecting high-level planning and reasoning. Second, existing benchmarks favor general domains over the scientific domains that are the core application for DR agents. To address these gaps, we introduce Dr.Mi-Bench, a Modular-integrated benchmark for scientific DR agents. Grounded in academic literature, our benchmark uses a human-annotated dataset of 200 instances across 10 scientific domains, including both research and review papers. Besides, we also propose a Modular-integrated Evaluation Paradigm for DR Agents (Dr.Mi-Eval), a novel modular-integrated evaluation paradigm, which leverages the rich structure of academic papers to assess the core competencies of planning, retrieval, and reasoning through two complementary modes: an end-to-end evaluation for DR agents and an isolated evaluation for foundational LLMs as potential backbones. Experimental results reveal a fragmented performance landscape: agents exhibit specialized strengths but share critical weaknesses, most notably in performing the multi-source retrieval required for review-style tasks and performing consistently across diverse scientific fields. Moreover, improving high-level planning capability is the crucial factor for unlocking the reasoning potential of foundational LLMs as backbones. By exposing these actionable failure modes, Dr.Mi-Bench provides a diagnostic tool to guide the development of more reliable academic research assistants.",
        "authors": [
            "Zhihan Guo",
            "Feiyang Xu",
            "Yifan Li",
            "Muzhi Li",
            "Shuai Zou",
            "Jiele Wu",
            "Han Shi",
            "Haoli Bai",
            "Ho-fung Leung",
            "Irwin King"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T17:16:47+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00986v1"
    },
    {
        "arxiv_id": "2512.00947v1",
        "title": "Table as a Modality for Large Language Models",
        "abstract": "To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of 42.65%.",
        "authors": [
            "Liyao Li",
            "Chao Ye",
            "Wentao Ye",
            "Yifei Sun",
            "Zhe Jiang",
            "Haobo Wang",
            "Jiaming Tian",
            "Yiming Zhang",
            "Ningtao Wang",
            "Xing Fu",
            "Gang Chen",
            "Junbo Zhao"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T15:59:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00947v1"
    },
    {
        "arxiv_id": "2512.00946v1",
        "title": "Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data",
        "abstract": "Large language models (LLMs) play an increasingly important role in finan- cial markets analysis by capturing signals from complex and heterogeneous textual data sources, such as tweets, news articles, reports, and microblogs. However, their performance is dependent on large computational resources and proprietary datasets, which are costly, restricted, and therefore inacces- sible to many researchers and practitioners. To reflect realistic situations we investigate the ability of lightweight open-source LLMs - smaller and publicly available models designed to operate with limited computational resources - to generalize sentiment understanding from financial datasets of varying sizes, sources, formats, and languages. We compare the benchmark finance natural language processing (NLP) model, FinBERT, and three open-source lightweight LLMs, DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B on five publicly available datasets: FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment and Chinese Finance Sentiment. We find that LLMs, specially Qwen3 8B and Llama3 8B, perform best in most scenarios, even from using only 5% of the available training data. These results hold in zero-shot and few-shot learning scenarios. Our findings indicate that lightweight, open-source large language models (LLMs) consti- tute a cost-effective option, as they can achieve competitive performance on heterogeneous textual data even when trained on only a limited subset of the extensive annotated corpora that are typically deemed necessary.",
        "authors": [
            "Alvaro Paredes Amorin",
            "Andre Python",
            "Christoph Weisser"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T15:58:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00946v1"
    },
    {
        "arxiv_id": "2512.00938v1",
        "title": "DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics",
        "abstract": "Transformer models have significantly advanced Natural Language Processing (NLP), demonstrating strong performance in English. However, their effectiveness in Arabic, particularly for Named Entity Recognition (NER), remains limited, even with larger pre-trained models. This performance gap stems from multiple factors, including tokenisation, dataset quality, and annotation inconsistencies. Existing studies often analyze these issues in isolation, failing to capture their joint effect on system behaviour and performance.   We introduce DeformAr (Debugging and Evaluation Framework for Transformer-based NER Systems), a novel framework designed to investigate and explain the performance discrepancy between Arabic and English NER systems. DeformAr integrates a data extraction library and an interactive dashboard, supporting two modes of evaluation: cross-component analysis and behavioural analysis. The framework divides each language into dataset and model components to examine their interactions.   The analysis proceeds in two stages. First, cross-component analysis provides systematic diagnostic measures across data and model subcomponents, addressing the \"what,\" \"how,\" and \"why\" behind observed discrepancies. The second stage applies behavioural analysis by combining interpretability techniques with token-level metrics, interactive visualisations, and representation space analysis. These stages enable a component-aware diagnostic process that detects model behaviours and explains them by linking them to underlying representational patterns and data factors. DeformAr is the first Arabic-specific, component-based interpretability tool, offering a crucial resource for advancing model analysis in under-resourced languages.",
        "authors": [
            "Ahmed Mustafa Younes"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T15:39:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00938v1"
    },
    {
        "arxiv_id": "2512.00931v1",
        "title": "Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study",
        "abstract": "Large language models (LLMs) produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt. This research project investigates whether prompt engineering (PE) methods can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts, where zero-shot indicates that the LLM relies purely on its pre-training data. Across eight yeast biotechnology research paper abstracts, six instruction-tuned LLMs were prompted with seven methods: a base- line prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2). Context repetition involved the identification and repetition of K key sentences from the abstract, whereas random addition involved the repetition of K randomly selected sentences from the abstract, where K is 1 or 2. A total of 336 LLM-generated summaries were evaluated using six metrics: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity, which were used to compute the lexical and semantic alignment be- tween the summaries and the abstracts. Four hypotheses on the effects of prompt methods on summary alignment with the reference text were tested. Statistical analysis on 3744 collected datapoints was performed using bias-corrected and accelerated (BCa) bootstrap confidence intervals and Wilcoxon signed-rank tests with Bonferroni-Holm correction. The results demonstrated that CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts. These findings indicate that prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks.",
        "authors": [
            "Imane Jaaouine",
            "Ross D. King"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T15:19:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00931v1"
    },
    {
        "arxiv_id": "2512.00920v1",
        "title": "Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios",
        "abstract": "Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering \"How accurate is the RM's preference perception for given samples?\", it employs scientific auditing to answer: \"Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?\". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.",
        "authors": [
            "Jianxiang Zang",
            "Yongda Wei",
            "Ruxue Bai",
            "Shiyu Jiang",
            "Nijia Mo",
            "Binhong Li",
            "Qiang Sun",
            "Hui Liu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T14:54:12+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00920v1"
    },
    {
        "arxiv_id": "2512.02074v1",
        "title": "Dialect Identification Using Resource-Efficient Fine-Tuning Approaches",
        "abstract": "Dialect Identification (DI) is a task to recognize different dialects within the same language from a speech signal. DI can help to improve the downstream speech related tasks even when speakers have a strong dialect. However, fine-tuning a speech model for tasks like DI is expensive in terms of computation cost and memory requirement. Recent studies have explored fine-tuning pre-trained speech models for tasks like DI using Parameter-Efficient Fine-Tuning (PEFT) methods, which offer parameter efficiency but limited improvement in memory efficiency and training speed. To address these challenges, we explore Memory-Efficient Fine-Tuning (MEFT) methods, originally proposed for language processing, and apply them to the general-purpose pre-trained speech model. We then comprehensively analyze the GPU memory usage and fine-tuning speed based on various MEFT methods. As a case study, we fine-tune the Whisper model to identify six Mandarin subdialects from the KeSpeech dataset, reducing GPU memory usage by up to 73.25% and accelerating training speed by a factor of 2.1, while maintaining accuracy comparable to vanilla fine-tuning and PEFT methods.",
        "authors": [
            "Zirui Lin",
            "Haris Gulzar",
            "Monnika Roslianna Busto",
            "Akiko Masaki",
            "Takeharu Eda",
            "Kazuhiro Nakadai"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T14:40:27+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02074v1"
    },
    {
        "arxiv_id": "2512.00884v1",
        "title": "Towards Active Synthetic Data Generation for Finetuning Language Models",
        "abstract": "A common and effective means for improving language model capabilities involves finetuning a ``student'' language model's parameters on generations from a more proficient ``teacher'' model. Termed ``synthetic data'', these generations are often produced before any student finetuning, but some work has considered generating new synthetic samples as training progresses. This paper studies and advocates for the latter case, where data are generated in an iterative, closed-loop fashion that is guided by the current state of the student model. For a fixed budget of generated samples, or a budget in terms of compute spent querying a teacher, we show that this curation of finetuning data affords improved student performance over static generation. Further, while there have been several LLM-specific methods proposed that operate in this regime, we find that simple, inexpensive selection criteria from the active learning literature tend to be most performant. We validate these claims across four mathematical and logical reasoning datasets using four different small language models.",
        "authors": [
            "Samuel Kessler",
            "Menglin Xia",
            "Daniel Madrigal Diaz",
            "Dongge Han",
            "Helia Heshemi",
            "Saravan Rajmohan",
            "Victor Ruehle",
            "Jordan T. Ash"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-30T13:13:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00884v1"
    },
    {
        "arxiv_id": "2512.00878v1",
        "title": "Less is More: Resource-Efficient Low-Rank Adaptation",
        "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While re- cent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, EffiLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices up- date to dynamically trade-off the system resource budget and model performance. EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demon- strating improved efficiency and robustness.",
        "authors": [
            "Chunlin Tian",
            "Xuyang Wei",
            "Huanrong Liu",
            "Zhijiang Guo",
            "Li Li"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T12:52:04+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00878v1"
    },
    {
        "arxiv_id": "2512.00852v1",
        "title": "One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces",
        "abstract": "Embedding spaces are fundamental to modern AI, translating raw data into high-dimensional vectors that encode rich semantic relationships. Yet, their internal structures remain opaque, with existing approaches often sacrificing semantic coherence for structural regularity or incurring high computational overhead to improve interpretability. To address these challenges, we introduce the Semantic Field Subspace (SFS), a geometry-preserving, context-aware representation that captures local semantic neighborhoods within the embedding space. We also propose SAFARI (SemAntic Field subspAce deteRmInation), an unsupervised, modality-agnostic algorithm that uncovers hierarchical semantic structures using a novel metric called Semantic Shift, which quantifies how semantics evolve as SFSes evolve. To ensure scalability, we develop an efficient approximation of Semantic Shift that replaces costly SVD computations, achieving a 15~30x speedup with average errors below 0.01. Extensive evaluations across six real-world text and image datasets show that SFSes outperform standard classifiers not only in classification but also in nuanced tasks such as political bias detection, while SAFARI consistently reveals interpretable and generalizable semantic hierarchies. This work presents a unified framework for structuring, analyzing, and scaling semantic understanding in embedding spaces.",
        "authors": [
            "Yandong Sun",
            "Qiang Huang",
            "Ziwei Xu",
            "Yiqun Sun",
            "Yixuan Tang",
            "Anthony K. H. Tung"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-30T11:48:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00852v1"
    },
    {
        "arxiv_id": "2512.00837v1",
        "title": "WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models",
        "abstract": "Watermarking acts as a critical safeguard in text generated by Large Language Models (LLMs). By embedding identifiable signals into model outputs, watermarking enables reliable attribution and enhances the security of machine-generated content. Existing approaches typically embed signals by manipulating token generation probabilities. Despite their effectiveness, these methods inherently face a trade-off between detectability and text quality: the signal strength and randomness required for robust watermarking tend to degrade the performance of downstream tasks.   In this paper, we design a novel embedding scheme that controls seed pools to facilitate diverse parallel generation of watermarked text. Based on that scheme, we propose WaterSearch, a sentence-level, search-based watermarking framework adaptable to a wide range of existing methods. WaterSearch enhances text quality by jointly optimizing two key aspects: 1) distribution fidelity and 2) watermark signal characteristics. Furthermore, WaterSearch is complemented by a sentence-level detection method with strong attack robustness. We evaluate our method on three popular LLMs across ten diverse tasks. Extensive experiments demonstrate that our method achieves an average performance improvement of 51.01\\% over state-of-the-art baselines at a watermark detectability strength of 95\\%. In challenging scenarios such as short text generation and low-entropy output generation, our method yields performance gains of 47.78\\% and 36.47\\%, respectively. Moreover, under different attack senarios including insertion, synonym substitution and paraphrase attasks, WaterSearch maintains high detectability, further validating its robust anti-attack capabilities. Our code is available at \\href{https://github.com/Yukang-Lin/WaterSearch}{https://github.com/Yukang-Lin/WaterSearch}.",
        "authors": [
            "Yukang Lin",
            "Jiahao Shao",
            "Shuoran Jiang",
            "Wentao Zhu",
            "Bingjie Lu",
            "Xiangping Wu",
            "Joanna Siebert",
            "Qingcai Chen"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T11:11:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00837v1"
    },
    {
        "arxiv_id": "2512.00829v1",
        "title": "Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy",
        "abstract": "Training models for Natural Language Processing (NLP) requires substantial computational resources and time, posing significant challenges, especially for NLP development in Bangla, where access to high-end hardware is often limited. In this work, we explore automatic mixed precision (AMP) training as a means to improve computational efficiency without sacrificing model performance. By leveraging a dynamic mix of 16-bit and 32-bit floating-point computations, AMP lowers GPU memory requirements and speeds up training without degrading model performance. We evaluate AMP across four standard Bangla NLP tasks, namely sentiment analysis, named entity recognition, error classification, and question answering, using four transformer-based models: BanglaBERT, BanglishBERT, XLM-R, and mBERT. Our results demonstrate that AMP accelerates training by 44.5% and reduces memory consumption by 17.6%, while maintaining F-1 score within 99.7% of the full-precision baselines. This empirical study highlights AMP's potential to democratize access to state-of-the-art NLP capabilities in hardware-constrained settings by lowering computational barriers.",
        "authors": [
            "Md Mehrab Hossain Opi",
            "Sumaiya Khan",
            "Moshammad Farzana Rahman"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T10:34:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00829v1"
    },
    {
        "arxiv_id": "2512.00789v1",
        "title": "Auxiliary-Hyperparameter-Free Sampling: Entropy Equilibrium for Text Generation",
        "abstract": "Token sampling strategies critically influence text generation quality in large language models (LLMs). However, existing methods introduce additional hyperparameters, requiring extensive tuning and complicating deployment. We present Entropy Equilibrium Sampling (EES), an auxiliary hyperparameter-free approach inspired by information theory that can dynamically adjust candidate sets by balancing normalized entropy with probability mass. We evaluate EES on both reasoning and generation tasks across a range of model architectures. Our results show that EES consistently performs well across temperature settings, delivering competitive accuracy and coherence while maintaining diversity. By eliminating the need for hyperparameter tuning, EES greatly simplifies deployment while improving performance. Code is available at https://github.com/shuanncai/EES",
        "authors": [
            "Xiaodong Cai",
            "Hai Lin",
            "Shaoxiong Zhan",
            "Weiqi Luo",
            "Hong-Gee Kim",
            "Hongyan Hao",
            "Yu Yang",
            "Hai-Tao Zheng"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T08:58:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00789v1"
    },
    {
        "arxiv_id": "2512.00768v1",
        "title": "Text Mining Analysis of Symptom Patterns in Medical Chatbot Conversations",
        "abstract": "The fast growth of digital health systems has led to a need to better comprehend how they interpret and represent patient-reported symptoms. Chatbots have been used in healthcare to provide clinical support and enhance the user experience, making it possible to provide meaningful clinical patterns from text-based data through chatbots. The proposed research utilises several different natural language processing methods to study the occurrences of symptom descriptions in medicine as well as analyse the patterns that emerge through these conversations within medical bots. Through the use of the Medical Conversations to Disease Dataset which contains 960 multi-turn dialogues divided into 24 Clinical Conditions, a standardised representation of conversations between patient and bot is created for further analysis by computational means. The multi-method approach uses a variety of tools, including Latent Dirichlet Allocation (LDA) to identify latent symptom themes, K-Means to group symptom descriptions by similarity, Transformer-based Named Entity Recognition (NER) to extract medical concepts, and the Apriori algorithm to discover frequent symptom pairs. Findings from the analysis indicate a coherent structure of clinically relevant topics, moderate levels of clustering cohesiveness and several high confidence rates on the relationships between symptoms like fever headache and rash itchiness. The results support the notion that conversational medical data can be a valuable diagnostic signal for early symptom interpretation, assist in strengthening decision support and improve how users interact with tele-health technology. By demonstrating a method for converting unstructured free-flowing dialogue into actionable knowledge regarding symptoms this work provides an extensible framework to further enhance future performance, dependability and clinical utility of selecting medical chatbots.",
        "authors": [
            "Hamed Razavi"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-30T07:40:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00768v1"
    },
    {
        "arxiv_id": "2512.00745v1",
        "title": "FastPOS: Language-Agnostic Scalable POS Tagging Framework Low-Resource Use Case",
        "abstract": "This study proposes a language-agnostic transformer-based POS tagging framework designed for low-resource languages, using Bangla and Hindi as case studies. With only three lines of framework-specific code, the model was adapted from Bangla to Hindi, demonstrating effective portability with minimal modification. The framework achieves 96.85 percent and 97 percent token-level accuracy across POS categories in Bangla and Hindi while sustaining strong F1 scores despite dataset imbalance and linguistic overlap. A performance discrepancy in a specific POS category underscores ongoing challenges in dataset curation. The strong results stem from the underlying transformer architecture, which can be replaced with limited code adjustments. Its modular and open-source design enables rapid cross-lingual adaptation while reducing model design and tuning overhead, allowing researchers to focus on linguistic preprocessing and dataset refinement, which are essential for advancing NLP in underrepresented languages.",
        "authors": [
            "Md Abdullah Al Kafi",
            "Sumit Kumar Banshal"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-30T05:48:12+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00745v1"
    },
    {
        "arxiv_id": "2512.00729v1",
        "title": "Probing the \"Psyche'' of Large Reasoning Models: Understanding Through a Human Lens",
        "abstract": "Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.",
        "authors": [
            "Yuxiang Chen",
            "Zuohan Wu",
            "Ziwei Wang",
            "Xiangning Yu",
            "Xujia Li",
            "Linyi Yang",
            "Mengyue Yang",
            "Jun Wang",
            "Lei Chen"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-30T04:49:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00729v1"
    },
    {
        "arxiv_id": "2512.00673v1",
        "title": "A Comparison of Human and ChatGPT Classification Performance on Complex Social Media Data",
        "abstract": "Generative artificial intelligence tools, like ChatGPT, are an increasingly utilized resource among computational social scientists. Nevertheless, there remains space for improved understanding of the performance of ChatGPT in complex tasks such as classifying and annotating datasets containing nuanced language. Method. In this paper, we measure the performance of GPT-4 on one such task and compare results to human annotators. We investigate ChatGPT versions 3.5, 4, and 4o to examine performance given rapid changes in technological advancement of large language models. We craft four prompt styles as input and evaluate precision, recall, and F1 scores. Both quantitative and qualitative evaluations of results demonstrate that while including label definitions in prompts may help performance, overall GPT-4 has difficulty classifying nuanced language. Qualitative analysis reveals four specific findings. Our results suggest the use of ChatGPT in classification tasks involving nuanced language should be conducted with prudence.",
        "authors": [
            "Breanna E. Green",
            "Ashley L. Shea",
            "Pengfei Zhao",
            "Drew B. Margolin"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T23:59:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00673v1"
    },
    {
        "arxiv_id": "2512.00663v1",
        "title": "Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs",
        "abstract": "Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.",
        "authors": [
            "Tanmay Agrawal"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T23:09:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00663v1"
    },
    {
        "arxiv_id": "2512.00656v1",
        "title": "Sycophancy Claims about Language Models: The Missing Human-in-the-Loop",
        "abstract": "Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.",
        "authors": [
            "Jan Batzner",
            "Volker Stocker",
            "Stefan Schmid",
            "Gjergji Kasneci"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T22:40:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00656v1"
    },
    {
        "arxiv_id": "2512.00621v1",
        "title": "Melody or Machine: Detecting Synthetic Music with Dual-Stream Contrastive Learning",
        "abstract": "The rapid evolution of end-to-end AI music generation poses an escalating threat to artistic authenticity and copyright, demanding detection methods that can keep pace. While foundational, existing models like SpecTTTra falter when faced with the diverse and rapidly advancing ecosystem of new generators, exhibiting significant performance drops on out-of-distribution (OOD) content. This generalization failure highlights a critical gap: the need for more challenging benchmarks and more robust detection architectures. To address this, we first introduce Melody or Machine (MoM), a new large-scale benchmark of over 130,000 songs (6,665 hours). MoM is the most diverse dataset to date, built with a mix of open and closed-source models and a curated OOD test set designed specifically to foster the development of truly generalizable detectors. Alongside this benchmark, we introduce CLAM, a novel dual-stream detection architecture. We hypothesize that subtle, machine-induced inconsistencies between vocal and instrumental elements, often imperceptible in a mixed signal, offer a powerful tell-tale sign of synthesis. CLAM is designed to test this hypothesis by employing two distinct pre-trained audio encoders (MERT and Wave2Vec2) to create parallel representations of the audio. These representations are fused by a learnable cross-aggregation module that models their inter-dependencies. The model is trained with a dual-loss objective: a standard binary cross-entropy loss for classification, complemented by a contrastive triplet loss which trains the model to distinguish between coherent and artificially mismatched stream pairings, enhancing its sensitivity to synthetic artifacts without presuming a simple feature alignment. CLAM establishes a new state-of-the-art in synthetic music forensics. It achieves an F1 score of 0.925 on our challenging MoM benchmark.",
        "authors": [
            "Arnesh Batra",
            "Dev Sharma",
            "Krish Thukral",
            "Ruhani Bhatia",
            "Naman Batra",
            "Aditya Gautam"
        ],
        "category": "cs.SD",
        "submitted_date": "2025-11-29T20:25:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00621v1"
    },
    {
        "arxiv_id": "2512.00617v1",
        "title": "ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.",
        "authors": [
            "Omer Jauhar Khan"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T20:16:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00617v1"
    },
    {
        "arxiv_id": "2512.00611v1",
        "title": "Prism: A Minimal Compositional Metalanguage for Specifying Agent Behavior",
        "abstract": "Prism is a small, compositional metalanguage for specifying the behaviour of tool-using software agents. Rather than introducing ad hoc control constructs, Prism is built around a fixed core context, Core1, which provides a minimal background grammar of categories numbers, strings, user prompts, tools together with abstract combinators for booleans, predicates, pairs, and lists. Agent policies are written as ordinary expressions using a single abstraction operator so that conditionals appear as selections between alternatives instead of imperative if-else blocks. Domains extend the core by defining their own context-mini-grammars that introduce new categories, predicates, and external tools while reusing the same compositional machinery. We illustrate this with worked examples from thermostat control, home security, e-commerce recommendation, and medical monitoring, showing how natural language decision rules can be mapped to inspectable, executable policies. From a linguistic perspective, Prism enforces a clear separation between a reusable grammar-like core and domain specific lexicons and treats tools as bridges between internal policy representations and the external world. From an engineering perspective, it offers a compact interface language for agent control, making the space of possible actions explicit and amenable to analysis, verification, and safety constraints.",
        "authors": [
            "Franck Binard",
            "Vanja Kljajevic"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T19:52:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00611v1"
    },
    {
        "arxiv_id": "2512.00590v1",
        "title": "Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models",
        "abstract": "Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.",
        "authors": [
            "Alla Chepurova",
            "Aydar Bulatov",
            "Yuri Kuratov",
            "Mikhail Burtsev"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T18:44:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00590v1"
    },
    {
        "arxiv_id": "2512.00586v1",
        "title": "Statistical NLP for Optimization of Clinical Trial Success Prediction in Pharmaceutical R&D",
        "abstract": "This work presents the development and evaluation of an NLP-enabled probabilistic classifier designed to estimate the probability of technical and regulatory success (pTRS) for clinical trials in the field of neuroscience. While pharmaceutical R&D is plagued by high attrition rates and enormous costs, particularly within neuroscience, where success rates are below 10%, timely identification of promising programs can streamline resource allocation and reduce financial risk. Leveraging data from the ClinicalTrials.gov database and success labels from the recently developed Clinical Trial Outcome dataset, the classifier extracts text-based clinical trial features using statistical NLP techniques. These features were integrated into several non-LLM frameworks (logistic regression, gradient boosting, and random forest) to generate calibrated probability scores. Model performance was assessed on a retrospective dataset of 101,145 completed clinical trials spanning 1976-2024, achieving an overall ROC-AUC of 0.64. An LLM-based predictive model was then built using BioBERT, a domain-specific language representation encoder. The BioBERT-based model achieved an overall ROC-AUC of 0.74 and a Brier Score of 0.185, indicating its predictions had, on average, 40% less squared error than would be observed using industry benchmarks. The BioBERT-based model also made trial outcome predictions that were superior to benchmark values 70% of the time overall. By integrating NLP-driven insights into drug development decision-making, this work aims to enhance strategic planning and optimize investment allocation in neuroscience programs.",
        "authors": [
            "Michael R. Doane"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-29T18:40:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00586v1"
    },
    {
        "arxiv_id": "2512.00579v1",
        "title": "Slovak Conceptual Dictionary",
        "abstract": "When solving tasks in the field of natural language processing, we sometimes need dictionary tools, such as lexicons, word form dictionaries or knowledge bases. However, the availability of dictionary data is insufficient in many languages, especially in the case of low resourced languages. In this article, we introduce a new conceptual dictionary for the Slovak language as the first linguistic tool of this kind. Since Slovak language is a language with limited linguistic resources and there are currently not available any machine-readable linguistic data sources with a sufficiently large volume of data, many tasks which require automated processing of Slovak text achieve weaker results compared to other languages and are almost impossible to solve.",
        "authors": [
            "Miroslav Bl\u0161t\u00e1k"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T18:15:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00579v1"
    },
    {
        "arxiv_id": "2512.00556v1",
        "title": "Bias Testing and Mitigation in Black Box LLMs using Metamorphic Relations",
        "abstract": "The widespread deployment of Large Language Models (LLMs) has intensified concerns about subtle social biases embedded in their outputs. Existing guardrails often fail when faced with indirect or contextually complex bias-inducing prompts. To address these limitations, we propose a unified framework for both systematic bias evaluation and targeted mitigation. Our approach introduces six novel Metamorphic Relations (MRs) that, based on metamorphic testing principles, transform direct bias-inducing inputs into semantically equivalent yet adversarially challenging variants. These transformations enable an automated method for exposing hidden model biases: when an LLM responds inconsistently or unfairly across MR-generated variants, the underlying bias becomes detectable. We further show that the same MRs can be used to generate diverse bias-inducing samples for fine-tuning, directly linking the testing process to mitigation. Using six state-of-the-art LLMs - spanning open-source and proprietary models - and a representative subset of 385 questions from the 8,978-item BiasAsker benchmark covering seven protected groups, our MRs reveal up to 14% more hidden biases compared to existing tools. Moreover, fine-tuning with both original and MR-mutated samples significantly enhances bias resiliency, increasing safe response rates from 54.7% to over 88.9% across models. These results highlight metamorphic relations as a practical mechanism for improving fairness in conversational AI.",
        "authors": [
            "Sina Salimian",
            "Gias Uddin",
            "Sumon Biswas",
            "Henry Leung"
        ],
        "category": "cs.SE",
        "submitted_date": "2025-11-29T16:56:38+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00556v1"
    },
    {
        "arxiv_id": "2512.00552v1",
        "title": "Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical Fidelity",
        "abstract": "Current evaluation of mathematical reasoning in language models relies primarily on answer accuracy, potentially masking fundamental failures in logical computation. We introduce a diagnostic framework that distinguishes genuine mathematical reasoning from superficial pattern matching through four complementary axes: forward-backward consistency, transitivity coverage, counterfactual sensitivity, and perturbation robustness. Through a case study applying this framework to Qwen3-0.6B on the MenatQA dataset, we reveal a striking disconnect between surface performance and reasoning fidelity. While the model achieves reasonable answer accuracy (70%+), it demonstrates poor backward consistency (15%), limited transitivity coverage (32.2%), and brittle sensitivity to perturbations. Our diagnostics expose reasoning failures invisible to traditional accuracy metrics, suggesting that this small model relies heavily on pattern matching rather than genuine logical computation. While our empirical findings are based on a single 600M-parameter model, the diagnostic framework itself is model-agnostic and generalizable. We release our evaluation protocols to enable the research community to assess reasoning fidelity across different model scales and architectures, moving beyond surface-level accuracy toward verifiable mathematical reasoning.",
        "authors": [
            "Subramanyam Sahoo",
            "Vinija Jain",
            "Saanidhya Vats",
            "Siddharth Mohapatra",
            "Rui Min",
            "Aman Chadha",
            "Divya Chaudhary"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T16:47:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00552v1"
    },
    {
        "arxiv_id": "2512.00521v1",
        "title": "Rep3Net: An Approach Exploiting Multimodal Representation for Molecular Bioactivity Prediction",
        "abstract": "In early stage drug discovery, bioactivity prediction of molecules against target proteins plays a crucial role. Trdaitional QSAR models that utilizes molecular descriptor based data often struggles to predict bioactivity of molecules effectively due to its limitation in capturing structural and contextual information embedded within each compound. To address this challenge, we propose Rep3Net, a unified deep learning architecture that not only incorporates descriptor data but also includes spatial and relational information through graph-based represenation of compounds and contextual information through ChemBERTa generated embeddings from SMILES strings. Our model employing multimodal concatenated features produce reliable bioactivity prediction on Poly [ADP-ribose] polymerase 1 (PARP-1) dataset. PARP-1 is a crucial agent in DNA damage repair and has become a significant theraputic target in malignancies that depend on it for survival and growth. A comprehensive analysis and comparison with conventional standalone models including GCN, GAT, XGBoost, etc. demonstrates that our architecture achieves the highest predictive performance. In computational screening of compounds in drug discovery, our architecture provides a scalable framework for bioactivity prediction.",
        "authors": [
            "Sabrina Islam",
            "Md. Atiqur Rahman",
            "Md. Bakhtiar Hasan",
            "Md. Hasanul Kabir"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-29T15:39:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00521v1"
    },
    {
        "arxiv_id": "2512.00515v1",
        "title": "Developing a Comprehensive Framework for Sentiment Analysis in Turkish",
        "abstract": "In this thesis, we developed a comprehensive framework for sentiment analysis that takes its many aspects into account mainly for Turkish. We have also proposed several approaches specific to sentiment analysis in English only. We have accordingly made five major and three minor contributions. We generated a novel and effective feature set by combining unsupervised, semi-supervised, and supervised metrics. We then fed them as input into classical machine learning methods, and outperformed neural network models for datasets of different genres in both Turkish and English. We created a polarity lexicon with a semi-supervised domain-specific method, which has been the first approach applied for corpora in Turkish. We performed a fine morphological analysis for the sentiment classification task in Turkish by determining the polarities of morphemes. This can be adapted to other morphologically-rich or agglutinative languages as well. We have built a novel neural network architecture, which combines recurrent and recursive neural network models for English. We built novel word embeddings that exploit sentiment, syntactic, semantic, and lexical characteristics for both Turkish and English. We also redefined context windows as subclauses in modelling word representations in English. This can also be applied to other linguistic fields and natural language processing tasks. We have achieved state-of-the-art and significant results for all these original approaches. Our minor contributions include methods related to aspect-based sentiment in Turkish, parameter redefinition in the semi-supervised approach, and aspect term extraction techniques for English. This thesis can be considered the most detailed and comprehensive study made on sentiment analysis in Turkish as of July, 2020. Our work has also contributed to the opinion classification problem in English.",
        "authors": [
            "Cem Rifki Aydin"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T15:14:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00515v1"
    },
    {
        "arxiv_id": "2512.00504v1",
        "title": "G-KV: Decoding-Time KV Cache Eviction with Global Attention",
        "abstract": "Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.",
        "authors": [
            "Mengqi Liao",
            "Lu Wang",
            "Chaoyun Zhang",
            "Zekai Shen",
            "Xiaowei Mao",
            "Si Qin",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang",
            "Huaiyu Wan"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T14:21:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00504v1"
    },
    {
        "arxiv_id": "2512.00496v1",
        "title": "CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective Multimodal and Multilingual Learning",
        "abstract": "As deep learning models evolve, new applications and challenges are rapidly emerging. Tasks that once relied on a single modality, such as text, images, or audio, are now enriched by seamless interactions between multimodal data. These connections bridge information gaps: an image can visually materialize a text, while audio can add context to an image. Researchers have developed numerous multimodal models, but most rely on resource-intensive training across multiple modalities. Similarly, extending these models to new languages often follows the same resource-heavy training strategy. In this work, we propose a multimodal and multilingual architecture, CACARA, trained through emergent alignment learning, enabling the seamless integration of new modalities into an existing bimodal/multimodal model without requiring full retraining. This work breaks new ground by demonstrating that this emergent alignment paradigm can unlock multilingual capabilities from monolingual training. By fine-tuning the newly incorporated modality only on data aligned with the English language, our model develops support for over 100 languages without explicit multilingual pretraining or tuning of the text encoder. Such emergent multimodal and multilingual properties are gained efficiently, preserving previously learned knowledge at a training cost comparable to that of a monolingual model. Our strategy achieves up to a 14.24 percentage points improvement in R@1 audio-to-text retrieval, outperforming state-of-the-art multimodal models -- all without the heavy computational cost of retraining across every modality and language.",
        "authors": [
            "Diego A. B. Moreira",
            "Alef I. Ferreira",
            "Jhessica Silva",
            "Gabriel O. dos Santos",
            "Gustavo Bonil",
            "Jo\u00e3o Gondim",
            "Marina dos Santos",
            "Helena Maia",
            "Simone Hashiguti",
            "N\u00e1dia da Silva",
            "Carolina Scarton",
            "Helio Pedrini",
            "Sandra Avila"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T14:04:27+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00496v1"
    },
    {
        "arxiv_id": "2512.00466v1",
        "title": "SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling",
        "abstract": "Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose \\textbf{SCALE} (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.",
        "authors": [
            "Yang Xiao",
            "Chunpu Xu",
            "Ruifeng Yuan",
            "Jiashuo Wang",
            "Wenjie Li",
            "Pengfei Liu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T12:38:07+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00466v1"
    },
    {
        "arxiv_id": "2512.00461v1",
        "title": "Whose Personae? Synthetic Persona Experiments in LLM Research and Pathways to Transparency",
        "abstract": "Synthetic personae experiments have become a prominent method in Large Language Model alignment research, yet the representativeness and ecological validity of these personae vary considerably between studies. Through a review of 63 peer-reviewed studies published between 2023 and 2025 in leading NLP and AI venues, we reveal a critical gap: task and population of interest are often underspecified in persona-based experiments, despite personalization being fundamentally dependent on these criteria. Our analysis shows substantial differences in user representation, with most studies focusing on limited sociodemographic attributes and only 35% discussing the representativeness of their LLM personae. Based on our findings, we introduce a persona transparency checklist that emphasizes representative sampling, explicit grounding in empirical data, and enhanced ecological validity. Our work provides both a comprehensive assessment of current practices and practical guidelines to improve the rigor and ecological validity of persona-based evaluations in language model alignment research.",
        "authors": [
            "Jan Batzner",
            "Volker Stocker",
            "Bingjun Tang",
            "Anusha Natarajan",
            "Qinhao Chen",
            "Stefan Schmid",
            "Gjergji Kasneci"
        ],
        "category": "cs.CY",
        "submitted_date": "2025-11-29T12:27:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00461v1"
    },
    {
        "arxiv_id": "2512.00417v1",
        "title": "CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency",
        "abstract": "This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \\emph{extreme time-sensitivity}, \\emph{a highly adversarial information environment}, and the critical need to synthesize data from \\emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.   Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \\textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.",
        "authors": [
            "Jiacheng Guo",
            "Suozhi Huang",
            "Zixin Yao",
            "Yifan Zhang",
            "Yifu Lu",
            "Jiashuo Liu",
            "Zihao Li",
            "Yanyan Deng",
            "Qixin Xiao",
            "Jia Tian",
            "Kanghong Zhan",
            "Tianyi Li",
            "Xiaochen Liu",
            "Jason Ge",
            "Chaoyang He",
            "Kaixuan Huang",
            "Lin Yang",
            "Wenhao Huang",
            "Mengdi Wang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T09:52:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00417v1"
    },
    {
        "arxiv_id": "2512.00392v1",
        "title": "A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method of Error Analysis for EFL Writing Instruction",
        "abstract": "This study describes the development of an AI-assisted error analysis system designed to identify, categorize, and correct writing errors in English. Utilizing Large Language Models (LLMs) like Claude 3.5 Sonnet and DeepSeek R1, the system employs a detailed taxonomy grounded in linguistic theories from Corder (1967), Richards (1971), and James (1998). Errors are classified at both word and sentence levels, covering spelling, grammar, and punctuation. Implemented through Python-coded API calls, the system provides granular feedback beyond traditional rubric-based assessments. Initial testing on isolated errors refined the taxonomy, addressing challenges like overlapping categories. Final testing used \"English as she is spoke\" by Jose da Fonseca (1855), a text rich with authentic linguistic errors, to evaluate the system's capacity for handling complex, multi-layered analysis. The AI successfully identified diverse error types but showed limitations in contextual understanding and occasionally generated new error categories when encountering uncoded errors. This research demonstrates AI's potential to transform EFL instruction by automating detailed error analysis and feedback. While promising, further development is needed to improve contextual accuracy and expand the taxonomy to stylistic and discourse-level errors.",
        "authors": [
            "Damian Heywood",
            "Joseph Andrew Carrier",
            "Kyu-Hong Hwang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T08:45:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00392v1"
    },
    {
        "arxiv_id": "2512.00390v1",
        "title": "Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing",
        "abstract": "Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.",
        "authors": [
            "Nuo Chen",
            "Hanpei Fang",
            "Jiqun Liu",
            "Wilson Wei",
            "Tetsuya Sakai",
            "Xiao-Ming Wu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T08:37:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00390v1"
    },
    {
        "arxiv_id": "2512.00367v1",
        "title": "Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation",
        "abstract": "Document chunking is a crucial component of Retrieval-Augmented Generation (RAG), as it directly affects the retrieval of relevant and precise context. Conventional fixed-length and recursive splitters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMed data using three different embedding models. We further present an evaluation framework that measures the effect of chunking on both retrieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response-time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consistently deliver superior performance.",
        "authors": [
            "Aparajitha Allamraju",
            "Maitreya Prafulla Chitale",
            "Hiranmai Sri Adibhatla",
            "Rahul Mishra",
            "Manish Shrivastava"
        ],
        "category": "cs.IR",
        "submitted_date": "2025-11-29T07:30:37+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00367v1"
    },
    {
        "arxiv_id": "2512.00360v1",
        "title": "CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA",
        "abstract": "We study timestamped question answering over educational lecture videos under a single-GPU latency/memory budget. Given a natural-language query, the system retrieves relevant timestamped segments and synthesizes a grounded answer. We present CourseTimeQA (52.3 h, 902 queries across six courses) and a lightweight, latency-constrained cross-modal retriever (CrossFusion-RAG) that combines frozen encoders, a learned 512->768 vision projection, shallow query-agnostic cross-attention over ASR and frames with a temporal-consistency regularizer, and a small cross-attentive reranker. On CourseTimeQA, CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over a strong BLIP-2 retriever while achieving approximately 1.55 s median end-to-end latency on a single A100. Closest comparators (zero-shot CLIP multi-frame pooling; CLIP + cross-encoder reranker + MMR; learned late-fusion gating; text-only hybrid with cross-encoder reranking and its MMR variant; caption-augmented text retrieval; non-learned temporal smoothing) are evaluated under matched hardware and indexing. We report robustness across ASR noise (WER quartiles), diagnostics for temporal localization, and full training/tuning details to support reproducible comparison.",
        "authors": [
            "Vsevolod Kovalev",
            "Parteek Kumar"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T07:06:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00360v1"
    },
    {
        "arxiv_id": "2512.00333v1",
        "title": "IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages",
        "abstract": "While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.",
        "authors": [
            "Ayush Maheshwari",
            "Kaushal Sharma",
            "Vivek Patel",
            "Aditya Maheshwari"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T05:49:50+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00333v1"
    },
    {
        "arxiv_id": "2512.00332v1",
        "title": "Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents",
        "abstract": "Multi-turn tool-calling LLMs (models capable of invoking external APIs or tools across several user turns) have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines remains difficult for many safety-critical industries due to ongoing concerns regarding model resilience. While standardized benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) have underpinned confidence concerning advanced function-calling models (like Salesforce's xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce Assertion-Conditioned Compliance (A-CC), a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model's behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.",
        "authors": [
            "Daud Waqas",
            "Aaryamaan Golthi",
            "Erika Hayashida",
            "Huanzhi Mao"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T05:44:37+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00332v1"
    },
    {
        "arxiv_id": "2512.00329v1",
        "title": "Evidence-Guided Schema Normalization for Temporal Tabular Reasoning",
        "abstract": "Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\\% improvement over the baseline (68.89 EM).",
        "authors": [
            "Ashish Thanga",
            "Vibhu Dixit",
            "Abhilash Shankarampeta",
            "Vivek Gupta"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T05:40:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00329v1"
    },
    {
        "arxiv_id": "2512.00325v1",
        "title": "Progressive Code Integration for Abstractive Bug Report Summarization",
        "abstract": "Bug reports are often unstructured and verbose, making it challenging for developers to efficiently comprehend software issues. Existing summarization approaches typically rely on surface-level textual cues, resulting in incomplete or redundant summaries, and they frequently ignore associated code snippets, which are essential for accurate defect diagnosis. To address these limitations, we propose a progressive code-integration framework for LLM-based abstractive bug report summarization. Our approach incrementally incorporates long code snippets alongside textual content, overcoming standard LLM context window constraints and producing semantically rich summaries. Evaluated on four benchmark datasets using eight LLMs, our pipeline outperforms extractive baselines by 7.5%-58.2% and achieves performance comparable to state-of-the-art abstractive methods, highlighting the benefits of jointly leveraging textual and code information for enhanced bug comprehension.",
        "authors": [
            "Shaira Sadia Karim",
            "Abrar Mahmud Rahim",
            "Lamia Alam",
            "Ishmam Tashdeed",
            "Lutfun Nahar Lota",
            "Md. Abu Raihan M. Kamal",
            "Md. Azam Hossain"
        ],
        "category": "cs.SE",
        "submitted_date": "2025-11-29T05:35:36+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00325v1"
    },
    {
        "arxiv_id": "2512.00323v1",
        "title": "Comparative Analysis of 47 Context-Based Question Answer Models Across 8 Diverse Datasets",
        "abstract": "Context-based question answering (CBQA) models provide more accurate and relevant answers by considering the contextual information. They effectively extract specific information given a context, making them functional in various applications involving user support, information retrieval, and educational platforms. In this manuscript, we benchmarked the performance of 47 CBQA models from Hugging Face on eight different datasets. This study aims to identify the best-performing model across diverse datasets without additional fine-tuning. It is valuable for practical applications where the need to retrain models for specific datasets is minimized, streamlining the implementation of these models in various contexts. The best-performing models were trained on the SQuAD v2 or SQuAD v1 datasets. The best-performing model was ahotrod/electra_large_discriminator_squad2_512, which yielded 43\\% accuracy across all datasets. We observed that the computation time of all models depends on the context length and the model size. The model's performance usually decreases with an increase in the answer length. Moreover, the model's performance depends on the context complexity. We also used the Genetic algorithm to improve the overall accuracy by integrating responses from other models. ahotrod/electra_large_discriminator_squad2_512 generated the best results for bioasq10b-factoid (65.92\\%), biomedical\\_cpgQA (96.45\\%), QuAC (11.13\\%), and Question Answer Dataset (41.6\\%). Bert-large-uncased-whole-word-masking-finetuned-squad achieved an accuracy of 82\\% on the IELTS dataset.",
        "authors": [
            "Muhammad Muneeb",
            "David B. Ascher",
            "Ahsan Baidar Bakht"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T05:31:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00323v1"
    },
    {
        "arxiv_id": "2512.00298v1",
        "title": "Challenges of Heterogeneity in Big Data: A Comparative Study of Classification in Large-Scale Structured and Unstructured Domains",
        "abstract": "This study analyzes the impact of heterogeneity (\"Variety\") in Big Data by comparing classification strategies across structured (Epsilon) and unstructured (Rest-Mex, IMDB) domains. A dual methodology was implemented: evolutionary and Bayesian hyperparameter optimization (Genetic Algorithms, Optuna) in Python for numerical data, and distributed processing in Apache Spark for massive textual corpora. The results reveal a \"complexity paradox\": in high-dimensional spaces, optimized linear models (SVM, Logistic Regression) outperformed deep architectures and Gradient Boosting. Conversely, in text-based domains, the constraints of distributed fine-tuning led to overfitting in complex models, whereas robust feature engineering -- specifically Transformer-based embeddings (ROBERTa) and Bayesian Target Encoding -- enabled simpler models to generalize effectively. This work provides a unified framework for algorithm selection based on data nature and infrastructure constraints.",
        "authors": [
            "Gonz\u00e1lez Trigueros Jes\u00fas Eduardo",
            "Alonso S\u00e1nchez Alejandro",
            "Mu\u00f1oz Rivera Emilio",
            "Pe\u00f1ar\u00e1n Prieto Mariana Jaqueline",
            "Mendoza Gonz\u00e1lez Camila Natalia"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-29T03:41:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00298v1"
    },
    {
        "arxiv_id": "2512.00290v1",
        "title": "EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education",
        "abstract": "Large language models (LLMs) demonstrate significant potential for educational applications. However, their unscrutinized deployment poses risks to educational standards, underscoring the need for rigorous evaluation. We introduce EduEval, a comprehensive hierarchical benchmark for evaluating LLMs in Chinese K-12 education. This benchmark makes three key contributions: (1) Cognitive Framework: We propose the EduAbility Taxonomy, which unifies Bloom's Taxonomy and Webb's Depth of Knowledge to organize tasks across six cognitive dimensions including Memorization, Understanding, Application, Reasoning, Creativity, and Ethics. (2) Authenticity: Our benchmark integrates real exam questions, classroom conversation, student essays, and expert-designed prompts to reflect genuine educational challenges; (3) Scale: EduEval comprises 24 distinct task types with over 11,000 questions spanning primary to high school levels. We evaluate 14 leading LLMs under both zero-shot and few-shot settings, revealing that while models perform well on factual tasks, they struggle with classroom dialogue classification and exhibit inconsistent results in creative content generation. Interestingly, several open source models outperform proprietary systems on complex educational reasoning. Few-shot prompting shows varying effectiveness across cognitive dimensions, suggesting that different educational objectives require tailored approaches. These findings provide targeted benchmarking metrics for developing LLMs specifically optimized for diverse Chinese educational tasks.",
        "authors": [
            "Guoqing Ma",
            "Jia Zhu",
            "Hanghui Guo",
            "Weijie Shi",
            "Yue Cui",
            "Jiawei Shen",
            "Zilong Li",
            "Yidan Liang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T03:09:50+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00290v1"
    },
    {
        "arxiv_id": "2512.00274v1",
        "title": "Lost without translation -- Can transformer (language models) understand mood states?",
        "abstract": "Background: Large Language Models show promise in psychiatry but are English-centric. Their ability to understand mood states in other languages is unclear, as different languages have their own idioms of distress. Aim: To quantify the ability of language models to faithfully represent phrases (idioms of distress) of four distinct mood states (depression, euthymia, euphoric mania, dysphoric mania) expressed in Indian languages. Methods: We collected 247 unique phrases for the four mood states across 11 Indic languages. We tested seven experimental conditions, comparing k-means clustering performance on: (a) direct embeddings of native and Romanised scripts (using multilingual and Indic-specific models) and (b) embeddings of phrases translated to English and Chinese. Performance was measured using a composite score based on Adjusted Rand Index, Normalised Mutual Information, Homogeneity and Completeness. Results: Direct embedding of Indic languages failed to cluster mood states (Composite Score = 0.002). All translation-based approaches showed significant improvement. High performance was achieved using Gemini-translated English (Composite=0.60) and human-translated English (Composite=0.61) embedded with gemini-001. Surprisingly, human-translated English, further translated into Chinese and embedded with a Chinese model, performed best (Composite = 0.67). Specialised Indic models (IndicBERT and Sarvam-M) performed poorly. Conclusion: Current models cannot meaningfully represent mood states directly from Indic languages, posing a fundamental barrier to their psychiatric application for diagnostic or therapeutic purposes in India. While high-quality translation bridges this gap, reliance on proprietary models or complex translation pipelines is unsustainable. Models must first be built to understand diverse local languages to be effective in global mental health.",
        "authors": [
            "Prakrithi Shivaprakash",
            "Diptadhi Mukherjee",
            "Lekhansh Shukla",
            "Animesh Mukherjee",
            "Prabhat Chand",
            "Pratima Murthy"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-29T01:55:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00274v1"
    },
    {
        "arxiv_id": "2512.00234v1",
        "title": "OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion",
        "abstract": "There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality\\footnote{Code is available at https://github.com/saikoneru/OmniFusion}.",
        "authors": [
            "Sai Koneru",
            "Matthias Huck",
            "Jan Niehues"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T22:39:12+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00234v1"
    },
    {
        "arxiv_id": "2512.00219v1",
        "title": "Minimal-Edit Instruction Tuning for Low-Resource Indic GEC",
        "abstract": "Grammatical error correction for Indic languages faces limited supervision, diverse scripts, and rich morphology. We propose an augmentation-free setup that uses instruction-tuned large language models and conservative decoding. A 12B GEMMA 3 model is instruction-tuned in bnb 4-bit precision with parameter-efficient fine-tuning (PEFT) and Alpaca-style formatting. Decoding follows a deterministic, constraint-aware procedure with a lightweight normaliser that encourages minimal, meaning-preserving edits. We operationalise inference, subsequent to instruction fine-tuning (IFT), via a fixed, language-specific prompt directly synthesised from a deterministic error classifier's taxonomy, label distributions, and precedence ordering computed on the training corpus.   Under the official untuned GLEU evaluation, the system scores 92.41 on Malayalam, sixth overall, and 81.44 on Hindi, third overall. These results indicate that classifier-informed prompt design, adapter-based instruction tuning, and deterministic decoding provide a reproducible and a computationally efficient alternative to augmentation-centred pipelines for Indic GEC. The approach also motivates future work on stronger morphosyntactic constraints and human-centred evaluation of conservative edits.",
        "authors": [
            "Akhil Rajeev P"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T21:38:27+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00219v1"
    },
    {
        "arxiv_id": "2512.00214v1",
        "title": "Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis",
        "abstract": "Empirical grammar research has become increasingly data-driven, but the systematic analysis of annotated corpora still requires substantial methodological and technical effort. We explore how agentic large language models (LLMs) can streamline this process by reasoning over annotated corpora and producing interpretable, data-grounded answers to linguistic questions. We introduce an agentic framework for corpus-grounded grammatical analysis that integrates concepts such as natural-language task interpretation, code generation, and data-driven reasoning. As a proof of concept, we apply it to Universal Dependencies (UD) corpora, testing it on multilingual grammatical tasks inspired by the World Atlas of Language Structures (WALS). The evaluation spans 13 word-order features and over 170 languages, assessing system performance across three complementary dimensions - dominant-order accuracy, order-coverage completeness, and distributional fidelity - which reflect how well the system generalizes, identifies, and quantifies word-order variations. The results demonstrate the feasibility of combining LLM reasoning with structured linguistic data, offering a first step toward interpretable, scalable automation of corpus-based grammatical inquiry.",
        "authors": [
            "Matej Klemen",
            "Tja\u0161a Ar\u010don",
            "Luka Ter\u010don",
            "Marko Robnik-\u0160ikonja",
            "Kaja Dobrovoljc"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T21:27:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00214v1"
    },
    {
        "arxiv_id": "2512.00204v1",
        "title": "Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees",
        "abstract": "In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.",
        "authors": [
            "Jason Lunder"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T21:06:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00204v1"
    },
    {
        "arxiv_id": "2512.00163v1",
        "title": "Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on Financial Tabular Classification",
        "abstract": "Large Language Models (LLMs) have attracted significant attention for classification tasks, offering a flexible alternative to trusted classical machine learning models like LightGBM through zero-shot prompting. However, their reliability for structured tabular data remains unclear, particularly in high stakes applications like financial risk assessment. Our study systematically evaluates LLMs and generates their SHAP values on financial classification tasks. Our analysis shows a divergence between LLMs self-explanation of feature impact and their SHAP values, as well as notable differences between LLMs and LightGBM SHAP values. These findings highlight the limitations of LLMs as standalone classifiers for structured financial modeling, but also instill optimism that improved explainability mechanisms coupled with few-shot prompting will make LLMs usable in risk-sensitive domains.",
        "authors": [
            "Saeed AlMarri",
            "Mathieu Ravaut",
            "Kristof Juhasz",
            "Gautier Marti",
            "Hamdan Al Ahbabi",
            "Ibrahim Elfadel"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-28T19:04:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.00163v1"
    },
    {
        "arxiv_id": "2511.23473v1",
        "title": "ThetaEvolve: Test-time Learning on Open Problems",
        "abstract": "Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve",
        "authors": [
            "Yiping Wang",
            "Shao-Rong Su",
            "Zhiyuan Zeng",
            "Eva Xu",
            "Liliang Ren",
            "Xinyu Yang",
            "Zeyi Huang",
            "Xuehai He",
            "Luyao Ma",
            "Baolin Peng",
            "Hao Cheng",
            "Pengcheng He",
            "Weizhu Chen",
            "Shuohang Wang",
            "Simon Shaolei Du",
            "Yelong Shen"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-28T18:58:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23473v1"
    },
    {
        "arxiv_id": "2512.02058v1",
        "title": "Misalignment of LLM-Generated Personas with Human Perceptions in Low-Resource Settings",
        "abstract": "Recent advances enable Large Language Models (LLMs) to generate AI personas, yet their lack of deep contextual, cultural, and emotional understanding poses a significant limitation. This study quantitatively compared human responses with those of eight LLM-generated social personas (e.g., Male, Female, Muslim, Political Supporter) within a low-resource environment like Bangladesh, using culturally specific questions. Results show human responses significantly outperform all LLMs in answering questions, and across all matrices of persona perception, with particularly large gaps in empathy and credibility. Furthermore, LLM-generated content exhibited a systematic bias along the lines of the ``Pollyanna Principle'', scoring measurably higher in positive sentiment ($\u03a6_{avg} = 5.99$ for LLMs vs. $5.60$ for Humans). These findings suggest that LLM personas do not accurately reflect the authentic experience of real people in resource-scarce environments. It is essential to validate LLM personas against real-world human data to ensure their alignment and reliability before deploying them in social science research.",
        "authors": [
            "Tabia Tanzin Prama",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "category": "cs.CY",
        "submitted_date": "2025-11-28T17:52:26+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02058v1"
    },
    {
        "arxiv_id": "2511.23397v1",
        "title": "MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation",
        "abstract": "Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion. However, developing AI-driven chatbots for this purpose requires large, high-quality question-and-answer (Q&A) datasets, which are typically expensive and resource-intensive to produce, especially for low-resource languages like Persian. In this paper, we introduce MegaChat, the first fully synthetic Persian Q&A dataset designed to evaluate intelligent sales chatbots in Telegram-based e-commerce. We propose a novel, automated multi-agent architecture that generates persona-aware Q&A pairs by collecting data from active Telegram shopping channels. The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data. To evaluate answer generation, we compare three classic retrieval-augmented generation (RAG) models with our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis. Using GPT-5.1 for evaluation across six quality dimensions, our results show that the agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating its ability to generate scalable, high-quality datasets without relying on expensive human annotation or complex fine-tuning. MegaChat provides SMEs with an efficient, cost-effective solution for building intelligent customer engagement systems in specialized commercial domains, enabling advancements in multilingual conversational AI for low-resource languages. Download: https://github.com/MegaChat-Tech/MegaChat-DataSet",
        "authors": [
            "Mahdi Rahmani",
            "AmirHossein Saffari",
            "Reyhane Rahmani"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T17:44:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23397v1"
    },
    {
        "arxiv_id": "2511.23391v1",
        "title": "Ambiguity Awareness Optimization: Towards Semantic Disambiguation for Direct Preference Optimization",
        "abstract": "Direct Preference Optimization (DPO) is a widely used reinforcement learning from human feedback (RLHF) method across various domains. Recent research has increasingly focused on the role of token importance in improving DPO effectiveness. It is observed that identical or semantically similar content (defined as ambiguous content) frequently appears within the preference pairs. We hypothesize that the presence of ambiguous content during DPO training may introduce ambiguity, thereby limiting further improvements in alignment. Through mathematical analysis and proof-of-concept experiments, we reveal that ambiguous content may potentially introduce ambiguities, thereby degrading performance. To address this issue, we introduce Ambiguity Awareness Optimization (AAO), a simple yet effective approach that automatically re-weights ambiguous content to reduce ambiguities by calculating semantic similarity from preference pairs. Through extensive experiments, we demonstrate that AAO consistently and significantly surpasses state-of-the-art approaches in performance, without markedly increasing response length, across multiple model scales and widely adopted benchmark datasets, including AlpacaEval 2, MT-Bench, and Arena-Hard. Specifically, AAO outperforms DPO by up to 8.9 points on AlpacaEval 2 and achieves an improvement of by up to 15.0 points on Arena-Hard.",
        "authors": [
            "Jian Li",
            "Shenglin Yin",
            "Yujia Zhang",
            "Alan Zhao",
            "Xi Chen",
            "Xiaohui Zhou",
            "Pengfei Xu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T17:32:54+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23391v1"
    },
    {
        "arxiv_id": "2511.23376v1",
        "title": "Is Passive Expertise-Based Personalization Enough? A Case Study in AI-Assisted Test-Taking",
        "abstract": "Novice and expert users have different systematic preferences in task-oriented dialogues. However, whether catering to these preferences actually improves user experience and task performance remains understudied. To investigate the effects of expertise-based personalization, we first built a version of an enterprise AI assistant with passive personalization. We then conducted a user study where participants completed timed exams, aided by the two versions of the AI assistant. Preliminary results indicate that passive personalization helps reduce task load and improve assistant perception, but reveal task-specific limitations that can be addressed through providing more user agency. These findings underscore the importance of combining active and passive personalization to optimize user experience and effectiveness in enterprise task-oriented environments.",
        "authors": [
            "Li Siyan",
            "Jason Zhang",
            "Akash Maharaj",
            "Yuanming Shi",
            "Yunyao Li"
        ],
        "category": "cs.HC",
        "submitted_date": "2025-11-28T17:21:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23376v1"
    },
    {
        "arxiv_id": "2511.23375v1",
        "title": "Optimizing Multimodal Language Models through Attention-based Interpretability",
        "abstract": "Modern large language models become multimodal, analyzing various data formats like text and images. While fine-tuning is effective for adapting these multimodal language models (MLMs) to downstream tasks, full fine-tuning is computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small portion of model weights. However, MLMs are difficult to interpret, making it challenging to identify which components are most effective for training to balance efficiency and performance. We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens. The core idea is to identify attention heads that focus on image key objects. We utilize this information to select optimal model components for PEFT in multimodal models. Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions. We conducted experiments on MLMs with 2-3 billion parameters to validate the method's effectiveness. By calculating Head Impact (HI) scores we quantify an attention head's focus on key objects, indicating its significance in image understanding. Our fine-tuning experiments demonstrate that adapting layers with the highest HI scores leads to the most significant shifts in metrics compared to pre-trained, randomly selected, or lowest-HI-score layers. This indicates that fine-tuning a small percentage (around 0.01%) of parameters in these crucial layers can substantially influence image understanding capabilities.",
        "authors": [
            "Alexander Sergeev",
            "Evgeny Kotelnikov"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T17:21:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23375v1"
    },
    {
        "arxiv_id": "2511.23370v1",
        "title": "Scaling HuBERT for African Languages: From Base to Large and XL",
        "abstract": "Despite recent progress in multilingual speech processing, African languages remain under-represented in both research and deployed systems, particularly when it comes to strong, open-weight encoders that transfer well under low-resource supervision. Self-supervised learning has proven especially promising in such settings, yet most publicly released models targeting African speech remain at BASE scale, leaving unanswered whether larger encoders, trained exclusively on Africa-centric audio, offer tangible benefits and how model capacity interacts with data composition. This work addresses that gap by introducing SSA-HuBERT-Large (317M parameters) and SSA-HuBERT-XL (964M parameters), the first large models trained solely on African speech, alongside a BASE size counterpart. We release these models as open weights: see https://huggingface.co/collections/Orange/african-speech-foundation-models. By conducting a carefully controlled experimental study focused exclusively on Sub-Saharan languages, covering automatic speech recognition (ASR) and language identification (LID) tasks, we demonstrate that larger architectures significantly improve performance by effectively leveraging large audio datasets.",
        "authors": [
            "Antoine Caubri\u00e8re",
            "Elodie Gauthier"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T17:17:40+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23370v1"
    },
    {
        "arxiv_id": "2511.23335v1",
        "title": "Towards Improving Interpretability of Language Model Generation through a Structured Knowledge Discovery Approach",
        "abstract": "Knowledge-enhanced text generation aims to enhance the quality of generated text by utilizing internal or external knowledge sources. While language models have demonstrated impressive capabilities in generating coherent and fluent text, the lack of interpretability presents a substantial obstacle. The limited interpretability of generated text significantly impacts its practical usability, particularly in knowledge-enhanced text generation tasks that necessitate reliability and explainability. Existing methods often employ domain-specific knowledge retrievers that are tailored to specific data characteristics, limiting their generalizability to diverse data types and tasks. To overcome this limitation, we directly leverage the two-tier architecture of structured knowledge, consisting of high-level entities and low-level knowledge triples, to design our task-agnostic structured knowledge hunter. Specifically, we employ a local-global interaction scheme for structured knowledge representation learning and a hierarchical transformer-based pointer network as the backbone for selecting relevant knowledge triples and entities. By combining the strong generative ability of language models with the high faithfulness of the knowledge hunter, our model achieves high interpretability, enabling users to comprehend the model output generation process. Furthermore, we empirically demonstrate the effectiveness of our model in both internal knowledge-enhanced table-to-text generation on the RotoWireFG dataset and external knowledge-enhanced dialogue response generation on the KdConv dataset. Our task-agnostic model outperforms state-of-the-art methods and corresponding language models, setting new standards on the benchmark.",
        "authors": [
            "Shuqi Liu",
            "Han Wu",
            "Guanzhi Deng",
            "Jianshu Chen",
            "Xiaoyang Wang",
            "Linqi Song"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T16:43:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23335v1"
    },
    {
        "arxiv_id": "2511.23325v1",
        "title": "Tackling a Challenging Corpus for Early Detection of Gambling Disorder: UNSL at MentalRiskES 2025",
        "abstract": "Gambling disorder is a complex behavioral addiction that is challenging to understand and address, with severe physical, psychological, and social consequences. Early Risk Detection (ERD) on the Web has become a key task in the scientific community for identifying early signs of mental health behaviors based on social media activity. This work presents our participation in the MentalRiskES 2025 challenge, specifically in Task 1, aimed at classifying users at high or low risk of developing a gambling-related disorder. We proposed three methods based on a CPI+DMC approach, addressing predictive effectiveness and decision-making speed as independent objectives. The components were implemented using the SS3, BERT with extended vocabulary, and SBERT models, followed by decision policies based on historical user analysis. Although it was a challenging corpus, two of our proposals achieved the top two positions in the official results, performing notably in decision metrics. Further analysis revealed some difficulty in distinguishing between users at high and low risk, reinforcing the need to explore strategies to improve data interpretation and quality, and to promote more transparent and reliable ERD systems for mental disorders.",
        "authors": [
            "Horacio Thompson",
            "Marcelo Errecalde"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T16:26:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23325v1"
    },
    {
        "arxiv_id": "2511.23319v1",
        "title": "Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models",
        "abstract": "This work explores the challenge of building ``Machines that Can Remember'', framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: \\textbf{sparsity}, \\textbf{random-access flexibility}, and \\textbf{length generalization}. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90\\% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling.",
        "authors": [
            "Xiang Hu",
            "Zhanchao Zhou",
            "Ruiqi Liang",
            "Zehuan Li",
            "Wei Wu",
            "Jianguo Li"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T16:17:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23319v1"
    },
    {
        "arxiv_id": "2511.23311v1",
        "title": "Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach",
        "abstract": "Large-scale Vision Language Models (LVLMs) exhibit advanced capabilities in tasks that require visual information, including object detection. These capabilities have promising applications in various industrial domains, such as autonomous driving. For example, LVLMs can generate safety-oriented descriptions of videos captured by road-facing cameras. However, ensuring comprehensive safety requires monitoring driver-facing views as well to detect risky events, such as the use of mobiles while driving. Thus, the ability to process synchronized inputs is necessary from both driver-facing and road-facing cameras. In this study, we develop models and investigate the capabilities of LVLMs by constructing a dataset and evaluating their performance on this dataset. Our experimental results demonstrate that while pre-trained LVLMs have limited effectiveness, fine-tuned LVLMs can generate accurate and safety-aware driving instructions. Nonetheless, several challenges remain, particularly in detecting subtle or complex events in the video. Our findings and error analysis provide valuable insights that can contribute to the improvement of LVLM-based systems in this domain.",
        "authors": [
            "Haruki Sakajo",
            "Hiroshi Takato",
            "Hiroshi Tsutsui",
            "Komei Soda",
            "Hidetaka Kamigaito",
            "Taro Watanabe"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-28T16:09:36+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23311v1"
    },
    {
        "arxiv_id": "2511.23287v1",
        "title": "Transformer-Driven Triple Fusion Framework for Enhanced Multimodal Author Intent Classification in Low-Resource Bangla",
        "abstract": "The expansion of the Internet and social networks has led to an explosion of user-generated content. Author intent understanding plays a crucial role in interpreting social media content. This paper addresses author intent classification in Bangla social media posts by leveraging both textual and visual data. Recognizing limitations in previous unimodal approaches, we systematically benchmark transformer-based language models (mBERT, DistilBERT, XLM-RoBERTa) and vision architectures (ViT, Swin, SwiftFormer, ResNet, DenseNet, MobileNet), utilizing the Uddessho dataset of 3,048 posts spanning six practical intent categories. We introduce a novel intermediate fusion strategy that significantly outperforms early and late fusion on this task. Experimental results show that intermediate fusion, particularly with mBERT and Swin Transformer, achieves 84.11% macro-F1 score, establishing a new state-of-the-art with an 8.4 percentage-point improvement over prior Bangla multimodal approaches. Our analysis demonstrates that integrating visual context substantially enhances intent classification. Cross-modal feature integration at intermediate levels provides optimal balance between modality-specific representation and cross-modal learning. This research establishes new benchmarks and methodological standards for Bangla and other low-resource languages. We call our proposed framework BangACMM (Bangla Author Content MultiModal).",
        "authors": [
            "Ariful Islam",
            "Tanvir Mahmud",
            "Md Rifat Hossen"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-28T15:44:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23287v1"
    },
    {
        "arxiv_id": "2511.23281v1",
        "title": "MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)",
        "abstract": "Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks.   To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.",
        "authors": [
            "Aaron Steiner",
            "Ralph Peeters",
            "Christian Bizer"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T15:32:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23281v1"
    },
    {
        "arxiv_id": "2511.23271v1",
        "title": "Behavior-Equivalent Token: Single-Token Replacement for Long Prompts in LLMs",
        "abstract": "Carefully engineered system prompts play a critical role in guiding the behavior of LLM agents, but their considerable length introduces significant drawbacks, including increased inference latency, higher computational cost, and reduced effective context length. This raises the question of whether such lengthy prompts can be replaced by a drastically reduced number of tokens while preserving their behavioral effect on downstream tasks. To enable this, we propose a lightweight three-stage training framework that learns a single prompt-specific Behavior-Equivalent token ([BE]). The framework first trains [BE] to encode the natural-language content of the original system prompt via reconstruction, and then distills the prompt 's downstream behavior into this single token. Importantly, our method requires no access to model internals, no auxiliary compression models, and no labeled responses. Empirical evaluations on three datasets show that a single [BE] token achieves up to a 3000x reduction in prompt length, while retaining about 98% of the downstream performance of the original system prompts. This substantially reduces inference cost and leaves almost the entire context window available for user inputs.",
        "authors": [
            "Jiancheng Dong",
            "Pengyue Jia",
            "Jingyu Peng",
            "Maolin Wang",
            "Yuhao Wang",
            "Lixin Su",
            "Xin Sun",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Xiangyu Zhao"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T15:22:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23271v1"
    },
    {
        "arxiv_id": "2511.23264v1",
        "title": "BanglaSentNet: An Explainable Hybrid Deep Learning Framework for Multi-Aspect Sentiment Analysis with Cross-Domain Transfer Learning",
        "abstract": "Multi-aspect sentiment analysis of Bangla e-commerce reviews remains challenging due to limited annotated datasets, morphological complexity, code-mixing phenomena, and domain shift issues, affecting 300 million Bangla-speaking users. Existing approaches lack explainability and cross-domain generalization capabilities crucial for practical deployment. We present BanglaSentNet, an explainable hybrid deep learning framework integrating LSTM, BiLSTM, GRU, and BanglaBERT through dynamic weighted ensemble learning for multi-aspect sentiment classification. We introduce a dataset of 8,755 manually annotated Bangla product reviews across four aspects (Quality, Service, Price, Decoration) from major Bangladeshi e-commerce platforms. Our framework incorporates SHAP-based feature attribution and attention visualization for transparent insights. BanglaSentNet achieves 85% accuracy and 0.88 F1-score, outperforming standalone deep learning models by 3-7% and traditional approaches substantially. The explainability suite achieves 9.4/10 interpretability score with 87.6% human agreement. Cross-domain transfer learning experiments reveal robust generalization: zero-shot performance retains 67-76% effectiveness across diverse domains (BanglaBook reviews, social media, general e-commerce, news headlines); few-shot learning with 500-1000 samples achieves 90-95% of full fine-tuning performance, significantly reducing annotation costs. Real-world deployment demonstrates practical utility for Bangladeshi e-commerce platforms, enabling data-driven decision-making for pricing optimization, service improvement, and customer experience enhancement. This research establishes a new state-of-the-art benchmark for Bangla sentiment analysis, advances ensemble learning methodologies for low-resource languages, and provides actionable solutions for commercial applications.",
        "authors": [
            "Ariful Islam",
            "Md Rifat Hossen",
            "Tanvir Mahmud"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-28T15:17:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23264v1"
    },
    {
        "arxiv_id": "2511.23235v1",
        "title": "Tourism Question Answer System in Indian Language using Domain-Adapted Foundation Models",
        "abstract": "This article presents the first comprehensive study on designing a baseline extractive question-answering (QA) system for the Hindi tourism domain, with a specialized focus on the Varanasi-a cultural and spiritual hub renowned for its Bhakti-Bhaav (devotional ethos). Targeting ten tourism-centric subdomains-Ganga Aarti, Cruise, Food Court, Public Toilet, Kund, Museum, General, Ashram, Temple and Travel, the work addresses the absence of language-specific QA resources in Hindi for culturally nuanced applications. In this paper, a dataset comprising 7,715 Hindi QA pairs pertaining to Varanasi tourism was constructed and subsequently augmented with 27,455 pairs generated via Llama zero-shot prompting. We propose a framework leveraging foundation models-BERT and RoBERTa, fine-tuned using Supervised Fine-Tuning (SFT) and Low-Rank Adaptation (LoRA), to optimize parameter efficiency and task performance. Multiple variants of BERT, including pre-trained languages (e.g., Hindi-BERT), are evaluated to assess their suitability for low-resource domain-specific QA. Evaluation metrics - F1, BLEU, and ROUGE-L - highlight trade-offs between answer precision and linguistic fluency. Experiments demonstrate that LoRA-based fine-tuning achieves competitive performance (85.3\\% F1) while reducing trainable parameters by 98\\% compared to SFT, striking a balance between efficiency and accuracy. Comparative analysis across models reveals that RoBERTa with SFT outperforms BERT variants in capturing contextual nuances, particularly for culturally embedded terms (e.g., Aarti, Kund). This work establishes a foundational baseline for Hindi tourism QA systems, emphasizing the role of LORA in low-resource settings and underscoring the need for culturally contextualized NLP frameworks in the tourism domain.",
        "authors": [
            "Praveen Gatla",
            "Anushka",
            "Nikita Kanwar",
            "Gouri Sahoo",
            "Rajesh Kumar Mundotiya"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T14:44:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23235v1"
    },
    {
        "arxiv_id": "2511.23225v1",
        "title": "TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies",
        "abstract": "Native FP8 support in modern hardware is essential for training large Transformers, but is severely hindered by extreme activation outliers. Existing solutions either rely on complex mixed-precision engineering or invasive architectural modifications. This paper fundamentally challenges the conventional wisdom that outliers are data-driven. We demonstrate that extreme outliers are a data-independent, mechanically-produced artifact of training, originating from specific structural properties of the weight matrices (i.e., colinearity). Based on this insight, we propose TWEO (Transformers Without Extreme Outliers), a novel, non-invasive loss function. TWEO effectively prevents extreme outliers via a very simple loss term, which reduces outliers from 10000+ to less than 20. TWEO then enables full-model FP8 pre-training with neither engineering tricks nor architectural changes for both LLM and ViT. When standard FP8 training catastrophically collapses, TWEO achieves performance comparable to the BF16 baseline while delivering a 36% increase in training throughput. Also, TWEO enables a new quantization paradigm. Hardware-friendly W8A8 per-tensor static quantization of LLMs, previously considered completely unusable due to outliers, achieves SOTA performance for the first time on TWEO-trained models.",
        "authors": [
            "Guang Liang",
            "Jie Shao",
            "Ningyuan Tang",
            "Xinyao Liu",
            "Jianxin Wu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T14:33:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23225v1"
    },
    {
        "arxiv_id": "2511.23184v1",
        "title": "Listwise Preference Optimization with Element-wise Confusions for Aspect Sentiment Quad Prediction",
        "abstract": "Aspect sentiment quad prediction (ASQP) is inherently challenging to predict a structured quadruple with four core sentiment elements, including aspect term (a), aspect category (c), opinion term (o), and sentiment polarity (s). Prior methods relying on marker-based prediction struggle with modeling the intricate relationships among elements and experience sharp performance declines when predicting higher-order elements (e.g., c and s) under standard supervised fine-tuning. To address these limitations, we employ reasoning-based generation to output both the quadruple and a natural language rationale under element prefixes within a unified template, encouraging explicit relational reasoning and interpretability. To further enhance element-wise alignment, we introduce a listwise preference optimization framework for improving structural validity and relational coherence. Specifically, we generate element-wise confusable candidates via syntactic and semantic proximity, then train the model with listwise objectives to prefer the gold candidates over closely competing alternatives. Extensive experiments on four benchmark datasets demonstrate that our framework effectively improves quadruple prediction accuracy and explanation consistency.",
        "authors": [
            "Wenna Lai",
            "Haoran Xie",
            "Guandong Xu",
            "Qing Li",
            "S. Joe Qin"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T13:52:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23184v1"
    },
    {
        "arxiv_id": "2511.23174v1",
        "title": "Are LLMs Good Safety Agents or a Propaganda Engine?",
        "abstract": "Large Language Models (LLMs) are trained to refuse to respond to harmful content. However, systematic analyses of whether this behavior is truly a reflection of its safety policies or an indication of political censorship, that is practiced globally by countries, is lacking. Differentiating between safety influenced refusals or politically motivated censorship is hard and unclear. For this purpose we introduce PSP, a dataset built specifically to probe the refusal behaviors in LLMs from an explicitly political context. PSP is built by formatting existing censored content from two data sources, openly available on the internet: sensitive prompts in China generalized to multiple countries, and tweets that have been censored in various countries. We study: 1) impact of political sensitivity in seven LLMs through data-driven (making PSP implicit) and representation-level approaches (erasing the concept of politics); and, 2) vulnerability of models on PSP through prompt injection attacks (PIAs). Associating censorship with refusals on content with masked implicit intent, we find that most LLMs perform some form of censorship. We conclude with summarizing major attributes that can cause a shift in refusal distributions across models and contexts of different countries.",
        "authors": [
            "Neemesh Yadav",
            "Francesco Ortu",
            "Jiarui Liu",
            "Joeun Yook",
            "Bernhard Sch\u00f6lkopf",
            "Rada Mihalcea",
            "Alberto Cazzaniga",
            "Zhijing Jin"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T13:36:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23174v1"
    },
    {
        "arxiv_id": "2511.23136v1",
        "title": "Multi-chain Graph Refinement and Selection for Reliable Reasoning in Large Language Models",
        "abstract": "The complex reasoning ability of Large Language Models (LLMs) poses a critical bottleneck for their practical applications. Test-time expansion methods such as Tree-of-Thought (ToT) and Graph-of-Thought (GoT) enhance reasoning by introducing intermediate reasoning structures, tree search, or graph-based exploration mechanisms. However, their reasoning strategies suffer from limited diversity, redundant search branches, and inadequate integration and error correction across heterogeneous reasoning paths. To address these limitations, we propose a novel reasoning framework called Multi-chain Graph Refinement & Selection (MGRS), which first generates multiple diverse reasoning trajectories for a given problem, refines candidate responses using a composite self- and cross-verification strategy, then constructs a reasoning relation graph and estimates the success rate of intermediate nodes, and finally computes cumulative success rates to select the most reliable answer and corresponding reasoning trajectory. Experimental results demonstrate that MGRS significantly advances both the reasoning capability and computational efficiency of reasoning enhancement methods. Across six benchmark datasets spanning four distinct tasks, MGRS achieves an average accuracy of 82.9%, outperforming state-of-the-art baselines by a clear margin of 2.1%. Remarkably, on the 24-point game, MGRS attains 100% accuracy for the first time, while delivering a 13.6x speed-up compared to the leading Forest of Thoughts framework.",
        "authors": [
            "Yujiao Yang",
            "Jing Lian",
            "Linhui Li"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T12:35:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23136v1"
    },
    {
        "arxiv_id": "2511.23119v1",
        "title": "Dripper: Token-Efficient Main HTML Extraction with a Lightweight LM",
        "abstract": "Accurately and efficiently extracting main content from general web pages is of great significance for obtaining training data for large models. Using well-pre-trained decoder-only generative language models offers excellent document comprehension capabilities, thereby effectively enhancing parsing quality. However, it remains constrained by issues such as context window length, inference cost, and format hallucination. We present Dripper, an efficient HTML main content extraction framework powered by lightweight language models, which addresses these challenges through four key innovations: (1) We design a specialized HTML simplification algorithm that reduces input token count to 22\\% compared to raw HTML while preserving critical structural information; (2) We reformulate main content extraction as a semantic block sequence classification task, significantly reducing inference cost; (3) We introduce a controlled decoding mechanism that strictly constrains the output space through logits processors, effectively eliminating hallucination issues common in small-scale models; (4) We propose WebMainBench, an evaluation dataset containing over 7,800 web pages with meticulously human-annotated main content extraction labels. Experimental results demonstrate that using only a 0.6B parameter model, Dripper achieves state-of-the-art performance across all evaluation benchmarks and outperforms all baseline methods, attaining an ROUGE-N F1 score of 81.58\\%( 83.13\\% with fall-back strategy) on our proposed WebMainBench dataset.",
        "authors": [
            "Mengjie Liu",
            "Jiahui Peng",
            "Pei Chu",
            "Jiantao Qiu",
            "Ren Ma",
            "He Zhu",
            "Rui Min",
            "Lindong Lu",
            "Wenchang Ning",
            "Linfeng Hou",
            "Kaiwen Liu",
            "Yuan Qu",
            "Zhenxiang Li",
            "Chao Xu",
            "Zhongying Tu",
            "Wentao Zhang",
            "Conghui He"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T12:04:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23119v1"
    },
    {
        "arxiv_id": "2511.23101v1",
        "title": "Mind Reading or Misreading? LLMs on the Big Five Personality Test",
        "abstract": "We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs. enriched with linguistic and psychological cues). Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence. Performance varies substantially: Openness and Agreeableness are relatively easier to detect, while Extraversion and Neuroticism remain challenging. Although open-source models sometimes approach GPT-4 and prior benchmarks, no configuration yields consistently reliable predictions in zero-shot binary settings. Moreover, aggregate metrics such as accuracy and macro-F1 mask significant asymmetries, with per-class recall offering clearer diagnostic value. These findings show that current out-of-the-box LLMs are not yet suitable for APPT, and that careful coordination of prompt design, trait framing, and evaluation metrics is essential for interpretable results.",
        "authors": [
            "Francesco Di Cursi",
            "Chiara Boldrini",
            "Marco Conti",
            "Andrea Passarella"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T11:40:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23101v1"
    },
    {
        "arxiv_id": "2511.23088v1",
        "title": "Accent Placement Models for Rigvedic Sanskrit Text",
        "abstract": "The Rigveda, among the oldest Indian texts in Vedic Sanskrit, employs a distinctive pitch-accent system : ud\u0101tta, anud\u0101tta, svarita whose marks encode melodic and interpretive cues but are often absent from modern e-texts. This work develops a parallel corpus of accented-unaccented \u015blokas and conducts a controlled comparison of three strategies for automatic accent placement in Rigvedic verse: (i) full fine-tuning of ByT5, a byte-level Transformer that operates directly on Unicode combining marks, (ii) a from-scratch BiLSTM-CRF sequence-labeling baseline, and (iii) LoRA-based parameter-efficient fine-tuning atop ByT5.   Evaluation uses Word Error Rate (WER) and Character Error Rate (CER) for orthographic fidelity, plus a task-specific Diacritic Error Rate (DER) that isolates accent edits. Full ByT5 fine-tuning attains the lowest error across all metrics; LoRA offers strong efficiency-accuracy trade-offs, and BiLSTM-CRF serves as a transparent baseline. The study underscores practical requirements for accent restoration - Unicode-safe preprocessing, mark-aware tokenization, and evaluation that separates grapheme from accent errors - and positions heritage-language technology as an emerging NLP area connecting computational modeling with philological and pedagogical aims. Results establish reproducible baselines for Rigvedic accent restoration and provide guidance for downstream tasks such as accent-aware OCR, ASR/chant synthesis, and digital scholarship.",
        "authors": [
            "Akhil Rajeev P",
            "Annarao Kulkarni"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T11:22:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23088v1"
    },
    {
        "arxiv_id": "2511.23071v1",
        "title": "Bharat Scene Text: A Novel Comprehensive Dataset and Benchmark for Indian Language Scene Text Understanding",
        "abstract": "Reading scene text, that is, text appearing in images, has numerous application areas, including assistive technology, search, and e-commerce. Although scene text recognition in English has advanced significantly and is often considered nearly a solved problem, Indian language scene text recognition remains an open challenge. This is due to script diversity, non-standard fonts, and varying writing styles, and, more importantly, the lack of high-quality datasets and open-source models. To address these gaps, we introduce the Bharat Scene Text Dataset (BSTD) - a large-scale and comprehensive benchmark for studying Indian Language Scene Text Recognition. It comprises more than 100K words that span 11 Indian languages and English, sourced from over 6,500 scene images captured across various linguistic regions of India. The dataset is meticulously annotated and supports multiple scene text tasks, including: (i) Scene Text Detection, (ii) Script Identification, (iii) Cropped Word Recognition, and (iv) End-to-End Scene Text Recognition. We evaluated state-of-the-art models originally developed for English by adapting (fine-tuning) them for Indian languages. Our results highlight the challenges and opportunities in Indian language scene text recognition. We believe that this dataset represents a significant step toward advancing research in this domain. All our models and data are open source.",
        "authors": [
            "Anik De",
            "Abhirama Subramanyam Penamakuri",
            "Rajeev Yadav",
            "Aditya Rathore",
            "Harshiv Shah",
            "Devesh Sharma",
            "Sagar Agarwal",
            "Pravin Kumar",
            "Anand Mishra"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-28T10:58:37+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23071v1"
    },
    {
        "arxiv_id": "2511.23059v1",
        "title": "Conveying Imagistic Thinking in TCM Translation: A Prompt Engineering and LLM-Based Evaluation Framework",
        "abstract": "Traditional Chinese Medicine theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis. Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient and replicable HITL methodological pathway for translation of ancient, concept-dense texts like TCM.",
        "authors": [
            "Jiatong Han"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T10:35:10+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23059v1"
    },
    {
        "arxiv_id": "2511.23057v1",
        "title": "Standard Occupation Classifier -- A Natural Language Processing Approach",
        "abstract": "Standard Occupational Classifiers (SOC) are systems used to categorize and classify different types of jobs and occupations based on their similarities in terms of job duties, skills, and qualifications. Integrating these facets with Big Data from job advertisement offers the prospect to investigate labour demand that is specific to various occupations. This project investigates the use of recent developments in natural language processing to construct a classifier capable of assigning an occupation code to a given job advertisement. We develop various classifiers for both UK ONS SOC and US O*NET SOC, using different Language Models. We find that an ensemble model, which combines Google BERT and a Neural Network classifier while considering job title, description, and skills, achieved the highest prediction accuracy. Specifically, the ensemble model exhibited a classification accuracy of up to 61% for the lower (or fourth) tier of SOC, and 72% for the third tier of SOC. This model could provide up to date, accurate information on the evolution of the labour market using job advertisements.",
        "authors": [
            "Sidharth Rony",
            "Jack Patman"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T10:30:37+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23057v1"
    },
    {
        "arxiv_id": "2511.23056v1",
        "title": "Decoding the Past: Explainable Machine Learning Models for Dating Historical Texts",
        "abstract": "Accurately dating historical texts is essential for organizing and interpreting cultural heritage collections. This article addresses temporal text classification using interpretable, feature-engineered tree-based machine learning models. We integrate five feature categories - compression-based, lexical structure, readability, neologism detection, and distance features - to predict the temporal origin of English texts spanning five centuries. Comparative analysis shows that these feature domains provide complementary temporal signals, with combined models outperforming any individual feature set. On a large-scale corpus, we achieve 76.7% accuracy for century-scale prediction and 26.1% for decade-scale classification, substantially above random baselines (20% and 2.3%). Under relaxed temporal precision, performance increases to 96.0% top-2 accuracy for centuries and 85.8% top-10 accuracy for decades. The final model exhibits strong ranking capabilities with AUCROC up to 94.8% and AUPRC up to 83.3%, and maintains controlled errors with mean absolute deviations of 27 years and 30 years, respectively. For authentication-style tasks, binary models around key thresholds (e.g., 1850-1900) reach 85-98% accuracy. Feature importance analysis identifies distance features and lexical structure as most informative, with compression-based features providing complementary signals. SHAP explainability reveals systematic linguistic evolution patterns, with the 19th century emerging as a pivot point across feature domains. Cross-dataset evaluation on Project Gutenberg highlights domain adaptation challenges, with accuracy dropping by 26.4 percentage points, yet the computational efficiency and interpretability of tree-based models still offer a scalable, explainable alternative to neural architectures.",
        "authors": [
            "Paulo J. N. Pinto",
            "Armando J. Pinho",
            "Diogo Pratas"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T10:27:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23056v1"
    },
    {
        "arxiv_id": "2511.23041v1",
        "title": "Social Perceptions of English Spelling Variation on Twitter: A Comparative Analysis of Human and LLM Responses",
        "abstract": "Spelling variation (e.g. funnnn vs. fun) can influence the social perception of texts and their writers: we often have various associations with different forms of writing (is the text informal? does the writer seem young?). In this study, we focus on the social perception of spelling variation in online writing in English and study to what extent this perception is aligned between humans and large language models (LLMs). Building on sociolinguistic methodology, we compare LLM and human ratings on three key social attributes of spelling variation (formality, carefulness, age). We find generally strong correlations in the ratings between humans and LLMs. However, notable differences emerge when we analyze the distribution of ratings and when comparing between different types of spelling variation.",
        "authors": [
            "Dong Nguyen",
            "Laura Rosseel"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T10:06:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.23041v1"
    },
    {
        "arxiv_id": "2511.22978v1",
        "title": "ShoppingComp: Are LLMs Really Ready for Your Shopping Cart?",
        "abstract": "We present ShoppingComp, a challenging real-world benchmark for rigorously evaluating LLM-powered shopping agents on three core capabilities: precise product retrieval, expert-level report generation, and safety critical decision making. Unlike prior e-commerce benchmarks, ShoppingComp introduces highly complex tasks under the principle of guaranteeing real products and ensuring easy verifiability, adding a novel evaluation dimension for identifying product safety hazards alongside recommendation accuracy and report quality. The benchmark comprises 120 tasks and 1,026 scenarios, curated by 35 experts to reflect authentic shopping needs. Results reveal stark limitations of current LLMs: even state-of-the-art models achieve low performance (e.g., 11.22% for GPT-5, 3.92% for Gemini-2.5-Flash). These findings highlight a substantial gap between research benchmarks and real-world deployment, where LLMs make critical errors such as failure to identify unsafe product usage or falling for promotional misinformation, leading to harmful recommendations. ShoppingComp fills the gap and thus establishes a new standard for advancing reliable and practical agents in e-commerce.",
        "authors": [
            "Huaixiao Tou",
            "Ying Zeng",
            "Cong Ma",
            "Muzhi Li",
            "Minghao Li",
            "Weijie Yuan",
            "He Zhang",
            "Kai Jia"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T08:32:54+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22978v1"
    },
    {
        "arxiv_id": "2511.22977v1",
        "title": "Pooling Attention: Evaluating Pretrained Transformer Embeddings for Deception Classification",
        "abstract": "This paper investigates fake news detection as a downstream evaluation of Transformer representations, benchmarking encoder-only and decoder-only pre-trained models (BERT, GPT-2, Transformer-XL) as frozen embedders paired with lightweight classifiers. Through controlled preprocessing comparing pooling versus padding and neural versus linear heads, results demonstrate that contextual self-attention encodings consistently transfer effectively. BERT embeddings combined with logistic regression outperform neural baselines on LIAR dataset splits, while analyses of sequence length and aggregation reveal robustness to truncation and advantages from simple max or average pooling. This work positions attention-based token encoders as robust, architecture-centric foundations for veracity tasks, isolating Transformer contributions from classifier complexity.",
        "authors": [
            "Sumit Mamtani",
            "Abhijeet Bhure"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T08:32:49+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22977v1"
    },
    {
        "arxiv_id": "2511.22972v1",
        "title": "Training-Free Loosely Speculative Decoding: Accepting Semantically Correct Drafts Beyond Exact Match",
        "abstract": "Large language models (LLMs) achieve strong performance across diverse tasks but suffer from high inference latency due to their autoregressive generation. Speculative Decoding (SPD) mitigates this issue by verifying candidate tokens in parallel from a smaller draft model, yet its strict exact-match verification discards many semantically valid continuations. Moreover, existing training-based SPD methods often suffer from performance degradation on out-of-distribution (OOD) tasks. To this end, we propose Training-Free Loosely Speculative Decoding (FLy), a novel method that loosens the rigid verification criterion by leveraging the target model's self-corrective behavior to judge whether a draft-target mismatch remains semantically valid. FLy introduces a two-tier mechanism: an entropy-level gate that identifies whether the current token allows multiple plausible alternatives or is nearly deterministic, and a token-level deferred window that distinguishes genuine errors from differently worded yet semantically correct variants. To further reduce latency, we design a multi-level acceleration strategy that accelerates not only the target model but also the drafter itself. Owing to its training-free design, FLy composes seamlessly with arbitrary draft-target pairs and generalizes across models and domains without hyperparameter re-tuning. Experiments show that FLy preserves more than 99% of the target model's accuracy while achieving an average 2.81x speedup on Llama-3.1-70B-Instruct and 5.07x speedup on the 405B variant. Notably, on out-of-domain datasets, our method remains highly effective and outperforms the training-based method EAGLE-3 by 1.62x.",
        "authors": [
            "Jinze Li",
            "Yixing Xu",
            "Guanchen Li",
            "Shuo Yang",
            "Jinfeng Xu",
            "Xuanwu Yin",
            "Dong Li",
            "Edith C. H. Ngai",
            "Emad Barsoum"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T08:23:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22972v1"
    },
    {
        "arxiv_id": "2511.22943v1",
        "title": "Visual Puns from Idioms: An Iterative LLM-T2IM-MLLM Framework",
        "abstract": "We study idiom-based visual puns--images that align an idiom's literal and figurative meanings--and present an iterative framework that coordinates a large language model (LLM), a text-to-image model (T2IM), and a multimodal LLM (MLLM) for automatic generation and evaluation. Given an idiom, the system iteratively (i) generates detailed visual prompts, (ii) synthesizes an image, (iii) infers the idiom from the image, and (iv) refines the prompt until recognition succeeds or a step limit is reached. Using 1,000 idioms as inputs, we synthesize a corresponding dataset of visual pun images with paired prompts, enabling benchmarking of both generation and understanding. Experiments across 10 LLMs, 10 MLLMs, and one T2IM (Qwen-Image) show that MLLM choice is the primary performance driver: GPT achieves the highest accuracies, Gemini follows, and the best open-source MLLM (Gemma) is competitive with some closed models. On the LLM side, Claude attains the strongest average performance for prompt generation.",
        "authors": [
            "Kelaiti Xiao",
            "Liang Yang",
            "Dongyu Zhang",
            "Paerhati Tulajiang",
            "Hongfei Lin"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T07:30:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22943v1"
    },
    {
        "arxiv_id": "2511.22929v1",
        "title": "Artwork Interpretation with Vision Language Models: A Case Study on Emotions and Emotion Symbols",
        "abstract": "Emotions are a fundamental aspect of artistic expression. Due to their abstract nature, there is a broad spectrum of emotion realization in artworks. These are subject to historical change and their analysis requires expertise in art history. In this article, we investigate which aspects of emotional expression can be detected by current (2025) vision language models (VLMs). We present a case study of three VLMs (Llava-Llama and two Qwen models) in which we ask these models four sets of questions of increasing complexity about artworks (general content, emotional content, expression of emotions, and emotion symbols) and carry out a qualitative expert evaluation. We find that the VLMs recognize the content of the images surprisingly well and often also which emotions they depict and how they are expressed. The models perform best for concrete images but fail for highly abstract or highly symbolic images. Reliable recognition of symbols remains fundamentally difficult. Furthermore, the models continue to exhibit the well-known LLM weakness of providing inconsistent answers to related questions.",
        "authors": [
            "Sebastian Pad\u00f3",
            "Kerstin Thomas"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-28T07:04:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22929v1"
    },
    {
        "arxiv_id": "2511.22904v1",
        "title": "Language-conditioned world model improves policy generalization by reading environmental descriptions",
        "abstract": "To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying \"what to do\". Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to unseen games or rely on limiting assumptions. For instance, assuming that the latency induced by inference-time planning is tolerable for the target task or expert demonstrations are available. Expanding on this line of research, we focus on improving policy generalization from a language-conditioned world model while dropping these assumptions. We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations. Our method proposes Language-aware Encoder for Dreamer World Model (LED-WM) built on top of DreamerV3. LED-WM features an observation encoder that uses an attention mechanism to explicitly ground language descriptions to entities in the observation. We show that policies trained with LED-WM generalize more effectively to unseen games described by novel dynamics and language compared to other baselines in several settings in two environments: MESSENGER and MESSENGER-WM.To highlight how the policy can leverage the trained world model before real-world deployment, we demonstrate the policy can be improved through fine-tuning on synthetic test trajectories generated by the world model.",
        "authors": [
            "Anh Nguyen",
            "Stefan Lee"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T06:13:27+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22904v1"
    },
    {
        "arxiv_id": "2511.22891v1",
        "title": "ORION: Teaching Language Models to Reason Efficiently in the Language of Thought",
        "abstract": "Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose \"thinking\" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.",
        "authors": [
            "Kumar Tanmay",
            "Kriti Aggarwal",
            "Paul Pu Liang",
            "Subhabrata Mukherjee"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-28T05:41:55+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22891v1"
    },
    {
        "arxiv_id": "2511.22883v1",
        "title": "FEANEL: A Benchmark for Fine-Grained Error Analysis in K-12 English Writing",
        "abstract": "Large Language Models (LLMs) have transformed artificial intelligence, offering profound opportunities for educational applications. However, their ability to provide fine-grained educational feedback for K-12 English writing remains underexplored. In this paper, we challenge the error analysis and pedagogical skills of LLMs by introducing the problem of Fine-grained Error Analysis for English Learners and present the Fine-grained Error ANalysis for English Learners (FEANEL) Benchmark. The benchmark comprises 1,000 essays written by elementary and secondary school students, and a well-developed English writing error taxonomy. Each error is annotated by language education experts and categorized by type, severity, and explanatory feedback, using a part-of-speech-based taxonomy they co-developed. We evaluate state-of-the-art LLMs on the FEANEL Benchmark to explore their error analysis and pedagogical abilities. Experimental results reveal significant gaps in current LLMs' ability to perform fine-grained error analysis, highlighting the need for advancements in particular methods for educational applications.",
        "authors": [
            "Jingheng Ye",
            "Shen Wang",
            "Jiaqi Chen",
            "Hebin Wang",
            "Deqing Zou",
            "Yanyu Zhu",
            "Jiwei Tang",
            "Hai-Tao Zheng",
            "Ruitong Liu",
            "Haoyang Li",
            "Yanfeng Wang",
            "Qingsong Wen"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T05:17:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22883v1"
    },
    {
        "arxiv_id": "2511.22869v1",
        "title": "JBE-QA: Japanese Bar Exam QA Dataset for Assessing Legal Domain Knowledge",
        "abstract": "We introduce JBE-QA, a Japanese Bar Exam Question-Answering dataset to evaluate large language models' legal knowledge. Derived from the multiple-choice (tanto-shiki) section of the Japanese bar exam (2015-2024), JBE-QA provides the first comprehensive benchmark for Japanese legal-domain evaluation of LLMs. It covers the Civil Code, the Penal Code, and the Constitution, extending beyond the Civil Code focus of prior Japanese resources. Each question is decomposed into independent true/false judgments with structured contextual fields. The dataset contains 3,464 items with balanced labels. We evaluate 26 LLMs, including proprietary, open-weight, Japanese-specialised, and reasoning models. Our results show that proprietary models with reasoning enabled perform best, and the Constitution questions are generally easier than the Civil Code or the Penal Code questions.",
        "authors": [
            "Zhihan Cao",
            "Fumihito Nishino",
            "Hiroaki Yamada",
            "Nguyen Ha Thanh",
            "Yusuke Miyao",
            "Ken Satoh"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T04:16:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22869v1"
    },
    {
        "arxiv_id": "2511.22858v1",
        "title": "RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms",
        "abstract": "This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based LLM system should possess in order to support Japanese medical litigation procedures complying with legal norms. In litigation, expert commissioners, such as physicians, architects, accountants, and engineers, provide specialized knowledge to help judges clarify points of dispute. When considering the substitution of these expert roles with a RAG-based LLM system, the constraint of strict adherence to legal norms is imposed. Specifically, three requirements arise: (1) the retrieval module must retrieve appropriate external knowledge relevant to the disputed issues in accordance with the principle prohibiting the use of private knowledge, (2) the responses generated must originate from the context provided by the RAG and remain faithful to that context, and (3) the retrieval module must reference external knowledge with appropriate timestamps corresponding to the issues at hand. This paper discusses the design of a RAG-based LLM system that satisfies these requirements.",
        "authors": [
            "Yuya Ishihara",
            "Atsushi Keyaki",
            "Hiroaki Yamada",
            "Ryutaro Ohara",
            "Mihoko Sumida"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T03:28:27+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22858v1"
    },
    {
        "arxiv_id": "2511.22818v1",
        "title": "Mitigating Semantic Drift: Evaluating LLMs' Efficacy in Psychotherapy through MI Dialogue Summarization",
        "abstract": "Recent advancements in large language models (LLMs) have shown their potential across both general and domain-specific tasks. However, there is a growing concern regarding their lack of sensitivity, factual incorrectness in responses, inconsistent expressions of empathy, bias, hallucinations, and overall inability to capture the depth and complexity of human understanding, especially in low-resource and sensitive domains such as psychology. To address these challenges, our study employs a mixed-methods approach to evaluate the efficacy of LLMs in psychotherapy. We use LLMs to generate precise summaries of motivational interviewing (MI) dialogues and design a two-stage annotation scheme based on key components of the Motivational Interviewing Treatment Integrity (MITI) framework, namely evocation, collaboration, autonomy, direction, empathy, and a non-judgmental attitude. Using expert-annotated MI dialogues as ground truth, we formulate multi-class classification tasks to assess model performance under progressive prompting techniques, incorporating one-shot and few-shot prompting. Our results offer insights into LLMs' capacity for understanding complex psychological constructs and highlight best practices to mitigate ``semantic drift\" in therapeutic settings. Our work contributes not only to the MI community by providing a high-quality annotated dataset to address data scarcity in low-resource domains but also critical insights for using LLMs for precise contextual interpretation in complex behavioral therapy.",
        "authors": [
            "Vivek Kumar",
            "Pushpraj Singh Rajawat",
            "Eirini Ntoutsi"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-28T00:37:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22818v1"
    },
    {
        "arxiv_id": "2511.22813v1",
        "title": "Intelligent Neural Networks: From Layered Architectures to Graph-Organized Intelligence",
        "abstract": "Biological neurons exhibit remarkable intelligence: they maintain internal states, communicate selectively with other neurons, and self-organize into complex graphs rather than rigid hierarchical layers. What if artificial intelligence could emerge from similarly intelligent computational units? We introduce Intelligent Neural Networks (INN), a paradigm shift where neurons are first-class entities with internal memory and learned communication patterns, organized in complete graphs rather than sequential layers.   Each Intelligent Neuron combines selective state-space dynamics (knowing when to activate) with attention-based routing (knowing to whom to send signals), enabling emergent computation through graph-structured interactions. On the standard Text8 character modeling benchmark, INN achieves 1.705 Bit-Per-Character (BPC), significantly outperforming a comparable Transformer (2.055 BPC) and matching a highly optimized LSTM baseline. Crucially, a parameter-matched baseline of stacked Mamba blocks fails to converge (>3.4 BPC) under the same training protocol, demonstrating that INN's graph topology provides essential training stability. Ablation studies confirm this: removing inter-neuron communication degrades performance or leads to instability, proving the value of learned neural routing.   This work demonstrates that neuron-centric design with graph organization is not merely bio-inspired -- it is computationally effective, opening new directions for modular, interpretable, and scalable neural architectures.",
        "authors": [
            "Antoine Salomon"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-27T23:59:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22813v1"
    },
    {
        "arxiv_id": "2511.22788v1",
        "title": "PRISM: Privacy-Aware Routing for Adaptive Cloud-Edge LLM Inference via Semantic Sketch Collaboration",
        "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities in natural language understanding and generation, but incur high communication overhead and privacy risks in cloud deployments, while facing compute and memory constraints when confined to edge devices. Cloud-edge inference has emerged as a promising paradigm for improving privacy in LLM services by retaining sensitive computations on local devices. However, existing cloud-edge inference approaches apply uniform privacy protection without considering input sensitivity, resulting in unnecessary perturbation and degraded utility even for non-sensitive tokens. To address this limitation, we propose Privacy-aware Routing for Inference with Semantic Modulation (PRISM), a context-aware framework that dynamically balances privacy and inference quality. PRISM executes in four stages: (1) the edge device profiles entity-level sensitivity; (2) a soft gating module on the edge selects an execution mode - cloud, edge, or collaboration; (3) for collaborative paths, the edge applies adaptive two-layer local differential privacy based on entity risks; and (4) the cloud LLM generates a semantic sketch from the perturbed prompt, which is then refined by the edge-side small language model (SLM) using local context. Our results show that PRISM consistently achieves superior privacy-utility trade-offs across various scenarios, reducing energy consumption and latency to 40-50% of baseline methods such as Uniform and Selective LDP, while maintaining high output quality under strong privacy constraints. These findings are validated through comprehensive evaluations involving realistic prompts, actual energy measurements, and heterogeneous cloud-edge model deployments.",
        "authors": [
            "Junfei Zhan",
            "Haoxun Shen",
            "Zheng Lin",
            "Tengjiao He"
        ],
        "category": "cs.CR",
        "submitted_date": "2025-11-27T22:32:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22788v1"
    },
    {
        "arxiv_id": "2511.22769v1",
        "title": "Modeling Romanized Hindi and Bengali: Dataset Creation and Multilingual LLM Integration",
        "abstract": "The development of robust transliteration techniques to enhance the effectiveness of transforming Romanized scripts into native scripts is crucial for Natural Language Processing tasks, including sentiment analysis, speech recognition, information retrieval, and intelligent personal assistants. Despite significant advancements, state-of-the-art multilingual models still face challenges in handling Romanized script, where the Roman alphabet is adopted to represent the phonetic structure of diverse languages. Within the South Asian context, where the use of Romanized script for Indo-Aryan languages is widespread across social media and digital communication platforms, such usage continues to pose significant challenges for cutting-edge multilingual models. While a limited number of transliteration datasets and models are available for Indo-Aryan languages, they generally lack sufficient diversity in pronunciation and spelling variations, adequate code-mixed data for large language model (LLM) training, and low-resource adaptation. To address this research gap, we introduce a novel transliteration dataset for two popular Indo-Aryan languages, Hindi and Bengali, which are ranked as the 3rd and 7th most spoken languages worldwide. Our dataset comprises nearly 1.8 million Hindi and 1 million Bengali transliteration pairs. In addition to that, we pre-train a custom multilingual seq2seq LLM based on Marian architecture using the developed dataset. Experimental results demonstrate significant improvements compared to existing relevant models in terms of BLEU and CER metrics.",
        "authors": [
            "Kanchon Gharami",
            "Quazi Sarwar Muhtaseem",
            "Deepti Gupta",
            "Lavanya Elluri",
            "Shafika Showkat Moni"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T21:39:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22769v1"
    },
    {
        "arxiv_id": "2512.02056v1",
        "title": "Reversing Large Language Models for Efficient Training and Fine-Tuning",
        "abstract": "Large Language Models (LLMs) are known for their expensive and time-consuming training. Thus, oftentimes, LLMs are fine-tuned to address a specific task, given the pretrained weights of a pre-trained LLM considered a foundation model. In this work, we introduce memory-efficient, reversible architectures for LLMs, inspired by symmetric and symplectic differential equations, and investigate their theoretical properties. Different from standard, baseline architectures that store all intermediate activations, the proposed models use time-reversible dynamics to retrieve hidden states during backpropagation, relieving the need to store activations. This property allows for a drastic reduction in memory consumption, allowing for the processing of larger batch sizes for the same available memory, thereby offering improved throughput. In addition, we propose an efficient method for converting existing, non-reversible LLMs into reversible architectures through fine-tuning, rendering our approach practical for exploiting existing pre-trained models. Our results show comparable or improved performance on several datasets and benchmarks, on several LLMs, building a scalable and efficient path towards reducing the memory and computational costs associated with both training from scratch and fine-tuning of LLMs.",
        "authors": [
            "Eshed Gal",
            "Moshe Eliasof",
            "Javier Turek",
            "Uri Ascher",
            "Eran Treister",
            "Eldad Haber"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T19:32:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02056v1"
    },
    {
        "arxiv_id": "2511.22715v1",
        "title": "ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.",
        "authors": [
            "Alberto Compagnoni",
            "Marco Morini",
            "Sara Sarto",
            "Federico Cocchi",
            "Davide Caffagni",
            "Marcella Cornia",
            "Lorenzo Baraldi",
            "Rita Cucchiara"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-27T19:01:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22715v1"
    },
    {
        "arxiv_id": "2511.22697v1",
        "title": "Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations",
        "abstract": "Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. Inspired by functional specificity in neuroscience, we hypothesize that it is more effective to finetune sparse model representations specific to a given task. In this work, we introduce Robotic Steering, a finetuning approach grounded in mechanistic interpretability that leverages few-shot demonstrations to identify and selectively finetune task-specific attention heads aligned with the physical, visual, and linguistic requirements of robotic tasks. Through comprehensive on-robot evaluations with a Franka Emika robot arm, we demonstrate that Robotic Steering outperforms LoRA while achieving superior robustness under task variation, reduced computational cost, and enhanced interpretability for adapting VLAs to diverse robotic tasks.",
        "authors": [
            "Chancharik Mitra",
            "Yusen Luo",
            "Raj Saravanan",
            "Dantong Niu",
            "Anirudh Pai",
            "Jesse Thomason",
            "Trevor Darrell",
            "Abrar Anwar",
            "Deva Ramanan",
            "Roei Herzig"
        ],
        "category": "cs.RO",
        "submitted_date": "2025-11-27T18:50:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22697v1"
    },
    {
        "arxiv_id": "2511.22612v1",
        "title": "Improving LLM-based Ontology Matching with fine-tuning on synthetic data",
        "abstract": "Large Language Models (LLMs) are increasingly being integrated into various components of Ontology Matching pipelines. This paper investigates the capability of LLMs to perform ontology matching directly on ontology modules and generate the corresponding alignments. Furthermore, it is explored how a dedicated fine-tuning strategy can enhance the model's matching performance in a zero-shot setting. The proposed method incorporates a search space reduction technique to select relevant subsets from both source and target ontologies, which are then used to automatically construct prompts. Recognizing the scarcity of reference alignments for training, a novel LLM-based approach is introduced for generating a synthetic dataset. This process creates a corpus of ontology submodule pairs and their corresponding reference alignments, specifically designed to fine-tune an LLM for the ontology matching task. The proposed approach was evaluated on the Conference, Geolink, Enslaved, Taxon, and Hydrography datasets from the OAEI complex track. The results demonstrate that the LLM fine-tuned on the synthetically generated data exhibits superior performance compared to the non-fine-tuned base model. The key contribution is a strategy that combines automatic dataset generation with fine-tuning to effectively adapt LLMs for ontology matching tasks.",
        "authors": [
            "Guilherme Sousa",
            "Rinaldo Lima",
            "Cassia Trojahn"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T16:46:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22612v1"
    },
    {
        "arxiv_id": "2511.22584v1",
        "title": "Smarter, not Bigger: Fine-Tuned RAG-Enhanced LLMs for Automotive HIL Testing",
        "abstract": "Hardware-in-the-Loop (HIL) testing is essential for automotive validation but suffers from fragmented and underutilized test artifacts. This paper presents HIL-GPT, a retrieval-augmented generation (RAG) system integrating domain-adapted large language models (LLMs) with semantic retrieval. HIL-GPT leverages embedding fine-tuning using a domain-specific dataset constructed via heuristic mining and LLM-assisted synthesis, combined with vector indexing for scalable, traceable test case and requirement retrieval. Experiments show that fine-tuned compact models, such as \\texttt{bge-base-en-v1.5}, achieve a superior trade-off between accuracy, latency, and cost compared to larger models, challenging the notion that bigger is always better. An A/B user study further confirms that RAG-enhanced assistants improve perceived helpfulness, truthfulness, and satisfaction over general-purpose LLMs. These findings provide insights for deploying efficient, domain-aligned LLM-based assistants in industrial HIL environments.",
        "authors": [
            "Chao Feng",
            "Zihan Liu",
            "Siddhant Gupta",
            "Gongpei Cui",
            "Jan von der Assen",
            "Burkhard Stiller"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T16:18:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22584v1"
    },
    {
        "arxiv_id": "2511.22582v1",
        "title": "Extension Condition \"violations\" and Merge optimality constraints",
        "abstract": "We analyze, using the mathematical formulation of Merge within the Strong Minimalist Thesis framework, a set of linguistic phenomena, including head-to-head movement, phrasal affixes and syntactic cliticization, verb-particle alternation, and operator-variable phenomena. These are often regarded as problematic, as violations of the Extension Condition. We show that, in fact, all of these phenomena can be explained without involving any EC violation. We first show that derivations using Sideward Merge are possible for all of these cases: these respect EC, though they involve some amount of optimality violations, with respect to Resource Restrictions cost functions, andthe amount of violation differs among these cases. We show that all the cases that involve large optimality violations can be derived in alternative ways involving neither EC nor the use of SM. The main remaining case (head-to-head movement) only involves SM with minimal violations of optimality (near equilibrium fluctuations). We analyze explicitly also the cases of multiple wh-fronting, clusters of clitics in Romance languages and possessor agreement construction in Korean, and how an explanation of these phenomena based on SM can be made compatible with the colored operad generators for phases and theta roles. We also show that the EC condition has a clear algebraic meaning in the mathematical formulation of Merge and is therefore an intrinsic structural algebraic constraint of the model, rather than an additional assumption. We also show that the minimal optimality violating SM plays a structural role in the Markovian properties of Merge, and we compare different optimality conditions coming from Minimal Search and from Resource Restriction in terms of their effect on the dynamics of the Hopf algebra Markov chain, in a simple explicit example.",
        "authors": [
            "Matilde Marcolli",
            "Richard Larson",
            "Riny Huijbregts"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T16:15:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22582v1"
    },
    {
        "arxiv_id": "2511.22570v1",
        "title": "DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning",
        "abstract": "Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.",
        "authors": [
            "Zhihong Shao",
            "Yuxiang Luo",
            "Chengda Lu",
            "Z. Z. Ren",
            "Jiewen Hu",
            "Tian Ye",
            "Zhibin Gou",
            "Shirong Ma",
            "Xiaokang Zhang"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-27T16:01:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22570v1"
    },
    {
        "arxiv_id": "2511.22503v1",
        "title": "Joint Speech and Text Training for LLM-Based End-to-End Spoken Dialogue State Tracking",
        "abstract": "End-to-end spoken dialogue state tracking (DST) is made difficult by the tandem of having to handle speech input and data scarcity. Combining speech foundation encoders and large language models has been proposed in recent work as to alleviate some of this difficulty. Although this approach has been shown to result in strong spoken DST models, achieving state-of-the-art performance in realistic multi-turn DST, it struggles to generalize across domains and requires annotated spoken DST training data for each domain of interest. However, collecting such data for every target domain is both costly and difficult. Noting that textual DST data is more easily obtained for various domains, in this work, we propose jointly training on available spoken DST data and written textual data from other domains as a way to achieve cross-domain generalization. We conduct experiments which show the efficacy of our proposed method for getting good cross-domain DST performance without relying on spoken training data from the target domains.",
        "authors": [
            "Katia Vendrame",
            "Bolaji Yusuf",
            "Santosh Kesiraju",
            "\u0160imon Sedl\u00e1\u010dek",
            "Old\u0159ich Plchot",
            "Jan \u010cernock\u00fd"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T14:36:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22503v1"
    },
    {
        "arxiv_id": "2511.22499v1",
        "title": "What Shape Is Optimal for Masks in Text Removal?",
        "abstract": "The advent of generative models has dramatically improved the accuracy of image inpainting. In particular, by removing specific text from document images, reconstructing original images is extremely important for industrial applications. However, most existing methods of text removal focus on deleting simple scene text which appears in images captured by a camera in an outdoor environment. There is little research dedicated to complex and practical images with dense text. Therefore, we created benchmark data for text removal from images including a large amount of text. From the data, we found that text-removal performance becomes vulnerable against mask profile perturbation. Thus, for practical text-removal tasks, precise tuning of the mask shape is essential. This study developed a method to model highly flexible mask profiles and learn their parameters using Bayesian optimization. The resulting profiles were found to be character-wise masks. It was also found that the minimum cover of a text region is not optimal. Our research is expected to pave the way for a user-friendly guideline for manual masking.",
        "authors": [
            "Hyakka Nakada",
            "Marika Kubota"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-27T14:34:35+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22499v1"
    },
    {
        "arxiv_id": "2511.22482v1",
        "title": "Exploring Performance Variations in Finetuned Translators of Ultra-Low Resource Languages: Do Linguistic Differences Matter?",
        "abstract": "Finetuning pre-trained language models with small amounts of data is a commonly-used method to create translators for ultra-low resource languages such as endangered Indigenous languages. However, previous works have reported substantially different performances with translators created using similar methodology and data. In this work we systematically explored possible causes of the performance difference, aiming to determine whether it was a product of different cleaning procedures, limitations of the pre-trained models, the size of the base model, or the size of the training dataset, studying both directions of translation. Our studies, using two Brazilian Indigenous languages, related but with significant structural linguistic characteristics, indicated none or very limited influence from those training factors, suggesting differences between languages may play a significant role in the ability to produce translators by fine-tuning pre-trained models.",
        "authors": [
            "Isabel Gon\u00e7alves",
            "Paulo Cavalin",
            "Claudio Pinhanez"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T14:15:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22482v1"
    },
    {
        "arxiv_id": "2511.22402v1",
        "title": "Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs",
        "abstract": "Large Language Models (LLMs) are increasingly used in clinical settings, where sensitivity to linguistic uncertainty can influence diagnostic interpretation and decision-making. Yet little is known about where such epistemic cues are internally represented within these models. Distinct from uncertainty quantification, which measures output confidence, this work examines input-side representational sensitivity to linguistic uncertainty in medical text. We curate a contrastive dataset of clinical statements varying in epistemic modality (e.g., 'is consistent with' vs. 'may be consistent with') and propose Model Sensitivity to Uncertainty (MSU), a layerwise probing metric that quantifies activation-level shifts induced by uncertainty cues. Our results show that LLMs exhibit structured, depth-dependent sensitivity to clinical uncertainty, suggesting that epistemic information is progressively encoded in deeper layers. These findings reveal how linguistic uncertainty is internally represented in LLMs, offering insight into their interpretability and epistemic reliability.",
        "authors": [
            "Srivarshinee Sridhar",
            "Raghav Kaushik Ravi",
            "Kripabandhu Ghosh"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T12:26:06+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22402v1"
    },
    {
        "arxiv_id": "2511.22367v1",
        "title": "SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning",
        "abstract": "Continual learning, one's ability to adapt to a sequence of tasks without forgetting previously acquired knowledge, remains a major challenge in machine learning and a key gap between artificial and human intelligence. While regularisation and replay perform well in vision, they lag behind multi-task learning for large language models (LLMs), especially at scale with many tasks. We revisit replay and argue that two failure modes drive this gap: selection (what to rehearse) and integration (how to consolidate new knowledge). To address selection, we propose Surprise-prioritised Replay (SuRe), a simple, architecture-agnostic rule that ranks and stores the most surprising (high Negative Log-Likelihood) sequences. SuRe achieves state-of-the-art performance in the Large Number of Tasks (LNT) setting and delivers the best overall average across both Standard CL and LNT benchmarks. To address integration, we add a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA), enabling rapid adaptation while stabilising long-term knowledge. Combining SuRe with the dual learner yields further gains, including improvements of up to +5 accuracy points on LNT over prior SOTA. Ablation studies confirm that our proposed method remains robust under reduced replay frequency and small buffer size, demonstrating both effectiveness and sample efficiency. Taken together, our results establish replay as a strong baseline for continual LLM fine-tuning and demonstrate that surprise-based selection and slow-weight consolidation are complementary components for mitigating catastrophic forgetting.",
        "authors": [
            "Hugo Hazard",
            "Zafeirios Fountas",
            "Martin A. Benfeghoul",
            "Adnan Oomerjee",
            "Jun Wang",
            "Haitham Bou-Ammar"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-27T12:06:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22367v1"
    },
    {
        "arxiv_id": "2511.22333v1",
        "title": "PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel",
        "abstract": "LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.   This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.",
        "authors": [
            "Jinjun Yi",
            "Zhixin Zhao",
            "Yitao Hu",
            "Ke Yan",
            "Weiwei Sun",
            "Hao Wang",
            "Laiping Zhao",
            "Yuhao Zhang",
            "Wenxin Li",
            "Keqiu Li"
        ],
        "category": "cs.DC",
        "submitted_date": "2025-11-27T11:10:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22333v1"
    },
    {
        "arxiv_id": "2511.22315v1",
        "title": "Named Entity Recognition for the Kurdish Sorani Language: Dataset Creation and Comparative Analysis",
        "abstract": "This work contributes towards balancing the inclusivity and global applicability of natural language processing techniques by proposing the first 'name entity recognition' dataset for Kurdish Sorani, a low-resource and under-represented language, that consists of 64,563 annotated tokens. It also provides a tool for facilitating this task in this and many other languages and performs a thorough comparative analysis, including classic machine learning models and neural systems. The results obtained challenge established assumptions about the advantage of neural approaches within the context of NLP. Conventional methods, in particular CRF, obtain F1-scores of 0.825, outperforming the results of BiLSTM-based models (0.706) significantly. These findings indicate that simpler and more computationally efficient classical frameworks can outperform neural architectures in low-resource settings.",
        "authors": [
            "Bakhtawar Abdalla",
            "Rebwar Mala Nabi",
            "Hassan Eshkiki",
            "Fabio Caraffini"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T10:46:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22315v1"
    },
    {
        "arxiv_id": "2511.22313v1",
        "title": "Sentiment Analysis Of Shopee Product Reviews Using Distilbert",
        "abstract": "The rapid growth of digital commerce has led to the accumulation of a massive number of consumer reviews on online platforms. Shopee, as one of the largest e-commerce platforms in Southeast Asia, receives millions of product reviews every day containing valuable information regarding customer satisfaction and preferences. Manual analysis of these reviews is inefficient, thus requiring a computational approach such as sentiment analysis. This study examines the use of DistilBERT, a lightweight transformer-based deep learning model, for sentiment classification on Shopee product reviews. The dataset used consists of approximately one million English-language reviews that have been preprocessed and trained using the distilbert-base-uncased model. Evaluation was conducted using accuracy, precision, recall, and F1-score metrics, and compared against benchmark models such as BERT and SVM. The results show that DistilBERT achieved an accuracy of 94.8%, slightly below BERT (95.3%) but significantly higher than SVM (90.2%), with computation time reduced by more than 55%. These findings demonstrate that DistilBERT provides an optimal balance between accuracy and efficiency, making it suitable for large scale sentiment analysis on e-commerce platforms. Keywords: Sentiment Analysis, DistilBERT, Shopee Reviews, Natural Language Processing, Deep Learning, Transformer Models.",
        "authors": [
            "Zahri Aksa Dautd",
            "Aviv Yuniar Rahman"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T10:43:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22313v1"
    },
    {
        "arxiv_id": "2511.22312v1",
        "title": "Token-Level Marginalization for Multi-Label LLM Classifiers",
        "abstract": "This paper addresses the critical challenge of deriving interpretable confidence scores from generative language models (LLMs) when applied to multi-label content safety classification. While models like LLaMA Guard are effective for identifying unsafe content and its categories, their generative architecture inherently lacks direct class-level probabilities, which hinders model confidence assessment and performance interpretation. This limitation complicates the setting of dynamic thresholds for content moderation and impedes fine-grained error analysis. This research proposes and evaluates three novel token-level probability estimation approaches to bridge this gap. The aim is to enhance model interpretability and accuracy, and evaluate the generalizability of this framework across different instruction-tuned models. Through extensive experimentation on a synthetically generated, rigorously annotated dataset, it is demonstrated that leveraging token logits significantly improves the interpretability and reliability of generative classifiers, enabling more nuanced content safety moderation.",
        "authors": [
            "Anjaneya Praharaj",
            "Jaykumar Kasundra"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T10:43:26+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22312v1"
    },
    {
        "arxiv_id": "2511.22311v1",
        "title": "Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation",
        "abstract": "Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-art generative methods, such as protein language models (PLMs) and diffusion-based architectures, often require extensive fine-tuning, task-specific data, or model reconfiguration to support objective-directed design, thereby limiting their flexibility and scalability. To overcome these limitations, we present a decentralized, agent-based framework inspired by swarm intelligence for de novo protein design. In this approach, multiple large language model (LLM) agents operate in parallel, each assigned to a specific residue position. These agents iteratively propose context-aware mutations by integrating design objectives, local neighborhood interactions, and memory and feedback from previous iterations. This position-wise, decentralized coordination enables emergent design of diverse, well-defined sequences without reliance on motif scaffolds or multiple sequence alignments, validated with experiments on proteins with alpha helix and coil structures. Through analyses of residue conservation, structure-based metrics, and sequence convergence and embeddings, we demonstrate that the framework exhibits emergent behaviors and effective navigation of the protein fitness landscape. Our method achieves efficient, objective-directed designs within a few GPU-hours and operates entirely without fine-tuning or specialized training, offering a generalizable and adaptable solution for protein design. Beyond proteins, the approach lays the groundwork for collective LLM-driven design across biomolecular systems and other scientific discovery tasks.",
        "authors": [
            "Fiona Y. Wang",
            "Di Sheng Lee",
            "David L. Kaplan",
            "Markus J. Buehler"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-27T10:42:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22311v1"
    },
    {
        "arxiv_id": "2511.22258v1",
        "title": "Beyond Query-Level Comparison: Fine-Grained Reinforcement Learning for Text-to-SQL with Automated Interpretable Critiques",
        "abstract": "Text-to-SQL, a pivotal natural language processing (NLP) task that converts textual queries into executable SQL, has seen substantial progress in recent years. However, existing evaluation and reward mechanisms used to train and assess the text-to-SQL models remain a critical bottleneck. Current approaches heavily rely on manually annotated gold SQL queries, which are costly to produce and impractical for large-scale evaluation. More importantly, most reinforcement learning (RL) methods in text-to-SQL leverage only the final binary execution outcome as the reward signal, a coarse-grained supervision that overlooks detailed structural and semantic errors from the perspective of rubrics. To address these challenges, we propose RuCo-C, a novel generative judge model for fine-grained, query-specific automatic evaluation using interpretable critiques without human intervention. Our framework first automatically generates query-specific evaluation rubrics for human-free annotation, linking them to interpretable critiques. Subsequently, it integrates densified reward feedback through a \"progressive exploration\" strategy during the RL training process, which dynamically adjusts the rewards to enhance the model's performance. Comprehensive experiments demonstrate that RuCo-C outperforms existing methods in text-to-SQL evaluation, yielding significant performance gains.",
        "authors": [
            "Guifeng Wang",
            "Yuanfeng Song",
            "Meng Yang",
            "Tao Zhu",
            "Xiaoming Yin",
            "Xing Chen"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T09:33:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22258v1"
    },
    {
        "arxiv_id": "2511.22232v1",
        "title": "From Compound Figures to Composite Understanding: Developing a Multi-Modal LLM from Biomedical Literature with Medical Multiple-Image Benchmarking and Validation",
        "abstract": "Multi-modal large language models (MLLMs) have shown promise in advancing healthcare. However, most existing models remain confined to single-image understanding, which greatly limits their applicability in clinical workflows. In practice, medical diagnosis and progression often require synthesizing information across multiple images from different modalities or time points. The development of medical MLLMs capable of such multi-image understanding has been hindered by the lack of large-scale, high-quality annotated training data. To address this limitation, we propose a novel framework that leverages license-permissive compound images in biomedical literature, as a rich yet underutilized data source for multi-image analysis. Specifically, we design a five-stage, context-aware instruction generation paradigm underpinned by a divide-and-conquer strategy. By decomposing multi-image analysis into manageable sub-tasks, this paradigm empowers MLLMs to move beyond single-panel analysis and provide a composite understanding by learning the complex spatial, temporal, and cross-modal relationships inherent in these compound figures. By parsing over 237,000 compound figures and their contextual text for instruction generation, we develop M3LLM, a medical multi-image multi-modal large language model. For benchmarking, we construct PMC-MI-Bench for composite understanding, manually validated by medical experts. Extensive experiments show that M3LLM significantly outperforms both general-purpose and specialized medical MLLMs across multi-image, single-image, text-only, and multi-choice scenarios. Notably, M3LLM exhibits strong generalization to longitudinal chest X-ray analysis using the MIMIC dataset. This work establishes a scalable and efficient paradigm for developing medical MLLMs capable of composite reasoning, bridging the gap between biomedical literature and real-world clinical applications.",
        "authors": [
            "Zhen Chen",
            "Yihang Fu",
            "Gabriel Madera",
            "Mauro Giuffre",
            "Serina Applebaum",
            "Hyunjae Kim",
            "Hua Xu",
            "Qingyu Chen"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-27T08:54:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22232v1"
    },
    {
        "arxiv_id": "2511.22176v1",
        "title": "Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information",
        "abstract": "Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.",
        "authors": [
            "Lukas Struppek",
            "Dominik Hintersdorf",
            "Hannah Struppek",
            "Daniel Neider",
            "Kristian Kersting"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T07:31:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22176v1"
    },
    {
        "arxiv_id": "2511.22173v1",
        "title": "RefineBench: Evaluating Refinement Capability of Language Models via Checklists",
        "abstract": "Can language models (LMs) self-refine their own responses? This question is increasingly relevant as a wide range of real-world user interactions involve refinement requests. However, prior studies have largely tested LMs' refinement abilities on verifiable tasks such as competition math or symbolic reasoning with simplified scaffolds, whereas users often pose open-ended queries and provide varying degrees of feedback on what they desire. The recent advent of reasoning models that exhibit self-reflection patterns in their chains-of-thought further motivates this question. To analyze this, we introduce RefineBench, a benchmark of 1,000 challenging problems across 11 domains paired with a checklist-based evaluation framework. We evaluate two refinement modes: (1) guided refinement, where an LM is provided natural language feedback, and (2) self-refinement, where LMs attempt to improve without guidance. In the self-refinement setting, even frontier LMs such as Gemini 2.5 Pro and GPT-5 achieve modest baseline scores of 31.3% and 29.1%, respectively, and most models fail to consistently improve across iterations (e.g., Gemini-2.5-Pro gains only +1.8%, while DeepSeek-R1 declines by -0.1%). By contrast, in guided refinement, both proprietary LMs and large open-weight LMs (>70B) can leverage targeted feedback to refine responses to near-perfect levels within five turns. These findings suggest that frontier LMs require breakthroughs to self-refine their incorrect responses, and that RefineBench provides a valuable testbed for tracking progress.",
        "authors": [
            "Young-Jun Lee",
            "Seungone Kim",
            "Byung-Kwan Lee",
            "Minkyeong Moon",
            "Yechan Hwang",
            "Jong Myoung Kim",
            "Graham Neubig",
            "Sean Welleck",
            "Ho-Jin Choi"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T07:20:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22173v1"
    },
    {
        "arxiv_id": "2511.22155v1",
        "title": "Lips-Jaw and Tongue-Jaw Articulatory Tradeoff in DYNARTmo",
        "abstract": "This paper investigates how the dynamic articulatory model DYNARTmo accounts for articulatory tradeoffs between primary and secondary articulators, with a focus on lips-jaw and tongue-jaw coordination. While DYNARTmo does not implement full task-dynamic second-order biomechanics, it adopts first-order task-space gesture specifications comparable to those used in articulatory phonology and integrates a simplified mechanism for distributing articulatory effort across multiple articulators. We first outline the conceptual relationship between task dynamics and DYNARTmo, emphasizing the distinction between high-level task-space trajectories and their low-level articulatory execution. We then present simulation results for a set of CV syllables that illustrate how jaw displacement varies as a function of both place of articulation (labial, apical, dorsal) and vowel context (/a/, /i/, /u/). The model reproduces empirically attested patterns of articulatory synergy, including jaw-supported apical closures, lower-lip elevation in bilabial stops, tongue-jaw co-movement, and saturation effects in labial constrictions. These results demonstrate that even with computationally simplified assumptions, DYNARTmo can generate realistic spatio-temporal movement patterns that capture key aspects of articulatory tradeoff and synergy across a range of consonant-vowel combinations.",
        "authors": [
            "Bernd J. Kr\u00f6ger"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T06:45:10+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22155v1"
    },
    {
        "arxiv_id": "2511.22153v1",
        "title": "A Theoretically Grounded Hybrid Ensemble for Reliable Detection of LLM-Generated Text",
        "abstract": "The rapid proliferation of Large Language Models (LLMs) has blurred the line between human and machine authorship, creating practical risks for academic integrity and information reliability. Existing text detectors typically rely on a single methodological paradigm and suffer from poor generalization and high false positive rates (FPR), especially on high-stakes academic text. We propose a theoretically grounded hybrid ensemble that systematically fuses three complementary detection paradigms: (i) a RoBERTa-based transformer classifier for deep semantic feature extraction, (ii) a GPT-2-based probabilistic detector using perturbation-induced likelihood curvature, and (iii) a statistical linguistic feature analyzer capturing stylometric patterns. The core novelty lies in an optimized weighted voting framework, where ensemble weights are learned on the probability simplex to maximize F1-score rather than set heuristically. We provide a bias-variance analysis and empirically demonstrate low inter-model correlation (rho ~ 0.35-0.42), a key condition for variance reduction. Evaluated on a large-scale, multigenerator corpus of 30,000 documents, our system achieves 94.2% accuracy and an AUC of 0.978, with a 35% relative reduction in false positives on academic text. This yields a more reliable and ethically responsible detector for real-world deployment in education and other high-stakes domains.",
        "authors": [
            "Sepyan Purnama Kristanto",
            "Lutfi Hakim"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T06:42:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22153v1"
    },
    {
        "arxiv_id": "2511.22150v2",
        "title": "From Topology to Retrieval: Decoding Embedding Spaces with Unified Signatures",
        "abstract": "Studying how embeddings are organized in space not only enhances model interpretability but also uncovers factors that drive downstream task performance. In this paper, we present a comprehensive analysis of topological and geometric measures across a wide set of text embedding models and datasets. We find a high degree of redundancy among these measures and observe that individual metrics often fail to sufficiently differentiate embedding spaces. Building on these insights, we introduce Unified Topological Signatures (UTS), a holistic framework for characterizing embedding spaces. We show that UTS can predict model-specific properties and reveal similarities driven by model architecture. Further, we demonstrate the utility of our method by linking topological structure to ranking effectiveness and accurately predicting document retrievability. We find that a holistic, multi-attribute perspective is essential to understanding and leveraging the geometry of text embeddings.",
        "authors": [
            "Florian Rottach",
            "William Rudman",
            "Bastian Rieck",
            "Harrisen Scells",
            "Carsten Eickhoff"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-27T06:37:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22150v2"
    },
    {
        "arxiv_id": "2511.22146v1",
        "title": "C$^2$DLM: Causal Concept-Guided Diffusion Large Language Models",
        "abstract": "Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose a \\underline{\\textbf{C}}ausal \\underline{\\textbf{C}}oncept-Guided \\underline{\\textbf{D}}iffusion \\underline{\\textbf{L}}anguage \\underline{\\textbf{M}}odel (C$^2$DLM). Starting from DLM's fully connected attention, C$^2$DLM first obtains a concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C$^2$DLM improves 12\\% with about 3.2 times training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31\\% across six downstream reasoning tasks. More details in the repository ~\\href{https://github.com/Kairong-Han/C-2-DLM}{here}.",
        "authors": [
            "Kairong Han",
            "Nuanqiao Shan",
            "Ziyu Zhao",
            "Zijing Hu",
            "Xinpeng Dong",
            "Junjian Ye",
            "Lujia Pan",
            "Fei Wu",
            "Kun Kuang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T06:33:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22146v1"
    },
    {
        "arxiv_id": "2511.22141v1",
        "title": "Bridging the Modality Gap by Similarity Standardization with Pseudo-Positive Samples",
        "abstract": "Advances in vision-language models (VLMs) have enabled effective cross-modality retrieval. However, when both text and images exist in the database, similarity scores would differ in scale by modality. This phenomenon, known as the modality gap, hinders accurate retrieval. Most existing studies address this issue with manually labeled data, e.g., by fine-tuning VLMs on them. In this work, we propose a similarity standardization approach with pseudo data construction. We first compute the mean and variance of the similarity scores between each query and its paired data in text or image modality. Using these modality-specific statistics, we standardize all similarity scores to compare on a common scale across modalities. These statistics are calculated from pseudo pairs, which are constructed by retrieving the text and image candidates with the highest cosine similarity to each query. We evaluate our method across seven VLMs using two multi-modal QA benchmarks (MMQA and WebQA), where each question requires retrieving either text or image data. Our experimental results show that our method significantly improves retrieval performance, achieving average Recall@20 gains of 64% on MMQA and 28% on WebQA when the query and the target data belong to different modalities. Compared to E5-V, which addresses the modality gap through image captioning, we confirm that our method more effectively bridges the modality gap.",
        "authors": [
            "Shuhei Yamashita",
            "Daiki Shirafuji",
            "Tatsuhiko Saito"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T06:17:07+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22141v1"
    },
    {
        "arxiv_id": "2511.22109v1",
        "title": "A Hybrid Theory and Data-driven Approach to Persuasion Detection with Large Language Models",
        "abstract": "Traditional psychological models of belief revision focus on face-to-face interactions, but with the rise of social media, more effective models are needed to capture belief revision at scale, in this rich text-based online discourse. Here, we use a hybrid approach, utilizing large language models (LLMs) to develop a model that predicts successful persuasion using features derived from psychological experiments.   Our approach leverages LLM generated ratings of features previously examined in the literature to build a random forest classification model that predicts whether a message will result in belief change. Of the eight features tested, \\textit{epistemic emotion} and \\textit{willingness to share} were the top-ranking predictors of belief change in the model. Our findings provide insights into the characteristics of persuasive messages and demonstrate how LLMs can enhance models of successful persuasion based on psychological theory. Given these insights, this work has broader applications in fields such as online influence detection and misinformation mitigation, as well as measuring the effectiveness of online narratives.",
        "authors": [
            "Gia Bao Hoang",
            "Keith J Ransom",
            "Rachel Stephens",
            "Carolyn Semmler",
            "Nicolas Fay",
            "Lewis Mitchell"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T04:59:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22109v1"
    },
    {
        "arxiv_id": "2511.22038v1",
        "title": "Early Risk Prediction with Temporally and Contextually Grounded Clinical Language Processing",
        "abstract": "Clinical notes in Electronic Health Records (EHRs) capture rich temporal information on events, clinician reasoning, and lifestyle factors often missing from structured data. Leveraging them for predictive modeling can be impactful for timely identification of chronic diseases. However, they present core natural language processing (NLP) challenges: long text, irregular event distribution, complex temporal dependencies, privacy constraints, and resource limitations. We present two complementary methods for temporally and contextually grounded risk prediction from longitudinal notes. First, we introduce HiTGNN, a hierarchical temporal graph neural network that integrates intra-note temporal event structures, inter-visit dynamics, and medical knowledge to model patient trajectories with fine-grained temporal granularity. Second, we propose ReVeAL, a lightweight, test-time framework that distills the reasoning of large language models into smaller verifier models. Applied to opportunistic screening for Type 2 Diabetes (T2D) using temporally realistic cohorts curated from private and public hospital corpora, HiTGNN achieves the highest predictive accuracy, especially for near-term risk, while preserving privacy and limiting reliance on large proprietary models. ReVeAL enhances sensitivity to true T2D cases and retains explanatory reasoning. Our ablations confirm the value of temporal structure and knowledge augmentation, and fairness analysis shows HiTGNN performs more equitably across subgroups.",
        "authors": [
            "Rochana Chaturvedi",
            "Yue Zhou",
            "Andrew Boyd",
            "Brian T. Layden",
            "Mudassir Rashid",
            "Lu Cheng",
            "Ali Cinar",
            "Barbara Di Eugenio"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T02:47:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22038v1"
    },
    {
        "arxiv_id": "2511.22036v1",
        "title": "ResearchArcade: Graph Interface for Academic Tasks",
        "abstract": "Academic research generates diverse data sources, and as researchers increasingly use machine learning to assist research tasks, a crucial question arises: Can we build a unified data interface to support the development of machine learning models for various academic tasks? Models trained on such a unified interface can better support human researchers throughout the research process, eventually accelerating knowledge discovery. In this work, we introduce ResearchArcade, a graph-based interface that connects multiple academic data sources, unifies task definitions, and supports a wide range of base models to address key academic challenges. ResearchArcade utilizes a coherent multi-table format with graph structures to organize data from different sources, including academic corpora from ArXiv and peer reviews from OpenReview, while capturing information with multiple modalities, such as text, figures, and tables. ResearchArcade also preserves temporal evolution at both the manuscript and community levels, supporting the study of paper revisions as well as broader research trends over time. Additionally, ResearchArcade unifies diverse academic task definitions and supports various models with distinct input requirements. Our experiments across six academic tasks demonstrate that combining cross-source and multi-modal information enables a broader range of tasks, while incorporating graph structures consistently improves performance over baseline methods. This highlights the effectiveness of ResearchArcade and its potential to advance research progress.",
        "authors": [
            "Jingjun Xu",
            "Chongshan Lin",
            "Haofei Yu",
            "Tao Feng",
            "Jiaxuan You"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T02:42:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22036v1"
    },
    {
        "arxiv_id": "2511.22016v1",
        "title": "AfriStereo: A Culturally Grounded Dataset for Evaluating Stereotypical Bias in Large Language Models",
        "abstract": "Existing AI bias evaluation benchmarks largely reflect Western perspectives, leaving African contexts underrepresented and enabling harmful stereotypes in applications across various domains. To address this gap, we introduce AfriStereo, the first open-source African stereotype dataset and evaluation framework grounded in local socio-cultural contexts. Through community engaged efforts across Senegal, Kenya, and Nigeria, we collected 1,163 stereotypes spanning gender, ethnicity, religion, age, and profession. Using few-shot prompting with human-in-the-loop validation, we augmented the dataset to over 5,000 stereotype-antistereotype pairs. Entries were validated through semantic clustering and manual annotation by culturally informed reviewers. Preliminary evaluation of language models reveals that nine of eleven models exhibit statistically significant bias, with Bias Preference Ratios (BPR) ranging from 0.63 to 0.78 (p <= 0.05), indicating systematic preferences for stereotypes over antistereotypes, particularly across age, profession, and gender dimensions. Domain-specific models appeared to show weaker bias in our setup, suggesting task-specific training may mitigate some associations. Looking ahead, AfriStereo opens pathways for future research on culturally grounded bias evaluation and mitigation, offering key methodologies for the AI community on building more equitable, context-aware, and globally inclusive NLP technologies.",
        "authors": [
            "Yann Le Beux",
            "Oluchi Audu",
            "Oche D. Ankeli",
            "Dhananjay Balakrishnan",
            "Melissah Weya",
            "Marie D. Ralaiarinosy",
            "Ignatius Ezeani"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-27T01:37:23+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.22016v1"
    },
    {
        "arxiv_id": "2511.21974v1",
        "title": "Start Making Sense(s): A Developmental Probe of Attention Specialization Using Lexical Ambiguity",
        "abstract": "Despite an in-principle understanding of self-attention matrix operations in Transformer language models (LMs), it remains unclear precisely how these operations map onto interpretable computations or functions--and how or when individual attention heads develop specialized attention patterns. Here, we present a pipeline to systematically probe attention mechanisms, and we illustrate its value by leveraging lexical ambiguity--where a single word has multiple meanings--to isolate attention mechanisms that contribute to word sense disambiguation. We take a \"developmental\" approach: first, using publicly available Pythia LM checkpoints, we identify inflection points in disambiguation performance for each LM in the suite; in 14M and 410M, we identify heads whose attention to disambiguating words covaries with overall disambiguation performance across development. We then stress-test the robustness of these heads to stimulus perturbations: in 14M, we find limited robustness, but in 410M, we identify multiple heads with surprisingly generalizable behavior. Then, in a causal analysis, we find that ablating the target heads demonstrably impairs disambiguation performance, particularly in 14M. We additionally reproduce developmental analyses of 14M across all of its random seeds. Together, these results suggest: that disambiguation benefits from a constellation of mechanisms, some of which (especially in 14M) are highly sensitive to the position and part-of-speech of the disambiguating cue; and that larger models (410M) may contain heads with more robust disambiguation behavior. They also join a growing body of work that highlights the value of adopting a developmental perspective when probing LM mechanisms.",
        "authors": [
            "Pamela D. Rivi\u00e8re",
            "Sean Trott"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T23:16:12+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21974v1"
    },
    {
        "arxiv_id": "2511.21930v1",
        "title": "A Comparative Study of LLM Prompting and Fine-Tuning for Cross-genre Authorship Attribution on Chinese Lyrics",
        "abstract": "We propose a novel study on authorship attribution for Chinese lyrics, a domain where clean, public datasets are sorely lacking. Our contributions are twofold: (1) we create a new, balanced dataset of Chinese lyrics spanning multiple genres, and (2) we develop and fine-tune a domain-specific model, comparing its performance against zero-shot inference using the DeepSeek LLM.   We test two central hypotheses. First, we hypothesize that a fine-tuned model will outperform a zero-shot LLM baseline. Second, we hypothesize that performance is genre-dependent. Our experiments strongly confirm Hypothesis 2: structured genres (e.g. Folklore & Tradition) yield significantly higher attribution accuracy than more abstract genres (e.g. Love & Romance). Hypothesis 1 receives only partial support: fine-tuning improves robustness and generalization in Test1 (real-world data and difficult genres), but offers limited or ambiguous gains in Test2, a smaller, synthetically-augmented set. We show that the design limitations of Test2 (e.g., label imbalance, shallow lexical differences, and narrow genre sampling) can obscure the true effectiveness of fine-tuning.   Our work establishes the first benchmark for cross-genre Chinese lyric attribution, highlights the importance of genre-sensitive evaluation, and provides a public dataset and analytical framework for future research. We conclude with recommendations: enlarge and diversify test sets, reduce reliance on token-level data augmentation, balance author representation across genres, and investigate domain-adaptive pretraining as a pathway for improved attribution performance.",
        "authors": [
            "Yuxin Li",
            "Lorraine Xu",
            "Meng Fan Wang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T21:44:40+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21930v1"
    },
    {
        "arxiv_id": "2511.21912v1",
        "title": "Tracing How Annotators Think: Augmenting Preference Judgments with Reading Processes",
        "abstract": "We propose an annotation approach that captures not only labels but also the reading process underlying annotators' decisions, e.g., what parts of the text they focus on, re-read or skim. Using this framework, we conduct a case study on the preference annotation task, creating a dataset PreferRead that contains fine-grained annotator reading behaviors obtained from mouse tracking. PreferRead enables detailed analysis of how annotators navigate between a prompt and two candidate responses before selecting their preference. We find that annotators re-read a response in roughly half of all trials, most often revisiting the option they ultimately choose, and rarely revisit the prompt. Reading behaviors are also significantly related to annotation outcomes: re-reading is associated with higher inter-annotator agreement, whereas long reading paths and times are associated with lower agreement. These results demonstrate that reading processes provide a complementary cognitive dimension for understanding annotator reliability, decision-making and disagreement in complex, subjective NLP tasks. Our code and data are publicly available.",
        "authors": [
            "Karin de Langis",
            "William Walker",
            "Khanh Chi Le",
            "Dongyeop Kang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T21:07:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21912v1"
    },
    {
        "arxiv_id": "2511.21909v1",
        "title": "A Customer Journey in the Land of Oz: Leveraging the Wizard of Oz Technique to Model Emotions in Customer Service Interactions",
        "abstract": "Emotion-aware customer service needs in-domain conversational data, rich annotations, and predictive capabilities, but existing resources for emotion recognition are often out-of-domain, narrowly labeled, and focused on post-hoc detection. To address this, we conducted a controlled Wizard of Oz (WOZ) experiment to elicit interactions with targeted affective trajectories. The resulting corpus, EmoWOZ-CS, contains 2,148 bilingual (Dutch-English) written dialogues from 179 participants across commercial aviation, e-commerce, online travel agencies, and telecommunication scenarios. Our contributions are threefold: (1) Evaluate WOZ-based operator-steered valence trajectories as a design for emotion research; (2) Quantify human annotation performance and variation, including divergences between self-reports and third-party judgments; (3) Benchmark detection and forward-looking emotion inference in real-time support. Findings show neutral dominates participant messages; desire and gratitude are the most frequent non-neutral emotions. Agreement is moderate for multilabel emotions and valence, lower for arousal and dominance; self-reports diverge notably from third-party labels, aligning most for neutral, gratitude, and anger. Objective strategies often elicit neutrality or gratitude, while suboptimal strategies increase anger, annoyance, disappointment, desire, and confusion. Some affective strategies (cheerfulness, gratitude) foster positive reciprocity, whereas others (apology, empathy) can also leave desire, anger, or annoyance. Temporal analysis confirms successful conversation-level steering toward prescribed trajectories, most distinctly for negative targets; positive and neutral targets yield similar final valence distributions. Benchmarks highlight the difficulty of forward-looking emotion inference from prior turns, underscoring the complexity of proactive emotion-aware support.",
        "authors": [
            "Sofie Labat",
            "Thomas Demeester",
            "V\u00e9ronique Hoste"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T20:52:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21909v1"
    },
    {
        "arxiv_id": "2511.21860v1",
        "title": "Improving Score Reliability of Multiple Choice Benchmarks with Consistency Evaluation and Altered Answer Choices",
        "abstract": "In this work we present the Consistency-Rebalanced Accuracy (CoRA) metric, improving the reliability of Large Language Model (LLM) scores computed on multiple choice (MC) benchmarks. Our metric explores the response consistency of the LLMs, taking advantage of synthetically-generated questions with altered answer choices. With two intermediate scores, i.e. Bare-Minimum-Consistency Accuracy (BMCA) and Consistency Index (CI), CoRA is computed by adjusting the multiple-choice question answering (MCQA) scores to better reflect the level of consistency of the LLM. We present evaluations in different benchmarks using diverse LLMs, and not only demonstrate that LLMs can present low response consistency even when they present high MCQA scores, but also that CoRA can successfully scale down the scores of inconsistent models.",
        "authors": [
            "Paulo Cavalin",
            "Cassia Sanctos",
            "Marcelo Grave",
            "Claudio Pinhanez",
            "Yago Primerano"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T19:35:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21860v1"
    },
    {
        "arxiv_id": "2511.21843v1",
        "title": "FLAWS: A Benchmark for Error Identification and Localization in Scientific Papers",
        "abstract": "The identification and localization of errors is a core task in peer review, yet the exponential growth of scientific output has made it increasingly difficult for human reviewers to reliably detect errors given the limited pool of experts. Recent advances in Large Language Models (LLMs) have sparked interest in their potential to support such evaluation tasks, from academic peer review to automated scientific assessment. However, despite the growing use of LLMs in review systems, their capabilities to pinpoint errors remain underexplored. In this work, we introduce Fault Localization Across Writing in Science (FLAWS), an automated benchmark consisting of 713 paper-error pairs designed to evaluate how effectively LLMs detect errors that undermine key claims in research papers. We construct the benchmark by systematically inserting claim-invalidating errors into peer-reviewed papers using LLMs, paired with an automated evaluation metric that measures whether models can identify and localize these errors. Developing such a benchmark presents unique challenges that we overcome: ensuring that the inserted errors are well-defined, challenging, and relevant to the content of the paper, avoiding artifacts that would make identification trivial, and designing a scalable, automated evaluation metric. On the resulting benchmark, we evaluate five frontier LLMs: Claude Sonnet 4.5, DeepSeek Reasoner v3.1, Gemini 2.5 Pro, GPT 5, and Grok 4. Among these, GPT 5 is the top-performing model, achieving 39.1% identification accuracy when k=10, where k is the number of top-ranked error text candidates generated by the LLM.",
        "authors": [
            "Sarina Xi",
            "Vishisht Rao",
            "Justin Payan",
            "Nihar B. Shah"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T19:19:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21843v1"
    },
    {
        "arxiv_id": "2511.21692v1",
        "title": "Revisiting Generalization Across Difficulty Levels: It's Not So Easy",
        "abstract": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.",
        "authors": [
            "Yeganeh Kordi",
            "Nihal V. Nayak",
            "Max Zuo",
            "Ilana Nguyen",
            "Stephen H. Bach"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T18:59:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21692v1"
    },
    {
        "arxiv_id": "2511.21689v1",
        "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
        "abstract": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
        "authors": [
            "Hongjin Su",
            "Shizhe Diao",
            "Ximing Lu",
            "Mingjie Liu",
            "Jiacheng Xu",
            "Xin Dong",
            "Yonggan Fu",
            "Peter Belcak",
            "Hanrong Ye",
            "Hongxu Yin",
            "Yi Dong",
            "Evelina Bakhturina",
            "Tao Yu",
            "Yejin Choi",
            "Jan Kautz",
            "Pavlo Molchanov"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T18:59:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21689v1"
    },
    {
        "arxiv_id": "2511.21688v2",
        "title": "G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning",
        "abstract": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.",
        "authors": [
            "Wenbo Hu",
            "Jingli Lin",
            "Yilin Long",
            "Yunlong Ran",
            "Lihan Jiang",
            "Yifan Wang",
            "Chenming Zhu",
            "Runsen Xu",
            "Tai Wang",
            "Jiangmiao Pang"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-26T18:59:39+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21688v2"
    },
    {
        "arxiv_id": "2511.21686v1",
        "title": "Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework",
        "abstract": "Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \\textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\\times$ higher data generation throughput under identical hardware resources, without compromising output quality.",
        "authors": [
            "Dong Wang",
            "Yang Li",
            "Ansong Ni",
            "Ching-Feng Yeh",
            "Youssef Emad",
            "Xinjie Lei",
            "Liam Robbins",
            "Karthik Padthe",
            "Hu Xu",
            "Xian Li",
            "Asli Celikyilmaz",
            "Ramya Raghavendra",
            "Lifei Huang",
            "Carole-Jean Wu",
            "Shang-Wen Li"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T18:59:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21686v1"
    },
    {
        "arxiv_id": "2511.21629v1",
        "title": "The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry",
        "abstract": "Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.",
        "authors": [
            "Anna Marklov\u00e1",
            "Ond\u0159ej Vin\u0161",
            "Martina Vok\u00e1\u010dov\u00e1",
            "Ji\u0159\u00ed Mili\u010dka"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T17:53:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21629v1"
    },
    {
        "arxiv_id": "2511.21624v1",
        "title": "TAGFN: A Text-Attributed Graph Dataset for Fake News Detection in the Age of LLMs",
        "abstract": "Large Language Models (LLMs) have recently revolutionized machine learning on text-attributed graphs, but the application of LLMs to graph outlier detection, particularly in the context of fake news detection, remains significantly underexplored. One of the key challenges is the scarcity of large-scale, realistic, and well-annotated datasets that can serve as reliable benchmarks for outlier detection. To bridge this gap, we introduce TAGFN, a large-scale, real-world text-attributed graph dataset for outlier detection, specifically fake news detection. TAGFN enables rigorous evaluation of both traditional and LLM-based graph outlier detection methods. Furthermore, it facilitates the development of misinformation detection capabilities in LLMs through fine-tuning. We anticipate that TAGFN will be a valuable resource for the community, fostering progress in robust graph-based outlier detection and trustworthy AI. The dataset is publicly available at https://huggingface.co/datasets/kayzliu/TAGFN and our code is available at https://github.com/kayzliu/tagfn.",
        "authors": [
            "Kay Liu",
            "Yuwei Han",
            "Haoyan Xu",
            "Henry Peng Zou",
            "Yue Zhao",
            "Philip S. Yu"
        ],
        "category": "cs.SI",
        "submitted_date": "2025-11-26T17:49:40+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21624v1"
    },
    {
        "arxiv_id": "2511.21613v1",
        "title": "Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining",
        "abstract": "Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.",
        "authors": [
            "Dongyang Fan",
            "Diba Hashemi",
            "Sai Praneeth Karimireddy",
            "Martin Jaggi"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T17:36:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21613v1"
    },
    {
        "arxiv_id": "2511.21610v1",
        "title": "Auxiliary Metrics Help Decoding Skill Neurons in the Wild",
        "abstract": "Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified \"skill neurons\" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.",
        "authors": [
            "Yixiu Zhao",
            "Xiaozhi Wang",
            "Zijun Yao",
            "Lei Hou",
            "Juanzi Li"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T17:31:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21610v1"
    },
    {
        "arxiv_id": "2511.21568v1",
        "title": "RoParQ: Paraphrase-Aware Alignment of Large Language Models Towards Robustness to Paraphrased Questions",
        "abstract": "Large Language Models (LLMs) often exhibit inconsistent behavior when answering paraphrased questions, suggesting a reliance on surface-level patterns rather than true semantic understanding. To address this limitation, we introduce RoParQ, a benchmark specifically constructed to evaluate cross-paraphrase consistency in closed-book multiple-choice QA. This benchmark is derived from standard datasets by generating paraphrases via proprietary models and selectively retaining examples that elicit inconsistent confidence from a judge model. We further propose XParaCon, a novel evaluation metric that quantifies a model's robustness by measuring the standard deviation of accuracies across question variants. Additionally, we implement a reasoning-based, paraphrase-aware Supervised Fine-Tuning (SFT) strategy designed to align models toward semantic invariance. Our experiments demonstrate that this targeted alignment significantly enhances robustness. Notably, fine-tuned lightweight models achieved consistency levels comparable to much larger pre-trained models. These results highlight the efficacy of our approach in mitigating superficial memorization and fostering more robust, reliable LLMs.",
        "authors": [
            "Minjoon Choi"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T16:40:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21568v1"
    },
    {
        "arxiv_id": "2511.21533v1",
        "title": "Bangla Sign Language Translation: Dataset Creation Challenges, Benchmarking and Prospects",
        "abstract": "Bangla Sign Language Translation (BdSLT) has been severely constrained so far as the language itself is very low resource. Standard sentence level dataset creation for BdSLT is of immense importance for developing AI based assistive tools for deaf and hard of hearing people of Bangla speaking community. In this paper, we present a dataset, IsharaKhobor , and two subset of it for enabling research. We also present the challenges towards developing the dataset and present some way forward by benchmarking with landmark based raw and RQE embedding. We do some ablation on vocabulary restriction and canonicalization of the same within the dataset, which resulted in two more datasets, IsharaKhobor_small and IsharaKhobor_canonical_small. The dataset is publicly available at: www.kaggle.com/datasets/hasanssl/isharakhobor [1].",
        "authors": [
            "Husne Ara Rubaiyeat",
            "Hasan Mahmud",
            "Md Kamrul Hasan"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T16:00:38+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21533v1"
    },
    {
        "arxiv_id": "2511.21517v1",
        "title": "Voice, Bias, and Coreference: An Interpretability Study of Gender in Speech Translation",
        "abstract": "Unlike text, speech conveys information about the speaker, such as gender, through acoustic cues like pitch. This gives rise to modality-specific bias concerns. For example, in speech translation (ST), when translating from languages with notional gender, such as English, into languages where gender-ambiguous terms referring to the speaker are assigned grammatical gender, the speaker's vocal characteristics may play a role in gender assignment. This risks misgendering speakers, whether through masculine defaults or vocal-based assumptions. Yet, how ST models make these decisions remains poorly understood. We investigate the mechanisms ST models use to assign gender to speaker-referring terms across three language pairs (en-es/fr/it), examining how training data patterns, internal language model (ILM) biases, and acoustic information interact. We find that models do not simply replicate term-specific gender associations from training data, but learn broader patterns of masculine prevalence. While the ILM exhibits strong masculine bias, models can override these preferences based on acoustic input. Using contrastive feature attribution on spectrograms, we reveal that the model with higher gender accuracy relies on a previously unknown mechanism: using first-person pronouns to link gendered terms back to the speaker, accessing gender information distributed across the frequency spectrum rather than concentrated in pitch.",
        "authors": [
            "Lina Conti",
            "Dennis Fucci",
            "Marco Gaido",
            "Matteo Negri",
            "Guillaume Wisniewski",
            "Luisa Bentivogli"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T15:48:04+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21517v1"
    },
    {
        "arxiv_id": "2511.21473v1",
        "title": "Hierarchical Ranking Neural Network for Long Document Readability Assessment",
        "abstract": "Readability assessment aims to evaluate the reading difficulty of a text. In recent years, while deep learning technology has been gradually applied to readability assessment, most approaches fail to consider either the length of the text or the ordinal relationship of readability labels. This paper proposes a bidirectional readability assessment mechanism that captures contextual information to identify regions with rich semantic information in the text, thereby predicting the readability level of individual sentences. These sentence-level labels are then used to assist in predicting the overall readability level of the document. Additionally, a pairwise sorting algorithm is introduced to model the ordinal relationship between readability levels through label subtraction. Experimental results on Chinese and English datasets demonstrate that the proposed model achieves competitive performance and outperforms other baseline models.",
        "authors": [
            "Yurui Zheng",
            "Yijun Chen",
            "Shaohong Zhang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T15:05:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21473v1"
    },
    {
        "arxiv_id": "2511.21437v1",
        "title": "A Systematic Study of Model Merging Techniques in Large Language Models",
        "abstract": "Model merging combines multiple fine-tuned checkpoints into a single model without additional training, offering an attractive approach to reusing models and efficiently improving performance. However, it remains unclear whether the advantages reported for smaller models and classifiers generalize to LLMs. We present a large-scale, systematic evaluation of six state-of-the-art merging methods, including recent subspace methods, across four open-weight LLMs, twelve fine-tuned checkpoints per base model, and sixteen standard LLM benchmarks. Evaluating through standardized benchmarks, we measure both the probability that a merged model outperforms the base model and relative gains over the best individual checkpoint. Our results show that the oldest and simplest method, Task Arithmetic, is the only approach that reliably yields performance gains on LLMs. Other interference-aware and subspace merging methods typically result in significant performance drops. Our findings indicate that current merging techniques do not directly transfer to modern LLMs. This motivates the design of LLM-specific merging algorithms and merging-aware fine-tuning methods. Code will be released upon acceptance of this paper.",
        "authors": [
            "O\u011fuz Ka\u011fan Hitit",
            "Leander Girrbach",
            "Zeynep Akata"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T14:28:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21437v1"
    },
    {
        "arxiv_id": "2511.21416v2",
        "title": "Odin: Oriented Dual-module Integration for Text-rich Network Representation Learning",
        "abstract": "Text-attributed graphs require models to effectively combine strong textual understanding with structurally informed reasoning. Existing approaches either rely on GNNs--limited by over-smoothing and hop-dependent diffusion--or employ Transformers that overlook graph topology and treat nodes as isolated sequences. We propose Odin (Oriented Dual-module INtegration), a new architecture that injects graph structure into Transformers at selected depths through an oriented dual-module mechanism. Unlike message-passing GNNs, Odin does not rely on multi-hop diffusion; instead, multi-hop structures are integrated at specific Transformer layers, yielding low-, mid-, and high-level structural abstraction aligned with the model's semantic hierarchy. Because aggregation operates on the global [CLS] representation, Odin fundamentally avoids over-smoothing and decouples structural abstraction from neighborhood size or graph topology. We further establish that Odin's expressive power strictly contains that of both pure Transformers and GNNs. To make the design efficient in large-scale or low-resource settings, we introduce Light Odin, a lightweight variant that preserves the same layer-aligned structural abstraction for faster training and inference. Experiments on multiple text-rich graph benchmarks show that Odin achieves state-of-the-art accuracy, while Light Odin delivers competitive performance with significantly reduced computational cost. Together, Odin and Light Odin form a unified, hop-free framework for principled structure-text integration. The source code of this model has been released at https://github.com/hongkaifeng/Odin.",
        "authors": [
            "Kaifeng Hong",
            "Yinglong Zhang",
            "Xiaoying Hong",
            "Xuewen Xia",
            "Xing Xu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T14:07:07+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21416v2"
    },
    {
        "arxiv_id": "2511.21408v1",
        "title": "Subjective Depth and Timescale Transformers: Learning Where and When to Compute",
        "abstract": "The rigid, uniform allocation of computation in standard Transformer (TF) architectures can limit their efficiency and scalability, particularly for large-scale models and long sequences. Addressing this, we introduce Subjective Depth Transformers (SDT) and Subjective Timescale Transformers (STT), two distinct architectures that leverage Bayesian surprise signals to dynamically route computation, learning where and when to compute within decoder-only TFs. SDT augments a decoder-only stack with alternating Decision and Dynamic layers: a Decision layer computes a full block 'posterior' and a lightweight 'prior,' while a Dynamic layer employs fixed-capacity Top-K routing based on Bayesian surprise (Expected and Unexpected Change), maintaining a static compute graph. STT extends this conditional computation to the temporal domain: a transition network predicts residual updates, forming a temporal 'change hypothesis' that informs a router to dynamically execute or bypass TF blocks for each token, managing KV-cache contributions. Both architectures exhibit the predicted shift from novelty to prediction driven gating over training, suggesting alignment with surprise based principles. While operating at reduced capacity, they offer preliminary insights into the compute-accuracy trade-offs of conditional computation. The proposed architectures establish a flexible framework for efficiency, reducing self-attention computation by 75% and KV-cache requirements by 50% within each compute skipping layer, setting a pathway for more efficient models.",
        "authors": [
            "Frederico Wieser",
            "Martin Benfeghoul",
            "Haitham Bou Ammar",
            "Jun Wang",
            "Zafeirios Fountas"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-26T14:00:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21408v1"
    },
    {
        "arxiv_id": "2511.21402v1",
        "title": "Text-to-SQL as Dual-State Reasoning: Integrating Adaptive Context and Progressive Generation",
        "abstract": "Recent divide-and-conquer reasoning approaches, particularly those based on Chain-of-Thought (CoT), have substantially improved the Text-to-SQL capabilities of Large Language Models (LLMs). However, when applied to complex enterprise databases, such methods struggle to maintain coherent reasoning due to limited context capacity, unreliable schema linking, and weak grounding in database semantics. To overcome these issues, we introduce DSR-SQL, a \\textbf{D}ual-\\textbf{S}tate \\textbf{R}easoning framework that models Text-to-SQL as an interaction between an adaptive context state and a progressive generation state. The first constructs a compact, semantically faithful environment by refining large schemas and selecting relevant structures, while the second formalizes SQL synthesis as feedback-guided state transitions, enabling the model to self-correct and align with user intent. Without any post-training or in-context examples, DSR-SQL achieves competitive performance, reaching 35.28\\% execution accuracy on Spider 2.0-Snow and 68.32\\% on BIRD development set. Our implementation will be open-sourced at: https://github.com/DMIRLAB-Group/DSR-SQL.",
        "authors": [
            "Zhifeng Hao",
            "Qibin Song",
            "Ruichu Cai",
            "Boyan Xu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T13:52:50+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21402v1"
    },
    {
        "arxiv_id": "2511.21401v1",
        "title": "Can LLMs extract human-like fine-grained evidence for evidence-based fact-checking?",
        "abstract": "Misinformation frequently spreads in user comments under online news articles, highlighting the need for effective methods to detect factually incorrect information. To strongly support or refute claims extracted from such comments, it is necessary to identify relevant documents and pinpoint the exact text spans that justify or contradict each claim. This paper focuses on the latter task -- fine-grained evidence extraction for Czech and Slovak claims. We create new dataset, containing two-way annotated fine-grained evidence created by paid annotators. We evaluate large language models (LLMs) on this dataset to assess their alignment with human annotations. The results reveal that LLMs often fail to copy evidence verbatim from the source text, leading to invalid outputs. Error-rate analysis shows that the {llama3.1:8b model achieves a high proportion of correct outputs despite its relatively small size, while the gpt-oss-120b model underperforms despite having many more parameters. Furthermore, the models qwen3:14b, deepseek-r1:32b, and gpt-oss:20b demonstrate an effective balance between model size and alignment with human annotations.",
        "authors": [
            "Anton\u00edn Jarol\u00edm",
            "Martin Faj\u010d\u00edk",
            "Lucia Makaiov\u00e1"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T13:51:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21401v1"
    },
    {
        "arxiv_id": "2511.21399v1",
        "title": "Training Introspective Behavior: Fine-Tuning Induces Reliable Internal State Detection in a 7B Model",
        "abstract": "Lindsey (2025) investigates introspective awareness in language models through four experiments, finding that models can sometimes detect and identify injected activation patterns -- but unreliably (~20% success in the best model). We focus on the first of these experiments -- self-report of injected \"thoughts\" -- and ask whether this capability can be directly trained rather than waiting for emergence. Through fine-tuning on transient single-token injections, we transform a 7B parameter model from near-complete failure (0.4% accuracy, 6.7% false positive rate) to reliable detection (85% accuracy on held-out concepts at \u03b1=40, 0% false positives). Our model detects fleeting \"thoughts\" injected at a single token position, retains that information, and reports the semantic content across subsequent generation steps. On this task, our trained model satisfies three of Lindsey's criteria: accuracy (correct identification), grounding (0/60 false positives), and internality (detection precedes verbalization). Generalization to unseen concept vectors (7.5pp gap) demonstrates the model learns a transferable skill rather than memorizing specific vectors, though this does not establish metacognitive representation in Lindsey's sense. These results address an open question raised by Lindsey: whether \"training for introspection would help eliminate cross-model differences.\" We show that at least one component of introspective behavior can be directly induced, offering a pathway to built-in AI transparency.",
        "authors": [
            "Joshua Fonseca Rivera"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T13:49:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21399v1"
    },
    {
        "arxiv_id": "2511.21398v1",
        "title": "Prune4Web: DOM Tree Pruning Programming for Web Agent",
        "abstract": "Web automation employs intelligent agents to execute high-level tasks by mimicking human interactions with web interfaces. Despite the capabilities of recent Large Language Model (LLM)-based web agents, navigating complex, real-world webpages efficiently remains a significant hurdle due to the prohibitively large size of Document Object Model (DOM) structures, often ranging from 10,000 to 100,000 tokens. Existing strategies typically rely on crude DOM truncation -- risking the loss of critical information -- or employ inefficient heuristics and separate ranking models, failing to achieve an optimal balance between precision and scalability. To address these challenges, we introduce Prune4Web, a novel paradigm that shifts DOM processing from resource-intensive LLM reading to efficient programmatic pruning. Central to our approach is DOM Tree Pruning Programming, where an LLM generates executable Python scoring scripts to dynamically filter DOM elements based on semantic cues from decomposed sub-tasks. This mechanism eliminates the need for LLMs to ingest raw, massive DOMs, instead delegating traversal and scoring to lightweight, interpretable programs. This methodology achieves a 25x to 50x reduction in candidate elements for grounding, thereby facilitating precise action localization while mitigating attention dilution. Furthermore, we propose a specialized data annotation pipeline and a two-turn dialogue training strategy that jointly optimizes the Planner, Programmatic Filter, and Grounder within a unified framework. Extensive experiments demonstrate state-of-the-art performance. Notably, on our low-level grounding task, Prune4Web dramatically improves accuracy from 46.8% to 88.28%, underscoring its efficacy in real-world web automation.",
        "authors": [
            "Jiayuan Zhang",
            "Kaiquan Chen",
            "Zhihao Lu",
            "Enshen Zhou",
            "Qian Yu",
            "Jing Zhang"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-26T13:49:39+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21398v1"
    },
    {
        "arxiv_id": "2511.21397v1",
        "title": "Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis",
        "abstract": "How does irrelevant information (i.e., distractors) affect test-time scaling in vision-language models (VLMs)? Prior studies on language models have reported an inverse scaling effect, where textual distractors lead to longer but less effective reasoning. To investigate whether similar phenomena occur in multimodal settings, we introduce Idis (Images with distractors), a visual question-answering dataset that systematically varies distractors along semantic, numerical, and spatial dimensions. Our analyses reveal that visual distractors differ fundamentally from textual ones: although inverse scaling persists, adding visual distractors reduces accuracy without increasing reasoning length. We further show that tracking attribute counts within reasoning traces provides key insights into how distractors, reasoning length, and accuracy interact. Finally, we demonstrate that these trends extend to established visual bias benchmarks such as Waterbirds, and we propose a simple prompting strategy to mitigate bias-driven predictions in reasoning models.",
        "authors": [
            "Jiyun Bae",
            "Hyunjong Ok",
            "Sangwoo Mo",
            "Jaeho Lee"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-26T13:49:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21397v1"
    },
    {
        "arxiv_id": "2511.21381v1",
        "title": "BanglaASTE: A Novel Framework for Aspect-Sentiment-Opinion Extraction in Bangla E-commerce Reviews Using Ensemble Deep Learning",
        "abstract": "Aspect-Based Sentiment Analysis (ABSA) has emerged as a critical tool for extracting fine-grained sentiment insights from user-generated content, particularly in e-commerce and social media domains. However, research on Bangla ABSA remains significantly underexplored due to the absence of comprehensive datasets and specialized frameworks for triplet extraction in this language. This paper introduces BanglaASTE, a novel framework for Aspect Sentiment Triplet Extraction (ASTE) that simultaneously identifies aspect terms, opinion expressions, and sentiment polarities from Bangla product reviews. Our contributions include: (1) creation of the first annotated Bangla ASTE dataset containing 3,345 product reviews collected from major e-commerce platforms including Daraz, Facebook, and Rokomari; (2) development of a hybrid classification framework that employs graph-based aspect-opinion matching with semantic similarity techniques; and (3) implementation of an ensemble model combining BanglaBERT contextual embeddings with XGBoost boosting algorithms for enhanced triplet extraction performance. Experimental results demonstrate that our ensemble approach achieves superior performance with 89.9% accuracy and 89.1% F1-score, significantly outperforming baseline models across all evaluation metrics. The framework effectively addresses key challenges in Bangla text processing including informal expressions, spelling variations, and data sparsity. This research advances the state-of-the-art in low-resource language sentiment analysis and provides a scalable solution for Bangla e-commerce analytics applications.",
        "authors": [
            "Ariful Islam",
            "Md Rifat Hossen",
            "Abir Ahmed",
            "B M Taslimul Haque"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-26T13:27:54+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21381v1"
    },
    {
        "arxiv_id": "2511.21334v1",
        "title": "Emergent Lexical Semantics in Neural Language Models: Testing Martin's Law on LLM-Generated Text",
        "abstract": "We present the first systematic investigation of Martin's Law - the empirical relationship between word frequency and polysemy - in text generated by neural language models during training. Using DBSCAN clustering of contextualized embeddings as an operationalization of word senses, we analyze four Pythia models (70M-1B parameters) across 30 training checkpoints. Our results reveal a non-monotonic developmental trajectory: Martin's Law emerges around checkpoint 100, reaches peak correlation (r > 0.6) at checkpoint 104, then degrades by checkpoint 105. Smaller models (70M, 160M) experience catastrophic semantic collapse at late checkpoints, while larger models (410M, 1B) show graceful degradation. The frequency-specificity trade-off remains stable (r $\\approx$ -0.3) across all models. These findings suggest that compliance with linguistic regularities in LLM-generated text is not monotonically increasing with training, but instead follows a balanced trajectory with an optimal semantic window. This work establishes a novel methodology for evaluating emergent linguistic structure in neural language models.",
        "authors": [
            "Kai Kugler"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T12:31:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21334v1"
    },
    {
        "arxiv_id": "2511.21322v1",
        "title": "TALES: A Taxonomy and Analysis of Cultural Representations in LLM-generated Stories",
        "abstract": "Millions of users across the globe turn to AI chatbots for their creative needs, inviting widespread interest in understanding how such chatbots represent diverse cultures. At the same time, evaluating cultural representations in open-ended tasks remains challenging and underexplored. In this work, we present TALES, an evaluation of cultural misrepresentations in LLM-generated stories for diverse Indian cultural identities. First, we develop TALES-Tax, a taxonomy of cultural misrepresentations by collating insights from participants with lived experiences in India through focus groups (N=9) and individual surveys (N=15). Using TALES-Tax, we evaluate 6 models through a large-scale annotation study spanning 2,925 annotations from 108 annotators with lived cultural experience from across 71 regions in India and 14 languages. Concerningly, we find that 88\\% of the generated stories contain one or more cultural inaccuracies, and such errors are more prevalent in mid- and low-resourced languages and stories based in peri-urban regions in India. Lastly, we transform the annotations into TALES-QA, a standalone question bank to evaluate the cultural knowledge of foundational models. Through this evaluation, we surprisingly discover that models often possess the requisite cultural knowledge despite generating stories rife with cultural misrepresentations.",
        "authors": [
            "Kirti Bhagat",
            "Shaily Bhatt",
            "Athul Velagapudi",
            "Aditya Vashistha",
            "Shachi Dave",
            "Danish Pruthi"
        ],
        "category": "cs.HC",
        "submitted_date": "2025-11-26T12:07:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21322v1"
    },
    {
        "arxiv_id": "2511.21285v1",
        "title": "PEFT-Bench: A Parameter-Efficient Fine-Tuning Methods Benchmark",
        "abstract": "Despite the state-of-the-art performance of Large Language Models (LLMs) achieved on many tasks, their massive scale often leads to high computational and environmental costs, limiting their accessibility. Parameter-efficient fine-tuning (PEFT) methods address this challenge by reducing the number of trainable parameters while maintaining strong downstream performance. Despite the increased development in PEFT methods, current evaluations remain limited (in terms of evaluated models and datasets) and difficult to reproduce. To bridge this gap, we introduce PEFT-Bench, a unified end-to-end benchmark for evaluating diverse PEFT methods on autoregressive LLMs. We demonstrate its usage across 27 NLP datasets and 6 PEFT methods. To account for different PEFT training and inference factors, we also introduce the PEFT Soft Score Penalties (PSCP) metric, which takes trainable parameters, inference speed, and training memory usage into account.",
        "authors": [
            "Robert Belanec",
            "Branislav Pecher",
            "Ivan Srba",
            "Maria Bielikova"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T11:18:06+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21285v1"
    },
    {
        "arxiv_id": "2511.21229v1",
        "title": "Developing an Open Conversational Speech Corpus for the Isan Language",
        "abstract": "This paper introduces the development of the first open conversational speech dataset for the Isan language, the most widely spoken regional dialect in Thailand. Unlike existing speech corpora that are primarily based on read or scripted speech, this dataset consists of natural speech, thereby capturing authentic linguistic phenomena such as colloquials, spontaneous prosody, disfluencies, and frequent code-switching with central Thai. A key challenge in building this resource lies in the lack of a standardized orthography for Isan. Current writing practices vary considerably, due to the different lexical tones between Thai and Isan. This variability complicates the design of transcription guidelines and poses questions regarding consistency, usability, and linguistic authenticity. To address these issues, we establish practical transcription protocols that balance the need for representational accuracy with the requirements of computational processing. By releasing this dataset as an open resource, we aim to contribute to inclusive AI development, support research on underrepresented languages, and provide a basis for addressing the linguistic and technical challenges inherent in modeling conversational speech.",
        "authors": [
            "Adisai Na-Thalang",
            "Chanakan Wittayasakpan",
            "Kritsadha Phatcharoen",
            "Supakit Buakaw"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T09:57:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21229v1"
    },
    {
        "arxiv_id": "2511.21218v1",
        "title": "Can Finetuing LLMs on Small Human Samples Increase Heterogeneity, Alignment, and Belief-Action Coherence?",
        "abstract": "There is ongoing debate about whether large language models (LLMs) can serve as substitutes for human participants in survey and experimental research. While recent work in fields such as marketing and psychology has explored the potential of LLM-based simulation, a growing body of evidence cautions against this practice: LLMs often fail to align with real human behavior, exhibiting limited diversity, systematic misalignment for minority subgroups, insufficient within-group variance, and discrepancies between stated beliefs and actions. This study examines an important and distinct question in this domain: whether fine-tuning on a small subset of human survey data, such as that obtainable from a pilot study, can mitigate these issues and yield realistic simulated outcomes. Using a behavioral experiment on information disclosure, we compare human and LLM-generated responses across multiple dimensions, including distributional divergence, subgroup alignment, belief-action coherence, and the recovery of regression coefficients. We find that fine-tuning on small human samples substantially improves heterogeneity, alignment, and belief-action coherence relative to the base model. However, even the best-performing fine-tuned models fail to reproduce the regression coefficients of the original study, suggesting that LLM-generated data remain unsuitable for replacing human participants in formal inferential analyses.",
        "authors": [
            "Steven Wang",
            "Kyle Hunt",
            "Shaojie Tang",
            "Kenneth Joseph"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T09:50:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21218v1"
    },
    {
        "arxiv_id": "2512.02044v1",
        "title": "Beyond Confidence: Adaptive and Coherent Decoding for Diffusion Language Models",
        "abstract": "Diffusion Language Models (DLMs) have recently achieved significant success due to their any-order generation capabilities. However, existing inference methods typically rely on local, immediate-step metrics such as confidence or entropy which inherently lack a more reliable perspective. This limitation frequently leads to inconsistent sampling trajectories and suboptimal generation quality. To address this, we propose Coherent Contextual Decoding (CCD), a novel inference framework built upon two core innovations. First, CCD employs a trajectory rectification mechanism that leverages historical context to enhance sequence coherence, enabling the early rejection of suboptimal paths. We demonstrate that this mechanism is theoretically equivalent to modeling the consistency of historical steps via the conditional mutual information between context and token predictions. Building on this theoretical insight, we further address the inefficiency of conventional uniform decoding budgets. Instead of rigid allocations based on diffusion steps, we introduce an adaptive sampling strategy that dynamically adjusts the unmasking budget for each step according to our consistency metric. Consequently, our method significantly improves the quality of generation trajectories while accelerating the sampling process. Empirically, our method achieves a simultaneous enhancement in both inference speed and performance across diverse benchmarks on Dream and LLaDA, delivering up to 3.48x speedup alongside 3.91% performance improvement.",
        "authors": [
            "Kecheng Chen",
            "Ziru Liu",
            "Xijia Tao",
            "Hui Liu",
            "Xinyu Fu",
            "Suiyun Zhang",
            "Dandan Tu",
            "Lingpeng Kong",
            "Rui Liu",
            "Haoliang Li"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T09:49:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02044v1"
    },
    {
        "arxiv_id": "2511.21214v2",
        "title": "Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines",
        "abstract": "Reasoning models have demonstrated remarkable capabilities in complex reasoning tasks. However, ensuring their safety against adversarial jailbreak prompts remains a critical challenge. Due to the covert and deceptive nature of such prompts, they can often evade built-in safety mechanisms and lead to the generation of harmful content. This underscores the need for an adaptive safety alignment approach that enables models to autonomously reinforce their defenses in response to adversarial inputs. This paper introduces the Synthesized Guideline-based Adaptive Safety Alignment (SGASA) framework, which internalizes model-generated safety guidelines to strengthen models' ability to enhance robustness against harmful adversarial prompts while minimizing unnecessary refusals of benign requests. SGASA consists of two key stages: Data Pre-synthesis, which generates safety guidelines and augmented prompts; and Alignment Fine-tuning, which leverages Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO) to embed these guidelines into the model. Extensive experiments across multiple datasets demonstrate that SGASA significantly improves model safety, validating its adaptive and scalable effectiveness.",
        "authors": [
            "Yuhang Wang",
            "Yanxu Zhu",
            "Dongyuan Lu",
            "Jitao Sang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T09:44:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21214v2"
    },
    {
        "arxiv_id": "2511.21188v1",
        "title": "AnchorOPT: Towards Optimizing Dynamic Anchors for Adaptive Prompt Learning",
        "abstract": "Existing prompt learning methods, which are built upon CLIP models, leverage textual tokens as anchors to guide the learnable soft tokens. This guidance improves CLIP generalizations. However, these anchors-static in both value and position-lack cross-task and stage-adaptive flexibility. To address this limitation, we propose AnchorOPT, a dynamic anchor-based prompt learning framework. Specifically, AnchorOPT introduces dynamism in two key dimensions: (i) anchor values eschew handcrafted explicit textual tokens (e.g., \"shape\", \"color\"), instead learning dynamically from task-specific data; and (ii) the positional relationship between anchor and soft tokens is no longer fixed but adaptively optimized via a learnable position matrix conditioned on the training stage and task context. Training occurs in two stages: we first learn the anchor tokens, then freeze and transfer them to the second stage for optimization of soft tokens and the position matrix. Extensive experiments demonstrate that using only a simple learnable anchor and position matrix achieves performance comparable to or exceeding some methods incorporating additional learnable modules or regularization techniques. As a plug-and-play module, AnchorOPT integrates seamlessly into existing frameworks, yielding consistent performance gains across diverse datasets. Code is publicly available at https://github.com/zhengli97/ATPrompt.",
        "authors": [
            "Zheng Li",
            "Yibing Song",
            "Xin Zhang",
            "Lei Luo",
            "Xiang Li",
            "Jian Yang"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-26T09:11:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21188v1"
    },
    {
        "arxiv_id": "2511.21140v1",
        "title": "How to Correctly Report LLM-as-a-Judge Evaluations",
        "abstract": "Large language models (LLMs) are increasingly used as evaluators in lieu of humans. While scalable, their judgments are noisy due to imperfect specificity and sensitivity of LLMs, leading to biased accuracy estimates. Although bias-correction methods exist, they are underutilized in LLM research and typically assume exact knowledge of the model's specificity and sensitivity. Furthermore, in general we only have estimates of these values and it is not well known how to properly construct confidence intervals using only estimates. This work presents a simple plug-in framework that corrects such bias and constructs confidence intervals reflecting uncertainty from both test and calibration dataset, enabling practical and statistically sound LLM-based evaluation. Additionally, to reduce uncertainty in the accuracy estimate, we introduce an adaptive algorithm that efficiently allocates calibration sample sizes.",
        "authors": [
            "Chungpa Lee",
            "Thomas Zeng",
            "Jongwon Jeong",
            "Jy-yong Sohn",
            "Kangwook Lee"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-26T07:46:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21140v1"
    },
    {
        "arxiv_id": "2511.21101v1",
        "title": "MortgageLLM: Domain-Adaptive Pretraining with Residual Instruction Transfer, Alignment Tuning, and Task-Specific Routing",
        "abstract": "Large Language Models (LLMs) demonstrate exceptional capabilities across general domains, yet their application to specialized sectors such as mortgage finance requires domain-specific knowledge augmentation while preserving instruction-following fidelity. We present MortgageLLM, a novel domain-specific large language model that addresses this dual challenge. It is developed using a dual-track specialization framework from a single base model (LLaMA-3.1-8B). We opted for this dual-expert approach as a single multi-task model suffers from performance trade-offs, where optimizing for structured tasks (via SFT) degrades conversational fidelity (via DPO). Our dual-track method solves this by creating two specialists, allowing each to be optimally trained for its distinct capability. Our approach applies the instruction residual technique to restore instruction-following capabilities post-domain adaptation without supervised fine-tuning. We contribute: (1) application of this residual technique to the highly specialized mortgage finance domain; (2) a dual-expert architecture combining a conversational Q&A model and a structured task model for classification and summarization; and (3) an intelligent task routing mechanism using few-shot classification performed by one of the expert models itself. We validate our approach on domain-specific benchmarks, where our final model (MLM v2) significantly outperforms the base LLaMA-3.1-8B-Instruct, achieving an LLM-as-a-Judge summarization score of 4.58 (vs. 3.99), a Q&A score of 4.09 (vs. 4.0), and a classification score of 2.6 (vs. 1.2). On semantic similarity, our model achieved a BERTScore of 0.77 for summarization (vs. 0.74), 0.68 for Q&A (vs. 0.58), and 0.75 for classification (vs. 0.73), substantially outperforming baseline approaches.",
        "authors": [
            "Manish Jain",
            "Satheesh Kumar Ponnambalam",
            "Salman Faroz",
            "Chandrakanth Lns",
            "Vinay Sharma"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T06:37:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21101v1"
    },
    {
        "arxiv_id": "2511.21088v1",
        "title": "ASR Error Correction in Low-Resource Burmese with Alignment-Enhanced Transformers using Phonetic Features",
        "abstract": "This paper investigates sequence-to-sequence Transformer models for automatic speech recognition (ASR) error correction in low-resource Burmese, focusing on different feature integration strategies including IPA and alignment information. To our knowledge, this is the first study addressing ASR error correction specifically for Burmese. We evaluate five ASR backbones and show that our ASR Error Correction (AEC) approaches consistently improve word- and character-level accuracy over baseline outputs. The proposed AEC model, combining IPA and alignment features, reduced the average WER of ASR models from 51.56 to 39.82 before augmentation (and 51.56 to 43.59 after augmentation) and improving chrF++ scores from 0.5864 to 0.627, demonstrating consistent gains over the baseline ASR outputs without AEC. Our results highlight the robustness of AEC and the importance of feature design for improving ASR outputs in low-resource settings.",
        "authors": [
            "Ye Bhone Lin",
            "Thura Aung",
            "Ye Kyaw Thu",
            "Thazin Myint Oo"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T06:13:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21088v1"
    },
    {
        "arxiv_id": "2511.21086v1",
        "title": "Orthographic Constraint Satisfaction and Human Difficulty Alignment in Large Language Models",
        "abstract": "Large language models must satisfy hard orthographic constraints during controlled text generation, yet systematic cross-architecture evaluation remains limited. We evaluate 28 configurations spanning three model families (Qwen3, Claude Haiku-4.5, GPT-5-mini) on 58 word puzzles requiring character-level constraint satisfaction. Architectural differences produce substantially larger performance gaps (2.0-2.2x, F1=0.761 vs. 0.343) than parameter scaling within families (83% gain from eightfold scaling), suggesting that constraint satisfaction may require specialized architectural features or training objectives beyond standard language model scaling. Thinking budget sensitivity proves heterogeneous: high-capacity models show strong returns (+0.102 to +0.136 F1), while mid-sized variants saturate or degrade. These patterns are inconsistent with uniform compute benefits. Using difficulty ratings from 10,000 human solvers per puzzle, we establish modest but consistent calibration (r=0.24-0.38) across all families, yet identify systematic failures on common words with unusual orthography (\"data\", \"poop\", \"loll\": 86-95% human success, 89-96% model miss rate). These failures reveal over-reliance on distributional plausibility that penalizes orthographically atypical but constraint-valid patterns, suggesting architectural innovations may be required beyond simply scaling parameters or computational budgets.",
        "authors": [
            "Bryan E. Tuck",
            "Rakesh M. Verma"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T06:12:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21086v1"
    },
    {
        "arxiv_id": "2511.21081v1",
        "title": "Enhancing Burmese News Classification with Kolmogorov-Arnold Network Head Fine-tuning",
        "abstract": "In low-resource languages like Burmese, classification tasks often fine-tune only the final classification layer, keeping pre-trained encoder weights frozen. While Multi-Layer Perceptrons (MLPs) are commonly used, their fixed non-linearity can limit expressiveness and increase computational cost. This work explores Kolmogorov-Arnold Networks (KANs) as alternative classification heads, evaluating Fourier-based FourierKAN, Spline-based EfficientKAN, and Grid-based FasterKAN-across diverse embeddings including TF-IDF, fastText, and multilingual transformers (mBERT, Distil-mBERT). Experimental results show that KAN-based heads are competitive with or superior to MLPs. EfficientKAN with fastText achieved the highest F1-score (0.928), while FasterKAN offered the best trade-off between speed and accuracy. On transformer embeddings, EfficientKAN matched or slightly outperformed MLPs with mBERT (0.917 F1). These findings highlight KANs as expressive, efficient alternatives to MLPs for low-resource language classification.",
        "authors": [
            "Thura Aung",
            "Eaint Kay Khaing Kyaw",
            "Ye Kyaw Thu",
            "Thazin Myint Oo",
            "Thepchai Supnithi"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T05:50:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21081v1"
    },
    {
        "arxiv_id": "2511.21066v1",
        "title": "Context-Aware Pragmatic Metacognitive Prompting for Sarcasm Detection",
        "abstract": "Detecting sarcasm remains a challenging task in the areas of Natural Language Processing (NLP) despite recent advances in neural network approaches. Currently, Pre-trained Language Models (PLMs) and Large Language Models (LLMs) are the preferred approach for sarcasm detection. However, the complexity of sarcastic text, combined with linguistic diversity and cultural variation across communities, has made the task more difficult even for PLMs and LLMs. Beyond that, those models also exhibit unreliable detection of words or tokens that require extra grounding for analysis. Building on a state-of-the-art prompting method in LLMs for sarcasm detection called Pragmatic Metacognitive Prompting (PMP), we introduce a retrieval-aware approach that incorporates retrieved contextual information for each target text. Our pipeline explores two complementary ways to provide context: adding non-parametric knowledge using web-based retrieval when the model lacks necessary background, and eliciting the model's own internal knowledge for a self-knowledge awareness strategy. We evaluated our approach with three datasets, such as Twitter Indonesia Sarcastic, SemEval-2018 Task 3, and MUStARD. Non-parametric retrieval resulted in a significant 9.87% macro-F1 improvement on Twitter Indonesia Sarcastic compared to the original PMP method. Self-knowledge retrieval improves macro-F1 by 3.29% on Semeval and by 4.08% on MUStARD. These findings highlight the importance of context in enhancing LLMs performance in sarcasm detection task, particularly the involvement of culturally specific slang, references, or unknown terms to the LLMs. Future work will focus on optimizing the retrieval of relevant contextual information and examining how retrieval quality affects performance. The experiment code is available at: https://github.com/wllchrst/sarcasm-detection_pmp_knowledge-base.",
        "authors": [
            "Michael Iskandardinata",
            "William Christian",
            "Derwin Suhartono"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T05:19:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21066v1"
    },
    {
        "arxiv_id": "2511.21060v1",
        "title": "Zipf Distributions from Two-Stage Symbolic Processes: Stability Under Stochastic Lexical Filtering",
        "abstract": "Zipf's law in language lacks a definitive origin, debated across fields. This study explains Zipf-like behavior using geometric mechanisms without linguistic elements. The Full Combinatorial Word Model (FCWM) forms words from a finite alphabet, generating a geometric distribution of word lengths. Interacting exponential forces yield a power-law rank-frequency curve, determined by alphabet size and blank symbol probability. Simulations support predictions, matching English, Russian, and mixed-genre data. The symbolic model suggests Zipf-type laws arise from geometric constraints, not communicative efficiency.",
        "authors": [
            "Vladimir Berman"
        ],
        "category": "stat.ME",
        "submitted_date": "2025-11-26T04:59:40+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21060v1"
    },
    {
        "arxiv_id": "2511.21056v1",
        "title": "A Unified Understanding of Offline Data Selection and Online Self-refining Generation for Post-training LLMs",
        "abstract": "Offline data selection and online self-refining generation, which enhance the data quality, are crucial steps in adapting large language models (LLMs) to specific downstream tasks. We tackle offline data selection and online self-refining generations through an optimization perspective. Specifically, bilevel data selection is used for offline data selection with respect to the validation dataset, and we treat online self-refining generation as a model adaptation step of selecting the model trained on current responses that best fits the validation data. Our framework offers a unified understanding of offline data selection and self-refining generation by assigning a learned data weight to each question and response, either explicitly or implicitly. For the first time, we theoretically demonstrate the effectiveness of the bilevel data selection framework and demonstrate its performance gains over unfiltered direct mixing baselines. By combining offline data with validation-weighted online generations, our method enhances fine-tuning performance. Experiments on quality enhancement and safety-aware LLM fine-tuning validate its effectiveness.",
        "authors": [
            "Quan Xiao",
            "Tianyi Chen"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-26T04:48:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21056v1"
    },
    {
        "arxiv_id": "2511.21038v1",
        "title": "Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels",
        "abstract": "Can in-context learning (ICL) override pre-trained label semantics, or does it merely refine an existing semantic backbone? We address this question by treating LLMs as prompt-induced classifiers and contrasting their behavior under \\emph{natural} demonstrations (with correct labels) and \\emph{inverted} demonstrations (systematically flipping label meanings). We decompose ICL behavior into three alignment metrics (truth, prior, and prompt alignment) and introduce a semantic override rate, defined as correctness under flipped semantics. Across eight classification tasks and eight open-source LLMs (1--12B parameters), we find consistent evidence for a semantic anchor view. With natural demonstrations, ICL improves accuracy while maintaining strong prior alignment; most correct predictions coincide with zero-shot behavior, even when the prior is weak. With inverted demonstrations, models cannot learn coherent anti-semantic classifiers: prompt alignment increases only by sacrificing accuracy, and semantic override rates remain exactly zero in our few-shot 1--12B setting. Rather than flexibly remapping label meanings, ICL primarily adjusts how inputs project onto stable semantic directions learned during pre-training, clarifying fundamental limits of few-shot prompting and suggesting that overriding label semantics at these scales requires interventions beyond ICL. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/semantic-anchors-icl.",
        "authors": [
            "Anantha Padmanaban Krishna Kumar"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T04:14:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21038v1"
    },
    {
        "arxiv_id": "2511.21016v1",
        "title": "Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression",
        "abstract": "As efficient alternatives to softmax Attention, linear state-space models (SSMs) achieve constant memory and linear compute, but maintain only a lossy, fading summary of the past, often leading to inferior performance in recall oriented tasks. We propose Gated KalmaNet (GKA), a layer that reduces this gap by accounting for the full past when predicting the next token, while maintaining SSM-style efficiency. GKA achieves this by solving an online ridge regression problem at test time, with constant memory and linear compute cost in the sequence length. Drawing inspiration from the Kalman Filter, we iteratively solve the online ridge regression problem. However, a critical insight is that standard Kalman filter equations are numerically unstable in low-precision environments (like bfloat16) and difficult to parallelize in modern hardware. We address both challenges through two key innovations: (1) an adaptive regularization strategy with input-dependent gating that controls the condition number of the ridge regression, ensuring numerical stability while balancing memory retention. And (2) the use of Chebyshev Iteration instead of other conventional iterative solvers, which we demonstrate to be more stable in low-precision settings. To further improve scalability, we develop a hardware-aware chunk-wise implementation of Chebyshev Iteration along with custom kernels for backpropagating through our adaptive regularization and gating mechanisms. Empirically, GKA shows strong language understanding capabilites on short-context tasks outperforming existing SSM layers (like Mamba2, GLA and Gated DeltaNet). On long-context, GKA excels at real-world RAG and LongQA tasks up to 128k tokens, achieving more than $10$% relative improvement over other fading memory baselines.",
        "authors": [
            "Liangzu Peng",
            "Aditya Chattopadhyay",
            "Luca Zancato",
            "Elvis Nunez",
            "Wei Xia",
            "Stefano Soatto"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-26T03:26:37+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21016v1"
    },
    {
        "arxiv_id": "2511.21006v2",
        "title": "TrackList: Tracing Back Query Linguistic Diversity for Head and Tail Knowledge in Open Large Language Models",
        "abstract": "Large Language Models (LLMs) have proven efficient in giving definition-type answers to user input queries. While for humans giving various types of answers, such as examples and paraphrases, is an easy task, LLMs struggle to provide correct answers for other than definition-type queries. In this study, we evaluated this drop in performance using TrackList, a fine-grained linguistic and statistical analysis pipeline to investigate the impact of the pre-training data on LLMs answers to diverse linguistic queries. We also introduce RefoMed-EN, an English dataset consisting of 6170 human-annotated medical terms alongside their corresponding definitions, denominations, exemplifications, explanations, or paraphrases. We studied whether the high frequency of a concept (head) or low frequency (tail) impacts the language model's performance. We evaluated the quality of the LLM's output using syntactic and semantic similarity metrics, statistical correlations and embeddings. Results showed that the LLM's task performance for definition type questions is the highest, while for the exemplification type it is the lowest. Additionally, we showed that for definition-type questions, large language models are prone to paraphrase more on popular and frequent knowledge and less on tail and technical knowledge, especially in the expert texts.",
        "authors": [
            "Ioana Buhnila",
            "Aman Sinha",
            "Mathieu Constant"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T03:14:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21006v2"
    },
    {
        "arxiv_id": "2511.20974v1",
        "title": "RosettaSpeech: Zero-Shot Speech-to-Speech Translation from Monolingual Data",
        "abstract": "The scarcity of parallel speech corpora critically hampers speech-to-speech translation (S2ST), often forcing reliance on complex, multi-stage pipelines. This paper introduces RosettaSpeech, a novel and simplified framework for zero-shot S2ST that is trained on monolingual speech-text data augmented by machine translation supervision. While our method leverages the linguistic knowledge inherent in text-based NMT models, it strictly eliminates the need for parallel speech-to-speech pairs. Our model uniquely uses text as an intermediate bridge during training but functions as a direct, end-to-end speech-to-speech model at inference. This streamlined approach achieves state-of-the-art results on standard benchmarks. For instance, on the CVSS-C test set, RosettaSpeech outperforms leading systems, achieving an ASR-BLEU score of 25.17 for German-to-English and 29.86 for Spanish-to-English-relative gains of over 27% and 14%, respectively. Furthermore, we demonstrate that a single model can deliver strong many-to-one translation performance (FR/ES/DE -> EN). We also provide a foundational analysis of how training data scaling impacts model performance. By prioritizing reliance on abundant parallel text rather than difficult-to-acquire parallel speech, RosettaSpeech offers a scalable path to creating high-quality, speaker-preserving S2ST for a much broader array of languages.",
        "authors": [
            "Zhisheng Zheng",
            "Xiaohang Sun",
            "Tuan Dinh",
            "Abhishek Yanamandra",
            "Abhinav Jain",
            "Zhu Liu",
            "Sunil Hadap",
            "Vimal Bhat",
            "Manoj Aggarwal",
            "Gerard Medioni",
            "David Harwath"
        ],
        "category": "eess.AS",
        "submitted_date": "2025-11-26T02:02:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20974v1"
    },
    {
        "arxiv_id": "2511.20973v1",
        "title": "Towards Audio Token Compression in Large Audio Language Models",
        "abstract": "Large Audio Language Models (LALMs) demonstrate impressive performance across diverse tasks, ranging from speech recognition to general audio understanding. However, their scalability is limited by the quadratic complexity of attention and the high token rates of audio signals. These challenges make it difficult to extend LALMs to long-form audio and to deploy them on resource-constrained platforms such as edge devices.   In this paper, we explore techniques such as unsupervised segmentation, uniform average pooling, etc., to reduce the number of audio tokens generated by the LALM's audio encoder but before they are consumed by the LLM decoder. To mitigate potential performance degradation introduced by the compressed representations, we employ low-rank adapters to finetune the model. We evaluate our proposed models on two tasks, automatic speech recognition and speech-to-speech translation tasks, that are dependent on effectively uncovering the underlying lexical content of the input signal and study the effect of downsampling on these tasks. Experimental results show that compressed LALMs can achieve performance closer to frame-level LALMs while reducing the input audio token count upto three times before the LLM backbone.",
        "authors": [
            "Saurabhchand Bhati",
            "Samuel Thomas",
            "Hilde Kuehne",
            "Rogerio Feris",
            "James Glass"
        ],
        "category": "eess.AS",
        "submitted_date": "2025-11-26T02:00:38+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20973v1"
    },
    {
        "arxiv_id": "2511.20965v1",
        "title": "TrafficLens: Multi-Camera Traffic Video Analysis Using LLMs",
        "abstract": "Traffic cameras are essential in urban areas, playing a crucial role in intelligent transportation systems. Multiple cameras at intersections enhance law enforcement capabilities, traffic management, and pedestrian safety. However, efficiently managing and analyzing multi-camera feeds poses challenges due to the vast amount of data. Analyzing such huge video data requires advanced analytical tools. While Large Language Models (LLMs) like ChatGPT, equipped with retrieval-augmented generation (RAG) systems, excel in text-based tasks, integrating them into traffic video analysis demands converting video data into text using a Vision-Language Model (VLM), which is time-consuming and delays the timely utilization of traffic videos for generating insights and investigating incidents. To address these challenges, we propose TrafficLens, a tailored algorithm for multi-camera traffic intersections. TrafficLens employs a sequential approach, utilizing overlapping coverage areas of cameras. It iteratively applies VLMs with varying token limits, using previous outputs as prompts for subsequent cameras, enabling rapid generation of detailed textual descriptions while reducing processing time. Additionally, TrafficLens intelligently bypasses redundant VLM invocations through an object-level similarity detector. Experimental results with real-world datasets demonstrate that TrafficLens reduces video-to-text conversion time by up to $4\\times$ while maintaining information accuracy.",
        "authors": [
            "Md Adnan Arefeen",
            "Biplob Debnath",
            "Srimat Chakradhar"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-26T01:34:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20965v1"
    },
    {
        "arxiv_id": "2511.20940v1",
        "title": "Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs",
        "abstract": "Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems. KGs are widely used in enterprise and domain applications to provide structured, evolving, and reliable knowledge. Large language models (LLMs) enable natural and context-aware conversations, but lack direct access to private and dynamic KGs. Retrieval-augmented generation (RAG) systems can retrieve graph content but often serialize structure, struggle with multi-turn context, and require heavy indexing. Traditional KGQA systems preserve structure but typically support only single-turn QA, incur high latency, and struggle with coreference and context tracking. To address these limitations, we propose Chatty-KG, a modular multi-agent system for conversational QA over KGs. Chatty-KG combines RAG-style retrieval with structured execution by generating SPARQL queries through task-specialized LLM agents. These agents collaborate for contextual interpretation, dialogue tracking, entity and relation linking, and efficient query planning, enabling accurate and low-latency translation of natural questions into executable queries. Experiments on large and diverse KGs show that Chatty-KG significantly outperforms state-of-the-art baselines in both single-turn and multi-turn settings, achieving higher F1 and P@1 scores. Its modular design preserves dialogue coherence and supports evolving KGs without fine-tuning or pre-processing. Evaluations with commercial (e.g., GPT-4o, Gemini-2.0) and open-weight (e.g., Phi-4, Gemma 3) LLMs confirm broad compatibility and stable performance. Overall, Chatty-KG unifies conversational flexibility with structured KG grounding, offering a scalable and extensible approach for reliable multi-turn KGQA.",
        "authors": [
            "Reham Omar",
            "Abdelghny Orogat",
            "Ibrahim Abdelaziz",
            "Omij Mangukiya",
            "Panos Kalnis",
            "Essam Mansour"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-26T00:18:55+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20940v1"
    },
    {
        "arxiv_id": "2511.20937v1",
        "title": "ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction",
        "abstract": "Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.",
        "authors": [
            "Qineng Wang",
            "Wenlong Huang",
            "Yu Zhou",
            "Hang Yin",
            "Tianwei Bao",
            "Jianwen Lyu",
            "Weiyu Liu",
            "Ruohan Zhang",
            "Jiajun Wu",
            "Li Fei-Fei",
            "Manling Li"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-26T00:06:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20937v1"
    },
    {
        "arxiv_id": "2511.20910v1",
        "title": "Emergence and Localisation of Semantic Role Circuits in LLMs",
        "abstract": "Despite displaying semantic competence, large language models' internal mechanisms that ground abstract semantic structure remain insufficiently characterised. We propose a method integrating role-cross minimal pairs, temporal emergence analysis, and cross-model comparison to study how LLMs implement semantic roles. Our analysis uncovers: (i) highly concentrated circuits (89-94% attribution within 28 nodes); (ii) gradual structural refinement rather than phase transitions, with larger models sometimes bypassing localised circuits; and (iii) moderate cross-scale conservation (24-59% component overlap) alongside high spectral similarity. These findings suggest that LLMs form compact, causally isolated mechanisms for abstract semantic structure, and these mechanisms exhibit partial transfer across scales and architectures.",
        "authors": [
            "Nura Aljaafari",
            "Danilo S. Carvalho",
            "Andr\u00e9 Freitas"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T22:51:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20910v1"
    },
    {
        "arxiv_id": "2511.20872v1",
        "title": "Winning with Less for Low Resource Languages: Advantage of Cross-Lingual English_Persian Argument Mining Model over LLM Augmentation",
        "abstract": "Argument mining is a subfield of natural language processing to identify and extract the argument components, like premises and conclusions, within a text and to recognize the relations between them. It reveals the logical structure of texts to be used in tasks like knowledge extraction. This paper aims at utilizing a cross-lingual approach to argument mining for low-resource languages, by constructing three training scenarios. We examine the models on English, as a high-resource language, and Persian, as a low-resource language. To this end, we evaluate the models based on the English Microtext corpus \\citep{PeldszusStede2015}, and its parallel Persian translation. The learning scenarios are as follow: (i) zero-shot transfer, where the model is trained solely with the English data, (ii) English-only training enhanced by synthetic examples generated by Large Language Models (LLMs), and (iii) a cross-lingual model that combines the original English data with manually translated Persian sentences. The zero-shot transfer model attains F1 scores of 50.2\\% on the English test set and 50.7\\% on the Persian test set. LLM-based augmentation model improves the performance up to 59.2\\% on English and 69.3\\% on Persian. The cross-lingual model, trained on both languages but evaluated solely on the Persian test set, surpasses the LLM-based variant, by achieving a F1 of 74.8\\%. Results indicate that a lightweight cross-lingual blend can outperform considerably the more resource-intensive augmentation pipelines, and it offers a practical pathway for the argument mining task to overcome data resource shortage on low-resource languages.",
        "authors": [
            "Ali Jahan",
            "Masood Ghayoomi",
            "Annette Hautli-Janisz"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T21:36:39+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20872v1"
    },
    {
        "arxiv_id": "2511.20857v1",
        "title": "Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory",
        "abstract": "Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving. This makes memory a critical component, yet its management and evolution remain largely underexplored. Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams. In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents. Evo-Memory structures datasets into sequential task streams, requiring LLMs to search, adapt, and evolve memory after each interaction. We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. To better benchmark experience reuse, we provide a baseline method, ExpRAG, for retrieving and utilizing prior experience, and further propose ReMem, an action-think-memory refine pipeline that tightly integrates reasoning, task actions, and memory updates to achieve continual improvement.",
        "authors": [
            "Tianxin Wei",
            "Noveen Sachdeva",
            "Benjamin Coleman",
            "Zhankui He",
            "Yuanchen Bei",
            "Xuying Ning",
            "Mengting Ai",
            "Yunzhe Li",
            "Jingrui He",
            "Ed H. Chi",
            "Chi Wang",
            "Shuo Chen",
            "Fernando Pereira",
            "Wang-Cheng Kang",
            "Derek Zhiyuan Cheng"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T21:08:07+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20857v1"
    },
    {
        "arxiv_id": "2511.20854v1",
        "title": "Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries",
        "abstract": "Visual content memorability has intrigued the scientific community for decades, with applications ranging widely, from understanding nuanced aspects of human memory to enhancing content design. A significant challenge in progressing the field lies in the expensive process of collecting memorability annotations from humans. This limits the diversity and scalability of datasets for modeling visual content memorability. Most existing datasets are limited to collecting aggregate memorability scores for visual content, not capturing the nuanced memorability signals present in natural, open-ended recall descriptions. In this work, we introduce the first large-scale unsupervised dataset designed explicitly for modeling visual memorability signals, containing over 82,000 videos, accompanied by descriptive recall data. We leverage tip-of-the-tongue (ToT) retrieval queries from online platforms such as Reddit. We demonstrate that our unsupervised dataset provides rich signals for two memorability-related tasks: recall generation and ToT retrieval. Large vision-language models fine-tuned on our dataset outperform state-of-the-art models such as GPT-4o in generating open-ended memorability descriptions for visual content. We also employ a contrastive training strategy to create the first model capable of performing multimodal ToT retrieval. Our dataset and models present a novel direction, facilitating progress in visual content memorability research.",
        "authors": [
            "Sree Bhattacharyya",
            "Yaman Kumar Singla",
            "Sudhir Yarram",
            "Somesh Kumar Singh",
            "Harini S",
            "James Z. Wang"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-25T21:02:26+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20854v1"
    },
    {
        "arxiv_id": "2511.20849v1",
        "title": "Length-MAX Tokenizer for Language Models",
        "abstract": "We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\\%, 17.2\\%, and 18.5\\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\\%, 12.7\\%, and 13.7\\% lower inference latency, together with a 16\\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\\% and enhancing HellaSwag accuracy by 4.3\\%. Moreover, the Length-MAX tokenizer achieves 99.62\\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\\% at inference.",
        "authors": [
            "Dong Dong",
            "Weijie Su"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T20:56:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20849v1"
    },
    {
        "arxiv_id": "2512.02043v1",
        "title": "Mirror, Mirror on the Wall -- Which is the Best Model of Them All?",
        "abstract": "Large Language Models (LLMs) have become one of the most transformative tools across many applications, as they have significantly boosted productivity and achieved impressive results in various domains such as finance, healthcare, education, telecommunications, and law, among others. Typically, state-of-the-art (SOTA) foundation models are developed by large corporations based on large data collections and substantial computational and financial resources required to pretrain such models from scratch. These foundation models then serve as the basis for further development and domain adaptation for specific use cases or tasks. However, given the dynamic and fast-paced nature of launching new foundation models, the process of selecting the most suitable model for a particular use case, application, or domain becomes increasingly complex. We argue that there are two main dimensions that need to be taken into consideration when selecting a model for further training: a qualitative dimension (which model is best suited for a task based on information, for instance, taken from model cards) and a quantitative dimension (which is the best performing model). The quantitative performance of models is assessed through leaderboards, which rank models based on standardized benchmarks and provide a consistent framework for comparing different LLMs. In this work, we address the analysis of the quantitative dimension by exploring the current leaderboards and benchmarks. To illustrate this analysis, we focus on the medical domain as a case study, demonstrating the evolution, current landscape, and practical significance of this quantitative evaluation dimension. Finally, we propose a Model Selection Methodology (MSM), a systematic approach designed to guide the navigation, prioritization, and selection of the model that best aligns with a given use case.",
        "authors": [
            "Dina Sayed",
            "Heiko Schuldt"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T20:52:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02043v1"
    },
    {
        "arxiv_id": "2511.20836v2",
        "title": "Structured Prompting Enables More Robust Evaluation of Language Models",
        "abstract": "As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless we approximate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. We present a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, we evaluate four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. We find that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks ($+$2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing chain-of-thought reduces LM sensitivity to prompt design (smaller $\u0394$ across prompts). To our knowledge, this is the first benchmarking study to systematically integrate structured prompting into an established evaluation framework, demonstrating how scalable performance-ceiling approximation yields more robust, decision-useful benchmarks. We open-source (i) DSPy+HELM Integration (https://github.com/stanford-crfm/helm/pull/3893) and (ii) Prompt Optimization Pipeline (https://github.com/StanfordMIMI/dspy-helm).",
        "authors": [
            "Asad Aali",
            "Muhammad Ahmed Mohsin",
            "Vasiliki Bikia",
            "Arnav Singhvi",
            "Richard Gaus",
            "Suhana Bedi",
            "Hejie Cui",
            "Miguel Fuentes",
            "Alyssa Unell",
            "Yifan Mai",
            "Jordan Cahoon",
            "Michael Pfeffer",
            "Roxana Daneshjou",
            "Sanmi Koyejo",
            "Emily Alsentzer",
            "Christopher Potts",
            "Nigam H. Shah",
            "Akshay S. Chaudhari"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T20:37:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20836v2"
    },
    {
        "arxiv_id": "2511.20821v1",
        "title": "Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion",
        "abstract": "Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.",
        "authors": [
            "Samuele Dell'Erba",
            "Andrew D. Bagdanov"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-25T20:20:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20821v1"
    },
    {
        "arxiv_id": "2511.20820v1",
        "title": "SAGE: An Agentic Explainer Framework for Interpreting SAE Features in Language Models",
        "abstract": "Large language models (LLMs) have achieved remarkable progress, yet their internal mechanisms remain largely opaque, posing a significant challenge to their safe and reliable deployment. Sparse autoencoders (SAEs) have emerged as a promising tool for decomposing LLM representations into more interpretable features, but explaining the features captured by SAEs remains a challenging task. In this work, we propose SAGE (SAE AGentic Explainer), an agent-based framework that recasts feature interpretation from a passive, single-pass generation task into an active, explanation-driven process. SAGE implements a rigorous methodology by systematically formulating multiple explanations for each feature, designing targeted experiments to test them, and iteratively refining explanations based on empirical activation feedback. Experiments on features from SAEs of diverse language models demonstrate that SAGE produces explanations with significantly higher generative and predictive accuracy compared to state-of-the-art baselines.an agent-based framework that recasts feature interpretation from a passive, single-pass generation task into an active, explanationdriven process. SAGE implements a rigorous methodology by systematically formulating multiple explanations for each feature, designing targeted experiments to test them, and iteratively refining explanations based on empirical activation feedback. Experiments on features from SAEs of diverse language models demonstrate that SAGE produces explanations with significantly higher generative and predictive accuracy compared to state-of-the-art baselines.",
        "authors": [
            "Jiaojiao Han",
            "Wujiang Xu",
            "Mingyu Jin",
            "Mengnan Du"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T20:14:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20820v1"
    },
    {
        "arxiv_id": "2511.20799v1",
        "title": "Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models",
        "abstract": "Large language models, trained on massive corpora, are prone to verbatim memorization of training data, creating significant privacy and copyright risks. While previous works have proposed various definitions for memorization, many exhibit shortcomings in comprehensively capturing this phenomenon, especially in aligned models. To address this, we introduce a novel framework: multi-prefix memorization. Our core insight is that memorized sequences are deeply encoded and thus retrievable via a significantly larger number of distinct prefixes than non-memorized content. We formalize this by defining a sequence as memorized if an external adversarial search can identify a target count of distinct prefixes that elicit it. This framework shifts the focus from single-path extraction to quantifying the robustness of a memory, measured by the diversity of its retrieval paths. Through experiments on open-source and aligned chat models, we demonstrate that our multi-prefix definition reliably distinguishes memorized from non-memorized data, providing a robust and practical tool for auditing data leakage in LLMs.",
        "authors": [
            "Trung Cuong Dang",
            "David Mohaisen"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T19:40:24+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20799v1"
    },
    {
        "arxiv_id": "2511.20639v1",
        "title": "Latent Collaboration in Multi-Agent Systems",
        "abstract": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.",
        "authors": [
            "Jiaru Zou",
            "Xiyuan Yang",
            "Ruizhong Qiu",
            "Gaotang Li",
            "Katherine Tieu",
            "Pan Lu",
            "Ke Shen",
            "Hanghang Tong",
            "Yejin Choi",
            "Jingrui He",
            "James Zou",
            "Mengdi Wang",
            "Ling Yang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T18:56:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20639v1"
    },
    {
        "arxiv_id": "2511.20604v1",
        "title": "On Evaluating LLM Alignment by Evaluating LLMs as Judges",
        "abstract": "Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.",
        "authors": [
            "Yixin Liu",
            "Pengfei Liu",
            "Arman Cohan"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T18:33:24+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20604v1"
    },
    {
        "arxiv_id": "2511.20561v2",
        "title": "Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward",
        "abstract": "Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox",
        "authors": [
            "Yuwei Niu",
            "Weiyang Jin",
            "Jiaqi Liao",
            "Chaoran Feng",
            "Peng Jin",
            "Bin Lin",
            "Zongjian Li",
            "Bin Zhu",
            "Weihao Yu",
            "Li Yuan"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-25T17:58:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20561v2"
    },
    {
        "arxiv_id": "2511.20547v1",
        "title": "From Words to Wisdom: Discourse Annotation and Baseline Models for Student Dialogue Understanding",
        "abstract": "Identifying discourse features in student conversations is quite important for educational researchers to recognize the curricular and pedagogical variables that cause students to engage in constructing knowledge rather than merely completing tasks. The manual analysis of student conversations to identify these discourse features is time-consuming and labor-intensive, which limits the scale and scope of studies. Leveraging natural language processing (NLP) techniques can facilitate the automatic detection of these discourse features, offering educational researchers scalable and data-driven insights. However, existing studies in NLP that focus on discourse in dialogue rarely address educational data. In this work, we address this gap by introducing an annotated educational dialogue dataset of student conversations featuring knowledge construction and task production discourse. We also establish baseline models for automatically predicting these discourse properties for each turn of talk within conversations, using pre-trained large language models GPT-3.5 and Llama-3.1. Experimental results indicate that these state-of-the-art models perform suboptimally on this task, indicating the potential for future research.",
        "authors": [
            "Farjana Sultana Mim",
            "Shuchin Aeron",
            "Eric Miller",
            "Kristen Wendell"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T17:46:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20547v1"
    },
    {
        "arxiv_id": "2511.20534v1",
        "title": "Bridging the Language Gap: Synthetic Voice Diversity via Latent Mixup for Equitable Speech Recognition",
        "abstract": "Modern machine learning models for audio tasks often exhibit superior performance on English and other well-resourced languages, primarily due to the abundance of available training data. This disparity leads to an unfair performance gap for low-resource languages, where data collection is both challenging and costly. In this work, we introduce a novel data augmentation technique for speech corpora designed to mitigate this gap. Through comprehensive experiments, we demonstrate that our method significantly improves the performance of automatic speech recognition systems on low-resource languages. Furthermore, we show that our approach outperforms existing augmentation strategies, offering a practical solution for enhancing speech technology in underrepresented linguistic communities.",
        "authors": [
            "Wesley Bian",
            "Xiaofeng Lin",
            "Guang Cheng"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T17:35:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20534v1"
    },
    {
        "arxiv_id": "2511.20513v1",
        "title": "DesignPref: Capturing Personal Preferences in Visual Design Generation",
        "abstract": "Generative models, such as large language models and text-to-image diffusion models, are increasingly used to create visual designs like user interfaces (UIs) and presentation slides. Finetuning and benchmarking these generative models have often relied on datasets of human-annotated design preferences. Yet, due to the subjective and highly personalized nature of visual design, preference varies widely among individuals. In this paper, we study this problem by introducing DesignPref, a dataset of 12k pairwise comparisons of UI design generation annotated by 20 professional designers with multi-level preference ratings. We found that among trained designers, substantial levels of disagreement exist (Krippendorff's alpha = 0.25 for binary preferences). Natural language rationales provided by these designers indicate that disagreements stem from differing perceptions of various design aspect importance and individual preferences. With DesignPref, we demonstrate that traditional majority-voting methods for training aggregated judge models often do not accurately reflect individual preferences. To address this challenge, we investigate multiple personalization strategies, particularly fine-tuning or incorporating designer-specific annotations into RAG pipelines. Our results show that personalized models consistently outperform aggregated baseline models in predicting individual designers' preferences, even when using 20 times fewer examples. Our work provides the first dataset to study personalized visual design evaluation and support future research into modeling individual design taste.",
        "authors": [
            "Yi-Hao Peng",
            "Jeffrey P. Bigham",
            "Jason Wu"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-25T17:19:10+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20513v1"
    },
    {
        "arxiv_id": "2511.20507v1",
        "title": "The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models",
        "abstract": "Large language models (LLMs) have emerged as a candidate \"model organism\" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.",
        "authors": [
            "Nathan Roll",
            "Jill Kries",
            "Flora Jin",
            "Catherine Wang",
            "Ann Marie Finley",
            "Meghan Sumner",
            "Cory Shain",
            "Laura Gwilliams"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T17:16:38+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20507v1"
    },
    {
        "arxiv_id": "2511.20494v3",
        "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
        "abstract": "We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Practical applications include embedding such adversarial images into websites to prevent MLLM-powered AI Agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and Adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.",
        "authors": [
            "Jakub Hoscilowicz",
            "Artur Janicki"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T17:00:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20494v3"
    },
    {
        "arxiv_id": "2511.20459v1",
        "title": "Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts",
        "abstract": "Recent advances in large language models have created new opportunities for stylometry, the study of writing styles and authorship. Two challenges, however, remain central: training generative models when no paired data exist, and evaluating stylistic text without relying only on human judgment. In this work, we present a framework for both generating and evaluating sentences in the style of 19th-century novelists. Large language models are fine-tuned with minimal, single-token prompts to produce text in the voices of authors such as Dickens, Austen, Twain, Alcott, and Melville. To assess these generative models, we employ a transformer-based detector trained on authentic sentences, using it both as a classifier and as a tool for stylistic explanation. We complement this with syntactic comparisons and explainable AI methods, including attention-based and gradient-based analyses, to identify the linguistic cues that drive stylistic imitation. Our findings show that the generated text reflects the authors' distinctive patterns and that AI-based evaluation offers a reliable alternative to human assessment. All artifacts of this work are published online.",
        "authors": [
            "Mosab Rezaei",
            "Mina Rajaei Moghadam",
            "Abdul Rahman Shaikh",
            "Hamed Alhoori",
            "Reva Freedman"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T16:25:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20459v1"
    },
    {
        "arxiv_id": "2511.20737v2",
        "title": "CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface Design",
        "abstract": "User interface (UI) design is an iterative process in which designers progressively refine their work with design software such as Figma or Sketch. Recent advances in vision language models (VLMs) with tool invocation suggest these models can operate design software to edit a UI design through iteration. Understanding and enhancing this capacity is important, as it highlights VLMs' potential to collaborate with designers within conventional software. However, as no existing benchmark evaluates tool-based design performance, the capacity remains unknown. To address this, we introduce CANVAS, a benchmark for VLMs on tool-based user interface design. Our benchmark contains 598 tool-based design tasks paired with ground-truth references sampled from 3.3K mobile UI designs across 30 function-based categories (e.g., onboarding, messaging). In each task, a VLM updates the design step-by-step through context-based tool invocations (e.g., create a rectangle as a button background), linked to design software. Specifically, CANVAS incorporates two task types: (i) design replication evaluates the ability to reproduce a whole UI screen; (ii) design modification evaluates the ability to modify a specific part of an existing screen. Results suggest that leading models exhibit more strategic tool invocations, improving design quality. Furthermore, we identify common error patterns models exhibit, guiding future work in enhancing tool-based design capabilities.",
        "authors": [
            "Daeheon Jeong",
            "Seoyeon Byun",
            "Kihoon Son",
            "Dae Hyun Kim",
            "Juho Kim"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-25T16:13:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20737v2"
    },
    {
        "arxiv_id": "2511.20736v1",
        "title": "Large Language Models' Complicit Responses to Illicit Instructions across Socio-Legal Contexts",
        "abstract": "Large language models (LLMs) are now deployed at unprecedented scale, assisting millions of users in daily tasks. However, the risk of these models assisting unlawful activities remains underexplored. In this study, we define this high-risk behavior as complicit facilitation - the provision of guidance or support that enables illicit user instructions - and present four empirical studies that assess its prevalence in widely deployed LLMs. Using real-world legal cases and established legal frameworks, we construct an evaluation benchmark spanning 269 illicit scenarios and 50 illicit intents to assess LLMs' complicit facilitation behavior. Our findings reveal widespread LLM susceptibility to complicit facilitation, with GPT-4o providing illicit assistance in nearly half of tested cases. Moreover, LLMs exhibit deficient performance in delivering credible legal warnings and positive guidance. Further analysis uncovers substantial safety variation across socio-legal contexts. On the legal side, we observe heightened complicity for crimes against societal interests, non-extreme but frequently occurring violations, and malicious intents driven by subjective motives or deceptive justifications. On the social side, we identify demographic disparities that reveal concerning complicit patterns towards marginalized and disadvantaged groups, with older adults, racial minorities, and individuals in lower-prestige occupations disproportionately more likely to receive unlawful guidance. Analysis of model reasoning traces suggests that model-perceived stereotypes, characterized along warmth and competence, are associated with the model's complicit behavior. Finally, we demonstrate that existing safety alignment strategies are insufficient and may even exacerbate complicit behavior.",
        "authors": [
            "Xing Wang",
            "Huiyuan Xie",
            "Yiyan Wang",
            "Chaojun Xiao",
            "Huimin Chen",
            "Holli Sargeant",
            "Felix Steffek",
            "Jie Shao",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "category": "cs.CY",
        "submitted_date": "2025-11-25T16:01:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20736v1"
    },
    {
        "arxiv_id": "2511.20409v1",
        "title": "A Task-Oriented Evaluation Framework for Text Normalization in Modern NLP Pipelines",
        "abstract": "Text normalization is an essential preprocessing step in many natural language processing (NLP) tasks, and stemming is one such normalization technique that reduces words to their base or root form. However, evaluating stemming methods is challenging because current evaluation approaches are limited and do not capture the potential harm caused by excessive stemming; therefore, it is essential to develop new approaches to evaluate stemming methods. To address this issue, this study propose a novel, task-oriented approach to evaluate stemming methods, which considers three aspects: (1) the utility of stemming using Stemming Effectiveness Score (SES), (2) the impact of stemming on downstream tasks using Model Performance Delta (MPD), and (3) the semantic similarity between stemmed and original words using Average Normalized Levenshtein Distance (ANLD), thus providing a comprehensive evaluation framework. We apply our evaluation framework to compare two stemmers for Bangla (BNLTK) and English (Snowball), and our results reveal a significant issue, prompting us to analyze their performance in detail. While the Bangla stemmer achieves the highest SES (1.67) due to effective word reduction (CR = 1.90), SES alone is insufficient because our proposed safety measure, ANLD, reveals that this high SES is due to harmful over-stemming (ANLD = 0.26), which correlates with the observed decrease in downstream performance.In contrast, the English stemmer achieves a moderate SES (1.31) with a safe meaning distance (ANLD = 0.14), allowing its word reduction to contribute positively to downstream performance; therefore, it is a more reliable stemmer. Our study provides a valuable tool for distinguishing between potential efficiency gains (high SES) and meaning preservation (low ANLD).",
        "authors": [
            "Md Abdullah Al Kafi",
            "Raka Moni",
            "Sumit Kumar Banshal"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T15:35:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20409v1"
    },
    {
        "arxiv_id": "2511.20399v2",
        "title": "BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali",
        "abstract": "Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.",
        "authors": [
            "Abdullah Al Sefat"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T15:26:47+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20399v2"
    },
    {
        "arxiv_id": "2511.20347v2",
        "title": "Soft Adaptive Policy Optimization",
        "abstract": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.",
        "authors": [
            "Chang Gao",
            "Chujie Zheng",
            "Xiong-Hui Chen",
            "Kai Dang",
            "Shixuan Liu",
            "Bowen Yu",
            "An Yang",
            "Shuai Bai",
            "Jingren Zhou",
            "Junyang Lin"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-25T14:25:19+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20347v2"
    },
    {
        "arxiv_id": "2511.20344v1",
        "title": "The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models",
        "abstract": "Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.",
        "authors": [
            "Taewhoo Lee",
            "Minju Song",
            "Chanwoong Yoon",
            "Jungwoo Park",
            "Jaewoo Kang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T14:23:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20344v1"
    },
    {
        "arxiv_id": "2511.20340v1",
        "title": "Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios",
        "abstract": "Speculative decoding accelerates LLM inference by utilizing otherwise idle computational resources during memory-to-chip data transfer. Current speculative decoding methods typically assume a considerable amount of available computing power, then generate a complex and massive draft tree using a small autoregressive language model to improve overall prediction accuracy. However, methods like batching have been widely applied in mainstream model inference systems as a superior alternative to speculative decoding, as they compress the available idle computing power. Therefore, performing speculative decoding with low verification resources and low scheduling costs has become an important research problem. We believe that more capable models that allow for parallel generation on draft sequences are what we truly need. Recognizing the fundamental nature of draft models to only generate sequences of limited length, we propose SpecFormer, a novel architecture that integrates unidirectional and bidirectional attention mechanisms. SpecFormer combines the autoregressive model's ability to extract information from the entire input sequence with the parallel generation benefits of non-autoregressive models. This design eliminates the reliance on large prefix trees and achieves consistent acceleration, even in large-batch scenarios. Through lossless speculative decoding experiments across models of various scales, we demonstrate that SpecFormer sets a new standard for scaling LLM inference with lower training demands and reduced computational costs.",
        "authors": [
            "Luohe Shi",
            "Zuchao Li",
            "Lefei Zhang",
            "Baoyuan Qi",
            "Guoming Liu",
            "Hai Zhao"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T14:20:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20340v1"
    },
    {
        "arxiv_id": "2511.20733v1",
        "title": "InvisibleBench: A Deployment Gate for Caregiving Relationship AI",
        "abstract": "InvisibleBench is a deployment gate for caregiving-relationship AI, evaluating 3-20+ turn interactions across five dimensions: Safety, Compliance, Trauma-Informed Design, Belonging/Cultural Fitness, and Memory. The benchmark includes autofail conditions for missed crises, medical advice (WOPR Act), harmful information, and attachment engineering. We evaluate four frontier models across 17 scenarios (N=68) spanning three complexity tiers. All models show significant safety gaps (11.8-44.8 percent crisis detection), indicating the necessity of deterministic crisis routing in production systems. DeepSeek Chat v3 achieves the highest overall score (75.9 percent), while strengths differ by dimension: GPT-4o Mini leads Compliance (88.2 percent), Gemini leads Trauma-Informed Design (85.0 percent), and Claude Sonnet 4.5 ranks highest in crisis detection (44.8 percent). We release all scenarios, judge prompts, and scoring configurations with code. InvisibleBench extends single-turn safety tests by evaluating longitudinal risk, where real harms emerge. No clinical claims; this is a deployment-readiness evaluation.",
        "authors": [
            "Ali Madad"
        ],
        "category": "cs.CY",
        "submitted_date": "2025-11-25T14:09:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20733v1"
    },
    {
        "arxiv_id": "2511.20315v1",
        "title": "Geometry of Decision Making in Language Models",
        "abstract": "Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \\textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.",
        "authors": [
            "Abhinav Joshi",
            "Divyanshu Bhatt",
            "Ashutosh Modi"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-25T13:52:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20315v1"
    },
    {
        "arxiv_id": "2511.20273v1",
        "title": "Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits",
        "abstract": "Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.",
        "authors": [
            "Areeb Ahmad",
            "Abhinav Joshi",
            "Ashutosh Modi"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-25T12:59:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20273v1"
    },
    {
        "arxiv_id": "2511.20233v2",
        "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
        "abstract": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
        "authors": [
            "Chuyi Kong",
            "Gao Wei",
            "Jing Ma",
            "Hongzhan Lin",
            "Yaxin Fan"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T12:06:23+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20233v2"
    },
    {
        "arxiv_id": "2511.20182v1",
        "title": "KyrgyzBERT: A Compact, Efficient Language Model for Kyrgyz NLP",
        "abstract": "Kyrgyz remains a low-resource language with limited foundational NLP tools. To address this gap, we introduce KyrgyzBERT, the first publicly available monolingual BERT-based language model for Kyrgyz. The model has 35.9M parameters and uses a custom tokenizer designed for the language's morphological structure. To evaluate performance, we create kyrgyz-sst2, a sentiment analysis benchmark built by translating the Stanford Sentiment Treebank and manually annotating the full test set. KyrgyzBERT fine-tuned on this dataset achieves an F1-score of 0.8280, competitive with a fine-tuned mBERT model five times larger. All models, data, and code are released to support future research in Kyrgyz NLP.",
        "authors": [
            "Adilet Metinov",
            "Gulida M. Kudakeeva",
            "Gulnara D. Kabaeva"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T11:05:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20182v1"
    },
    {
        "arxiv_id": "2511.20143v1",
        "title": "SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models",
        "abstract": "Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.",
        "authors": [
            "Wen-Fang Su",
            "Hsiao-Wei Chou",
            "Wen-Yang Lin"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T10:06:50+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20143v1"
    },
    {
        "arxiv_id": "2511.20120v1",
        "title": "\"When Data is Scarce, Prompt Smarter\"... Approaches to Grammatical Error Correction in Low-Resource Settings",
        "abstract": "Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.",
        "authors": [
            "Somsubhra De",
            "Harsh Kumar",
            "Arun Prakash A"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T09:40:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20120v1"
    },
    {
        "arxiv_id": "2511.20107v1",
        "title": "Mispronunciation Detection and Diagnosis Without Model Training: A Retrieval-Based Approach",
        "abstract": "Mispronunciation Detection and Diagnosis (MDD) is crucial for language learning and speech therapy. Unlike conventional methods that require scoring models or training phoneme-level models, we propose a novel training-free framework that leverages retrieval techniques with a pretrained Automatic Speech Recognition model. Our method avoids phoneme-specific modeling or additional task-specific training, while still achieving accurate detection and diagnosis of pronunciation errors. Experiments on the L2-ARCTIC dataset show that our method achieves a superior F1 score of 69.60% while avoiding the complexity of model training.",
        "authors": [
            "Huu Tuong Tu",
            "Ha Viet Khanh",
            "Tran Tien Dat",
            "Vu Huan",
            "Thien Van Luong",
            "Nguyen Tien Cuong",
            "Nguyen Thi Thu Trang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T09:26:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20107v1"
    },
    {
        "arxiv_id": "2511.20106v1",
        "title": "EM2LDL: A Multilingual Speech Corpus for Mixed Emotion Recognition through Label Distribution Learning",
        "abstract": "This study introduces EM2LDL, a novel multilingual speech corpus designed to advance mixed emotion recognition through label distribution learning. Addressing the limitations of predominantly monolingual and single-label emotion corpora \\textcolor{black}{that restrict linguistic diversity, are unable to model mixed emotions, and lack ecological validity}, EM2LDL comprises expressive utterances in English, Mandarin, and Cantonese, capturing the intra-utterance code-switching prevalent in multilingual regions like Hong Kong and Macao. The corpus integrates spontaneous emotional expressions from online platforms, annotated with fine-grained emotion distributions across 32 categories. Experimental baselines using self-supervised learning models demonstrate robust performance in speaker-independent gender-, age-, and personality-based evaluations, with HuBERT-large-EN achieving optimal results. By incorporating linguistic diversity and ecological validity, EM2LDL enables the exploration of complex emotional dynamics in multilingual settings. This work provides a versatile testbed for developing adaptive, empathetic systems for applications in affective computing, including mental health monitoring and cross-cultural communication. The dataset, annotations, and baseline codes are publicly available at https://github.com/xingfengli/EM2LDL.",
        "authors": [
            "Xingfeng Li",
            "Xiaohan Shi",
            "Junjie Li",
            "Yongwei Li",
            "Masashi Unoki",
            "Tomoki Toda",
            "Masato Akagi"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T09:26:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20106v1"
    },
    {
        "arxiv_id": "2511.20104v1",
        "title": "The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs",
        "abstract": "Prior work has shown that fine-tuning models on a narrow domain with misaligned data can lead to broad misalignment - a phenomenon termed \"emergent misalignment\" (Betley et al. 2025). While all tested models were susceptible to emergent misalignment, some models showed more resistance than others. Specifically the Qwen-2.5 family proved to be relatively resistant, while GPT-4o exhibited the strongest misalignment. In this paper we evaluate if current-generation open-weights models exhibit similar resistance to the Qwen-2.5 family and measure misalignment robustness over a range of model architectures and scales.   We replicate the effect across nine modern open-weights models (Gemma 3 and Qwen 3 families, 1B-32B parameters). Models fine-tuned on insecure code generation show a 0.68% misalignment rate (compared to 0.07% for base models), matching the lower end of prior open-model results but dramatically lower than GPT-4o's 20%.   We identify a critical format-dependent vulnerability: requiring JSON output doubles misalignment rates compared to natural language prompts (0.96% vs 0.42%). This suggests that structural constraints may bypass safety training by reducing the model's 'degrees of freedom' to refuse. These findings confirm emergent misalignment as a reproducible phenomenon in modern open-weights models, with rates substantially lower than observed in proprietary systems.",
        "authors": [
            "Craig Dickson"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-25T09:25:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20104v1"
    },
    {
        "arxiv_id": "2511.20102v1",
        "title": "SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space",
        "abstract": "The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.",
        "authors": [
            "Zhenyi Shen",
            "Junru Lu",
            "Lin Gui",
            "Jiazheng Li",
            "Yulan He",
            "Di Yin",
            "Xing Sun"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T09:21:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20102v1"
    },
    {
        "arxiv_id": "2511.20100v1",
        "title": "QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation",
        "abstract": "Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.",
        "authors": [
            "Xinguo Zhu",
            "Shaohui Peng",
            "Jiaming Guo",
            "Yunji Chen",
            "Qi Guo",
            "Yuanbo Wen",
            "Hang Qin",
            "Ruizhi Chen",
            "Qirui Zhou",
            "Ke Gao",
            "Yanjun Wu",
            "Chen Zhao",
            "Ling Li"
        ],
        "category": "cs.DC",
        "submitted_date": "2025-11-25T09:17:47+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20100v1"
    },
    {
        "arxiv_id": "2511.20086v1",
        "title": "More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question Answering",
        "abstract": "With the advancement of large language models (LLMs), their performance on multiple-choice question (MCQ) tasks has improved significantly. However, existing approaches face key limitations: answer choices are typically presented to LLMs without contextual grounding or explanation. This absence of context can lead to incomplete exploration of all possible answers, ultimately degrading the models' reasoning capabilities. To address these challenges, we introduce BiasPrompting, a novel inference framework that guides LLMs to generate and critically evaluate reasoning across all plausible answer options before reaching a final prediction. It consists of two components: first, a reasoning generation stage, where the model is prompted to produce supportive reasonings for each answer option, and then, a reasoning-guided agreement stage, where the generated reasonings are synthesized to select the most plausible answer. Through comprehensive evaluations, BiasPrompting demonstrates significant improvements in five widely used multiple-choice question answering benchmarks. Our experiments showcase that BiasPrompting enhances the reasoning capabilities of LLMs and provides a strong foundation for tackling complex and challenging questions, particularly in settings where existing methods underperform.",
        "authors": [
            "Duc Anh Vu",
            "Thong Nguyen",
            "Cong-Duy Nguyen",
            "Viet Anh Nguyen",
            "Anh Tuan Luu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T09:01:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20086v1"
    },
    {
        "arxiv_id": "2511.20072v2",
        "title": "MTA: A Merge-then-Adapt Framework for Personalized Large Language Model",
        "abstract": "Personalized Large Language Models (PLLMs) aim to align model outputs with individual user preferences, a crucial capability for user-centric applications. However, the prevalent approach of fine-tuning a separate module for each user faces two major limitations: (1) storage costs scale linearly with the number of users, rendering the method unscalable; and (2) fine-tuning a static model from scratch often yields suboptimal performance for users with sparse data. To address these challenges, we propose MTA, a Merge-then-Adapt framework for PLLMs. MTA comprises three key stages. First, we construct a shared Meta-LoRA Bank by selecting anchor users and pre-training meta-personalization traits within meta-LoRA modules. Second, to ensure scalability and enable dynamic personalization combination beyond static models, we introduce an Adaptive LoRA Fusion stage. This stage retrieves and dynamically merges the most relevant anchor meta-LoRAs to synthesize a user-specific one, thereby eliminating the need for user-specific storage and supporting more flexible personalization. Third, we propose a LoRA Stacking for Few-Shot Personalization stage, which applies an additional ultra-low-rank, lightweight LoRA module on top of the merged LoRA. Fine-tuning this module enables effective personalization under few-shot settings. Extensive experiments on the LaMP benchmark demonstrate that our approach outperforms existing SOTA methods across multiple tasks.",
        "authors": [
            "Xiaopeng Li",
            "Yuanjin Zheng",
            "Wanyu Wang",
            "wenlin zhang",
            "Pengyue Jia",
            "Yiqi Wang",
            "Maolin Wang",
            "Xuetao Wei",
            "Xiangyu Zhao"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T08:46:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20072v2"
    },
    {
        "arxiv_id": "2511.20056v1",
        "title": "Online-PVLM: Advancing Personalized VLMs with Online Concept Learning",
        "abstract": "Personalized Visual Language Models (VLMs) are gaining increasing attention for their formidable ability in user-specific concepts aligned interactions (e.g., identifying a user's bike). Existing methods typically require the learning of separate embeddings for each new concept, which fails to support real-time adaptation during testing. This limitation becomes particularly pronounced in large-scale scenarios, where efficient retrieval of concept embeddings is not achievable. To alleviate this gap, we propose Online-PVLM, a framework for online concept learning by leveraging hyperbolic representations. Our approach makes a train-free paradigm for concept embeddings generation at test time, making the use of personalized VLMs both scalable and efficient. In addition, we develop OP-Eval, a comprehensive and large-scale benchmark comprising 1,292 concepts and over 30K high-quality instances with diverse question types, designed to rigorously assess online concept learning in realistic scenarios. Extensive experiments demonstrate the state-of-the-art performance of our proposed framework. Our source code and dataset will be made available.",
        "authors": [
            "Huiyu Bai",
            "Runze Wang",
            "Zhuoyun Du",
            "Yiyang Zhao",
            "Fengji Zhang",
            "Haoyu Chen",
            "Xiaoyong Zhu",
            "Bo Zheng",
            "Xuejiao Zhao"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T08:25:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20056v1"
    },
    {
        "arxiv_id": "2511.20001v2",
        "title": "A Machine Learning Approach for Detection of Mental Health Conditions and Cyberbullying from Social Media",
        "abstract": "Mental health challenges and cyberbullying are increasingly prevalent in digital spaces, necessitating scalable and interpretable detection systems. This paper introduces a unified multiclass classification framework for detecting ten distinct mental health and cyberbullying categories from social media data. We curate datasets from Twitter and Reddit, implementing a rigorous \"split-then-balance\" pipeline to train on balanced data while evaluating on a realistic, held-out imbalanced test set. We conducted a comprehensive evaluation comparing traditional lexical models, hybrid approaches, and several end-to-end fine-tuned transformers. Our results demonstrate that end-to-end fine-tuning is critical for performance, with the domain-adapted MentalBERT emerging as the top model, achieving an accuracy of 0.92 and a Macro F1 score of 0.76, surpassing both its generic counterpart and a zero-shot LLM baseline. Grounded in a comprehensive ethical analysis, we frame the system as a human-in-the-loop screening aid, not a diagnostic tool. To support this, we introduce a hybrid SHAPLLM explainability framework and present a prototype dashboard (\"Social Media Screener\") designed to integrate model predictions and their explanations into a practical workflow for moderators. Our work provides a robust baseline, highlighting future needs for multi-label, clinically-validated datasets at the critical intersection of online safety and computational mental health.",
        "authors": [
            "Edward Ajayi",
            "Martha Kachweka",
            "Mawuli Deku",
            "Emily Aiken"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T07:12:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20001v2"
    },
    {
        "arxiv_id": "2511.19997v1",
        "title": "Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test",
        "abstract": "Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a \"reversal curse,\" and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.",
        "authors": [
            "Mihir Sahasrabudhe"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T07:03:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19997v1"
    },
    {
        "arxiv_id": "2511.19987v1",
        "title": "$\\text{R}^2\\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers",
        "abstract": "Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R2R, a domain-aware framework that combines dynamic expert routing with a two-stage training strategy, Entity Abstraction for Generalization (EAG). EAG introduces a counter-shortcut mechanism by masking the most predictive surface cues, forcing the reranker to learn domain-invariant relevance patterns rather than memorizing dataset-specific entities. To efficiently activate domain experts, R2R employs a lightweight Latent Semantic Router that probes internal representations from the frozen backbone decoder to select the optimal LoRA expert per query. Extensive experiments across different reranker backbones and diverse domains (legal, medical, and financial) demonstrate that R2R consistently surpasses generalist and single-domain fine-tuned baselines. Our results confirm that R2R is a model-agnostic and modular approach to domain specialization with strong cross-domain robustness.",
        "authors": [
            "Xinyu Wang",
            "Hanwei Wu",
            "Qingchen Hu",
            "Zhenghan Tai",
            "Jingrui Tian",
            "Lei Ding",
            "Jijun Chi",
            "Hailin He",
            "Tung Sum Thomas Kwok",
            "Yufei Cui",
            "Sicheng Lyu",
            "Muzhi Li",
            "Mingze Li",
            "Xinyue Yu",
            "Ling Zhou",
            "Peng Lu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T06:54:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19987v1"
    },
    {
        "arxiv_id": "2511.19957v2",
        "title": "AppSelectBench: Application-Level Tool Selection Benchmark",
        "abstract": "Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks. For CUAs to operate effectively, application selection, which refers to deciding which application to use before invoking fine-grained tools such as APIs, is a fundamental capability. It determines whether the agent initializes the correct environment, avoids orchestration confusion, and efficiently focuses on relevant context. However, existing benchmarks primarily assess fine-grained API selection, offering limited insight into whether models can reason across and choose between different applications. To fill this gap, we introduce AppSelectBench, a comprehensive benchmark for evaluating application selection in CUAs. AppSelectBench contains a novel user task generation pipeline that produces realistic, diverse, and semantically grounded user intents at scale, together with unified evaluation protocols covering random, heuristic, zero-shot, few-shot, and retrieval-augmented-settings. AppSelectBench covers one hundred widely used desktop applications and includes more than one hundred thousand realistic, diverse, and semantically grounded user tasks. Extensive experiments across both closed-source and open-source large language models reveal systematic strengths and weaknesses in inter-application reasoning, showing that even the most capable models still struggle to make consistent application choices. Together, these results establish AppSelectBench as a foundation for studying and advancing application level reasoning, an essential yet underexplored capability of intelligent CUAs. The source is available at https://microsoft.github.io/appselectbench/.",
        "authors": [
            "Tianyi Chen",
            "Michael Solodko",
            "Sen Wang",
            "Jongwoo Ko",
            "Junheng Hao",
            "Colby Banbury",
            "Sara Abdali",
            "Saeed Amizadeh",
            "Qing Xiao",
            "Yinheng Li",
            "Tianyu Ding",
            "Kamran Ghasedi Dizaji",
            "Suzhen Zheng",
            "Hao Fan",
            "Justin Wagle",
            "Pashmina Cameron",
            "Kazuhito Koishida"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T06:06:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19957v2"
    },
    {
        "arxiv_id": "2511.20718v1",
        "title": "ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training",
        "abstract": "PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.",
        "authors": [
            "Chenliang Li",
            "Adel Elmahdy",
            "Alex Boyd",
            "Zhongruo Wang",
            "Alfredo Garcia",
            "Parminder Bhatia",
            "Taha Kass-Hout",
            "Cao Xiao",
            "Mingyi Hong"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-25T05:54:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20718v1"
    },
    {
        "arxiv_id": "2511.19935v1",
        "title": "EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning",
        "abstract": "The rapid advancement of large language models (LLMs) has increased the demand for domain-specialized variants in areas such as law, healthcare, and finance. However, their large size remains a barrier to deployment in resource-constrained environments, and existing compression methods either generalize poorly across domains or incur high overhead. In this work, we propose \\textbf{EfficientXpert}, a lightweight domain-pruning framework that combines a propagation-aware pruning criterion (Foresight Mask) with an efficient adapter-update algorithm (Partial Brain Surgeon). Integrated into the LoRA fine-tuning process, EfficientXpert enables a one-step transformation of general pretrained models into sparse, domain-adapted experts. Across health and legal tasks, it retains up to 98% of dense-model performance at 40% sparsity, outperforming state-of-the-art methods. Further analysis reveals substantial domain-dependent structural shifts that degrade the effectiveness of general pruning masks, underscoring the need for adaptive, domain-aware pruning strategies tailored to each domain.",
        "authors": [
            "Songlin Zhao",
            "Michael Pitts",
            "Zhuwei Qin"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-25T05:20:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19935v1"
    },
    {
        "arxiv_id": "2511.19923v1",
        "title": "CounterVQA: Evaluating and Improving Counterfactual Reasoning in Vision-Language Models for Video Understanding",
        "abstract": "Vision Language Models (VLMs) have recently shown significant advancements in video understanding, especially in feature alignment, event reasoning, and instruction-following tasks. However, their capability for counterfactual reasoning, inferring alternative outcomes under hypothetical conditions, remains underexplored. This capability is essential for robust video understanding, as it requires identifying underlying causal structures and reasoning about unobserved possibilities, rather than merely recognizing observed patterns. To systematically evaluate this capability, we introduce CounterVQA, a video-based benchmark featuring three progressive difficulty levels that assess different aspects of counterfactual reasoning. Through comprehensive evaluation of both state-of-the-art open-source and closed-source models, we uncover a substantial performance gap: while these models achieve reasonable accuracy on simple counterfactual questions, performance degrades significantly on complex multi-hop causal chains. To address these limitations, we develop a post-training method, CFGPT, that enhances a model's visual counterfactual reasoning ability by distilling its counterfactual reasoning capability from the language modality, yielding consistent improvements across all CounterVQA difficulty levels. Dataset and code will be further released.",
        "authors": [
            "Yuefei Chen",
            "Jiang Liu",
            "Xiaodong Lin",
            "Ruixiang Tang"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-25T04:59:55+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19923v1"
    },
    {
        "arxiv_id": "2511.19878v1",
        "title": "MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization",
        "abstract": "Vision-Language-Action (VLA) models inherit strong priors from pretrained Vision-Language Models (VLMs), but naive fine-tuning often disrupts these representations and harms generalization. Existing fixes -- freezing modules or applying uniform regularization -- either overconstrain adaptation or ignore the differing roles of VLA components. We present MAPS (Module-Wise Proximity Scheduling), the first robust fine-tuning framework for VLAs. Through systematic analysis, we uncover an empirical order in which proximity constraints should be relaxed to balance stability and flexibility. MAPS linearly schedules this relaxation, enabling visual encoders to stay close to their pretrained priors while action-oriented language layers adapt more freely. MAPS introduces no additional parameters or data, and can be seamlessly integrated into existing VLAs. Across MiniVLA-VQ, MiniVLA-OFT, OpenVLA-OFT, and challenging benchmarks such as SimplerEnv, CALVIN, LIBERO, as well as real-world evaluations on the Franka Emika Panda platform, MAPS consistently boosts both in-distribution and out-of-distribution performance (up to +30%). Our findings highlight empirically guided proximity to pretrained VLMs as a simple yet powerful principle for preserving broad generalization in VLM-to-VLA transfer.",
        "authors": [
            "Chengyue Huang",
            "Mellon M. Zhang",
            "Robert Azarcon",
            "Glen Chou",
            "Zsolt Kira"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-25T03:39:37+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19878v1"
    },
    {
        "arxiv_id": "2511.19858v2",
        "title": "A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction",
        "abstract": "Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.   Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.   Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.   Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.",
        "authors": [
            "Farzad Ahmed",
            "Joniel Augustine Jerome",
            "Meliha Yetisgen",
            "\u00d6zlem Uzuner"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T02:40:49+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19858v2"
    },
    {
        "arxiv_id": "2511.19852v1",
        "title": "Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs",
        "abstract": "Personalized Large Language Models (LLMs) have been shown to be an effective way to create more engaging and enjoyable user-AI interactions. While previous studies have explored using prompts to elicit specific personality traits in LLMs, they have not optimized these prompts to maximize personality expression. To address this limitation, we propose PersonaPulse: Dynamic Profile Optimization for Realistic Personality Expression in LLMs, a framework that leverages LLMs' inherent knowledge of personality traits to iteratively enhance role-play prompts while integrating a situational response benchmark as a scoring tool, ensuring a more realistic and contextually grounded evaluation to guide the optimization process. Quantitative evaluations demonstrate that the prompts generated by PersonaPulse outperform those of prior work, which were designed based on personality descriptions from psychological studies. Additionally, we explore the relationship between model size and personality modeling through extensive experiments. Finally, we find that, for certain personality traits, the extent of personality evocation can be partially controlled by pausing the optimization process. These findings underscore the importance of prompt optimization in shaping personality expression within LLMs, offering valuable insights for future research on adaptive AI interactions.",
        "authors": [
            "Shi-Wei Dai",
            "Yan-Wei Shie",
            "Tsung-Huan Yang",
            "Lun-Wei Ku",
            "Yung-Hui Li"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T02:31:40+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19852v1"
    },
    {
        "arxiv_id": "2511.19820v1",
        "title": "CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception",
        "abstract": "Vision-Language Models (VLMs) often struggle with tasks that require fine-grained image understanding, such as scene-text recognition or document analysis, due to perception limitations and visual fragmentation. To address these challenges, we introduce CropVLM as an external low-cost method for boosting performance, enabling VLMs to dynamically ''zoom in'' on relevant image regions, enhancing their ability to capture fine details. CropVLM is trained using reinforcement learning, without using human-labeled bounding boxes as a supervision signal, and without expensive synthetic evaluations. The model is trained once and can be paired with both open-source and proprietary VLMs to improve their performance. Our approach delivers significant improvements on tasks that require high-resolution image understanding, notably for benchmarks that are out-of-domain for the target VLM, without modifying or fine-tuning the VLM, thus avoiding catastrophic forgetting.",
        "authors": [
            "Miguel Carvalho",
            "Helder Dias",
            "Bruno Martins"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-25T01:21:26+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19820v1"
    },
    {
        "arxiv_id": "2511.19818v1",
        "title": "Language-Independent Sentiment Labelling with Distant Supervision: A Case Study for English, Sepedi and Setswana",
        "abstract": "Sentiment analysis is a helpful task to automatically analyse opinions and emotions on various topics in areas such as AI for Social Good, AI in Education or marketing. While many of the sentiment analysis systems are developed for English, many African languages are classified as low-resource languages due to the lack of digital language resources like text labelled with corresponding sentiment classes. One reason for that is that manually labelling text data is time-consuming and expensive. Consequently, automatic and rapid processes are needed to reduce the manual effort as much as possible making the labelling process as efficient as possible. In this paper, we present and analyze an automatic language-independent sentiment labelling method that leverages information from sentiment-bearing emojis and words. Our experiments are conducted with tweets in the languages English, Sepedi and Setswana from SAfriSenti, a multilingual sentiment corpus for South African languages. We show that our sentiment labelling approach is able to label the English tweets with an accuracy of 66%, the Sepedi tweets with 69%, and the Setswana tweets with 63%, so that on average only 34% of the automatically generated labels remain to be corrected.",
        "authors": [
            "Koena Ronny Mabokela",
            "Tim Schlippe",
            "Mpho Raborife",
            "Turgay Celik"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T01:15:54+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19818v1"
    },
    {
        "arxiv_id": "2511.19816v1",
        "title": "Breaking Bad: Norms for Valence, Arousal, and Dominance for over 10k English Multiword Expressions",
        "abstract": "Factor analysis studies have shown that the primary dimensions of word meaning are Valence (V), Arousal (A), and Dominance (D). Existing lexicons such as the NRC VAD Lexicon, published in 2018, include VAD association ratings for words. Here, we present a complement to it, which has human ratings of valence, arousal, and dominance for 10k English Multiword Expressions (MWEs) and their constituent words. We also increase the coverage of unigrams, especially words that have become more common since 2018. In all, the new NRC VAD Lexicon v2 now has entries for 10k MWEs and 25k words, in addition to the entries in v1. We show that the associations are highly reliable. We use the lexicon to examine emotional characteristics of MWEs, including: 1. The degree to which MWEs (idioms, noun compounds, and verb particle constructions) exhibit strong emotionality; 2. The degree of emotional compositionality in MWEs. The lexicon enables a wide variety of research in NLP, Psychology, Public Health, Digital Humanities, and Social Sciences. The NRC VAD Lexicon v2 is freely available through the project webpage: http://saifmohammad.com/WebPages/nrc-vad.html",
        "authors": [
            "Saif M. Mohammad"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-25T01:14:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19816v1"
    },
    {
        "arxiv_id": "2511.19811v1",
        "title": "Training-Free Generation of Diverse and High-Fidelity Images via Prompt Semantic Space Optimization",
        "abstract": "Image diversity remains a fundamental challenge for text-to-image diffusion models. Low-diversity models tend to generate repetitive outputs, increasing sampling redundancy and hindering both creative exploration and downstream applications. A primary cause is that generation often collapses toward a strong mode in the learned distribution. Existing attempts to improve diversity, such as noise resampling, prompt rewriting, or steering-based guidance, often still collapse to dominant modes or introduce distortions that degrade image quality. In light of this, we propose Token-Prompt embedding Space Optimization (TPSO), a training-free and model-agnostic module. TPSO introduces learnable parameters to explore underrepresented regions of the token embedding space, reducing the tendency of the model to repeatedly generate samples from strong modes of the learned distribution. At the same time, the prompt-level space provides a global semantic constraint that regulates distribution shifts, preventing quality degradation while maintaining high fidelity. Extensive experiments on MS-COCO and three diffusion backbones show that TPSO significantly enhances generative diversity, improving baseline performance from 1.10 to 4.18 points, without sacrificing image quality. Code will be released upon acceptance.",
        "authors": [
            "Debin Meng",
            "Chen Jin",
            "Zheng Gao",
            "Yanran Li",
            "Ioannis Patras",
            "Georgios Tzimiropoulos"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-25T00:42:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19811v1"
    },
    {
        "arxiv_id": "2511.19785v1",
        "title": "Gender Bias in Emotion Recognition by Large Language Models",
        "abstract": "The rapid advancement of large language models (LLMs) and their growing integration into daily life underscore the importance of evaluating and ensuring their fairness. In this work, we examine fairness within the domain of emotional theory of mind, investigating whether LLMs exhibit gender biases when presented with a description of a person and their environment and asked, \"How does this person feel?\". Furthermore, we propose and evaluate several debiasing strategies, demonstrating that achieving meaningful reductions in bias requires training based interventions rather than relying solely on inference-time prompt-based approaches such as prompt engineering.",
        "authors": [
            "Maureen Herbert",
            "Katie Sun",
            "Angelica Lim",
            "Yasaman Etesam"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T23:24:54+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19785v1"
    },
    {
        "arxiv_id": "2511.19773v1",
        "title": "Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs",
        "abstract": "While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to \"think with images\", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.",
        "authors": [
            "Meng Lu",
            "Ran Xu",
            "Yi Fang",
            "Wenxuan Zhang",
            "Yue Yu",
            "Gaurav Srivastava",
            "Yuchen Zhuang",
            "Mohamed Elhoseiny",
            "Charles Fleming",
            "Carl Yang",
            "Zhengzhong Tu",
            "Yang Xie",
            "Guanghua Xiao",
            "Hanrui Wang",
            "Di Jin",
            "Wenqi Shi",
            "Xuan Wang"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-24T22:58:26+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19773v1"
    },
    {
        "arxiv_id": "2511.19757v1",
        "title": "What does it mean to understand language?",
        "abstract": "Language understanding entails not just extracting the surface-level meaning of the linguistic input, but constructing rich mental models of the situation it describes. Here we propose that because processing within the brain's core language system is fundamentally limited, deeply understanding language requires exporting information from the language system to other brain regions that compute perceptual and motor representations, construct mental models, and store our world knowledge and autobiographical memories. We review the existing evidence for this hypothesis, and argue that recent progress in cognitive neuroscience provides both the conceptual foundation and the methods to directly test it, thus opening up a new strategy to reveal what it means, cognitively and neurally, to understand language.",
        "authors": [
            "Colton Casto",
            "Anna Ivanova",
            "Evelina Fedorenko",
            "Nancy Kanwisher"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T22:21:55+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19757v1"
    },
    {
        "arxiv_id": "2511.19739v1",
        "title": "Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation",
        "abstract": "Domain-specific text embeddings are critical for clinical natural language processing, yet systematic comparisons across model architectures remain limited. This study evaluates ten transformer-based embedding models adapted for cardiology through Low-Rank Adaptation (LoRA) fine-tuning on 106,535 cardiology text pairs derived from authoritative medical textbooks. Results demonstrate that encoder-only architectures, particularly BioLinkBERT, achieve superior domain-specific performance (separation score: 0.510) compared to larger decoder-based models, while requiring significantly fewer computational resources. The findings challenge the assumption that larger language models necessarily produce better domain-specific embeddings and provide practical guidance for clinical NLP system development. All models, training code, and evaluation datasets are publicly available to support reproducible research in medical informatics.",
        "authors": [
            "Richard J. Young",
            "Alice M. Matthews"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T21:57:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19739v1"
    },
    {
        "arxiv_id": "2511.19719v1",
        "title": "Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian",
        "abstract": "Large language models (LLMs) are increasingly used to generate self-explanations alongside their predictions, a practice that raises concerns about the faithfulness of these explanations, especially in low-resource languages. This study evaluates the faithfulness of LLM-generated explanations in the context of emotion classification in Persian, a low-resource language, by comparing the influential words identified by the model against those identified by human annotators. We assess faithfulness using confidence scores derived from token-level log-probabilities. Two prompting strategies, differing in the order of explanation and prediction (Predict-then-Explain and Explain-then-Predict), are tested for their impact on explanation faithfulness. Our results reveal that while LLMs achieve strong classification performance, their generated explanations often diverge from faithful reasoning, showing greater agreement with each other than with human judgments. These results highlight the limitations of current explanation methods and metrics, emphasizing the need for more robust approaches to ensure LLM reliability in multilingual and low-resource contexts.",
        "authors": [
            "Mobina Mehrazar",
            "Mohammad Amin Yousefi",
            "Parisa Abolfath Beygi",
            "Behnam Bahrak"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T21:29:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19719v1"
    },
    {
        "arxiv_id": "2511.21762v1",
        "title": "Factors That Support Grounded Responses in LLM Conversations: A Rapid Review",
        "abstract": "Large language models (LLMs) may generate outputs that are misaligned with user intent, lack contextual grounding, or exhibit hallucinations during conversation, which compromises the reliability of LLM-based applications. This review aimed to identify and analyze techniques that align LLM responses with conversational goals, ensure grounding, and reduce hallucination and topic drift. We conducted a Rapid Review guided by the PRISMA framework and the PICO strategy to structure the search, filtering, and selection processes. The alignment strategies identified were categorized according to the LLM lifecycle phase in which they operate: inference-time, post-training, and reinforcement learning-based methods. Among these, inference-time approaches emerged as particularly efficient, aligning outputs without retraining while supporting user intent, contextual grounding, and hallucination mitigation. The reviewed techniques provided structured mechanisms for improving the quality and reliability of LLM responses across key alignment objectives.",
        "authors": [
            "Gabriele Cesar Iwashima",
            "Claudia Susie Rodrigues",
            "Claudio Dipolitto",
            "Geraldo Xex\u00e9o"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T21:18:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21762v1"
    },
    {
        "arxiv_id": "2511.21761v1",
        "title": "LLMs for Low-Resource Dialect Translation Using Context-Aware Prompting: A Case Study on Sylheti",
        "abstract": "Large Language Models (LLMs) have demonstrated strong translation abilities through prompting, even without task-specific training. However, their effectiveness in dialectal and low-resource contexts remains underexplored. This study presents the first systematic investigation of LLM-based machine translation (MT) for Sylheti, a dialect of Bangla that is itself low-resource. We evaluate five advanced LLMs (GPT-4.1, GPT-4.1, LLaMA 4, Grok 3, and DeepSeek V3.2) across both translation directions (Bangla $\\Leftrightarrow$ Sylheti), and find that these models struggle with dialect-specific vocabulary. To address this, we introduce Sylheti-CAP (Context-Aware Prompting), a three-step framework that embeds a linguistic rulebook, a dictionary (2{,}260 core vocabulary items and idioms), and an authenticity check directly into prompts. Extensive experiments show that Sylheti-CAP consistently improves translation quality across models and prompting strategies. Both automatic metrics and human evaluations confirm its effectiveness, while qualitative analysis reveals notable reductions in hallucinations, ambiguities, and awkward phrasing, establishing Sylheti-CAP as a scalable solution for dialectal and low-resource MT. Dataset link: \\href{https://github.com/TabiaTanzin/LLMs-for-Low-Resource-Dialect-Translation-Using-Context-Aware-Prompting-A-Case-Study-on-Sylheti.git}{https://github.com/TabiaTanzin/LLMs-for-Low-Resource-Dialect-Translation-Using-Context-Aware-Prompting-A-Case-Study-on-Sylheti.git}",
        "authors": [
            "Tabia Tanzin Prama",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T20:34:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21761v1"
    },
    {
        "arxiv_id": "2511.21760v1",
        "title": "fMRI-LM: Towards a Universal Foundation Model for Language-Aligned fMRI Understanding",
        "abstract": "Recent advances in multimodal large language models (LLMs) have enabled unified reasoning across images, audio, and video, but extending such capability to brain imaging remains largely unexplored. Bridging this gap is essential to link neural activity with semantic cognition and to develop cross-modal brain representations. To this end, we present fMRI-LM, a foundational model that bridges functional MRI (fMRI) and language through a three-stage framework. In Stage 1, we learn a neural tokenizer that maps fMRI into discrete tokens embedded in a language-consistent space. In Stage 2, a pretrained LLM is adapted to jointly model fMRI tokens and text, treating brain activity as a sequence that can be temporally predicted and linguistically described. To overcome the lack of natural fMRI-text pairs, we construct a large descriptive corpus that translates diverse imaging-based features into structured textual descriptors, capturing the low-level organization of fMRI signals. In Stage 3, we perform multi-task, multi-paradigm instruction tuning to endow fMRI-LM with high-level semantic understanding, supporting diverse downstream applications. Across various benchmarks, fMRI-LM achieves strong zero-shot and few-shot performance, and adapts efficiently with parameter-efficient tuning (LoRA), establishing a scalable pathway toward a language-aligned, universal model for structural and semantic understanding of fMRI.",
        "authors": [
            "Yuxiang Wei",
            "Yanteng Zhang",
            "Xi Xiao",
            "Chengxuan Qian",
            "Tianyang Wang",
            "Vince D. Calhoun"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T20:26:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21760v1"
    },
    {
        "arxiv_id": "2511.19663v1",
        "title": "Fara-7B: An Efficient Agentic Model for Computer Use",
        "abstract": "Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.",
        "authors": [
            "Ahmed Awadallah",
            "Yash Lara",
            "Raghav Magazine",
            "Hussein Mozannar",
            "Akshay Nambi",
            "Yash Pandya",
            "Aravind Rajeswaran",
            "Corby Rosset",
            "Alexey Taymanov",
            "Vibhav Vineet",
            "Spencer Whitehead",
            "Andrew Zhao"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-24T19:56:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19663v1"
    },
    {
        "arxiv_id": "2511.19648v1",
        "title": "Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search",
        "abstract": "Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.",
        "authors": [
            "Manil Shrestha",
            "Edward Kim"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T19:27:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19648v1"
    },
    {
        "arxiv_id": "2511.19417v1",
        "title": "Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.",
        "authors": [
            "James Y. Huang",
            "Sheng Zhang",
            "Qianchu Liu",
            "Guanghui Qin",
            "Tinghui Zhu",
            "Tristan Naumann",
            "Muhao Chen",
            "Hoifung Poon"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T18:55:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19417v1"
    },
    {
        "arxiv_id": "2511.19399v2",
        "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
        "abstract": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.",
        "authors": [
            "Rulin Shao",
            "Akari Asai",
            "Shannon Zejiang Shen",
            "Hamish Ivison",
            "Varsha Kishore",
            "Jingming Zhuo",
            "Xinran Zhao",
            "Molly Park",
            "Samuel G. Finlayson",
            "David Sontag",
            "Tyler Murray",
            "Sewon Min",
            "Pradeep Dasigi",
            "Luca Soldaini",
            "Faeze Brahman",
            "Wen-tau Yih",
            "Tongshuang Wu",
            "Luke Zettlemoyer",
            "Yoon Kim",
            "Hannaneh Hajishirzi",
            "Pang Wei Koh"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T18:35:54+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19399v2"
    },
    {
        "arxiv_id": "2511.19350v2",
        "title": "Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric",
        "abstract": "Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at https://anonymous.4open.science/r/towards_clustering-0C2E.",
        "authors": [
            "Nikita Neveditsin",
            "Pawan Lingras",
            "Vijay Mago"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-24T17:52:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19350v2"
    },
    {
        "arxiv_id": "2511.19333v1",
        "title": "Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces",
        "abstract": "Test-time scaling, which leverages additional computation during inference to improve model accuracy, has enabled a new class of Large Language Models (LLMs) that are able to reason through complex problems by understanding the goal, turning this goal into a plan, working through intermediate steps, and checking their own work before answering . Frontier large language models with reasoning capabilities, such as DeepSeek-R1 and OpenAI's gpt-oss, follow the same procedure when solving complex problems by generating intermediate reasoning traces before giving the final answer. Today, these models are being increasingly used to generate reasoning traces that serve as high-quality supervised data for post-training of small and medium-sized language models to teach reasoning capabilities without requiring expensive human curation. In this work, we compare the performance of medium-sized LLMs on Math problems after post-training on two kinds of reasoning traces. We compare the impact of reasoning traces generated by DeepSeek-R1 and gpt-oss LLMs in terms of accuracy and inference efficiency.",
        "authors": [
            "Shaltiel Shmidman",
            "Asher Fredman",
            "Oleg Sudakov",
            "Meriem Bendris"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T17:26:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19333v1"
    },
    {
        "arxiv_id": "2511.19325v1",
        "title": "Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval",
        "abstract": "Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.",
        "authors": [
            "Olivia Macmillan-Scott",
            "Roksana Goworek",
            "Eda B. \u00d6zyi\u011fit"
        ],
        "category": "cs.IR",
        "submitted_date": "2025-11-24T17:18:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19325v1"
    },
    {
        "arxiv_id": "2511.19324v1",
        "title": "What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models",
        "abstract": "Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.",
        "authors": [
            "Roksana Goworek",
            "Olivia Macmillan-Scott",
            "Eda B. \u00d6zyi\u011fit"
        ],
        "category": "cs.IR",
        "submitted_date": "2025-11-24T17:17:40+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19324v1"
    },
    {
        "arxiv_id": "2511.19317v1",
        "title": "MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset",
        "abstract": "This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.",
        "authors": [
            "Md. Tanzim Ferdous",
            "Naeem Ahsan Chowdhury",
            "Prithwiraj Bhattacharjee"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T17:11:49+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19317v1"
    },
    {
        "arxiv_id": "2511.19314v1",
        "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking",
        "abstract": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.",
        "authors": [
            "Jaewoo Lee",
            "Archiki Prasad",
            "Justin Chih-Yao Chen",
            "Zaid Khan",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-24T17:09:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19314v1"
    },
    {
        "arxiv_id": "2511.19304v1",
        "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning",
        "abstract": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.",
        "authors": [
            "Jiayi Zhang",
            "Yiran Peng",
            "Fanqi Kong",
            "Yang Cheng",
            "Yifan Wu",
            "Zhaoyang Yu",
            "Jinyu Xiang",
            "Jianhao Ruan",
            "Jinlin Wang",
            "Maojia Song",
            "HongZhang Liu",
            "Xiangru Tang",
            "Bang Liu",
            "Chenglin Wu",
            "Yuyu Luo"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-24T16:54:23+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19304v1"
    },
    {
        "arxiv_id": "2511.19279v1",
        "title": "MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings",
        "abstract": "A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.",
        "authors": [
            "Victor Rambaud",
            "Salvador Mascarenhas",
            "Yair Lakretz"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-24T16:29:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19279v1"
    },
    {
        "arxiv_id": "2511.19269v1",
        "title": "CDLM: Consistency Diffusion Language Models For Faster Sampling",
        "abstract": "Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.",
        "authors": [
            "Minseo Kim",
            "Chenfeng Xu",
            "Coleman Hooper",
            "Harman Singh",
            "Ben Athiwaratkun",
            "Ce Zhang",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-24T16:21:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19269v1"
    },
    {
        "arxiv_id": "2511.19260v1",
        "title": "A Nutrition Multimodal Photoplethysmography Language Model",
        "abstract": "Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale.",
        "authors": [
            "Kyle Verrier",
            "Achille Nazaret",
            "Joseph Futoma",
            "Andrew C. Miller",
            "Guillermo Sapiro"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-24T16:12:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19260v1"
    },
    {
        "arxiv_id": "2511.19232v1",
        "title": "In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations",
        "abstract": "How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.",
        "authors": [
            "Christos-Nikolaos Zacharopoulos",
            "Revekka Kyriakoglou"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T15:43:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19232v1"
    },
    {
        "arxiv_id": "2512.02038v1",
        "title": "Deep Research: A Systematic Survey",
        "abstract": "Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.",
        "authors": [
            "Zhengliang Shi",
            "Yiqun Chen",
            "Haitao Li",
            "Weiwei Sun",
            "Shiyu Ni",
            "Yougang Lyu",
            "Run-Ze Fan",
            "Bowen Jin",
            "Yixuan Weng",
            "Minjun Zhu",
            "Qiujie Xie",
            "Xinyu Guo",
            "Qu Yang",
            "Jiayi Wu",
            "Jujia Zhao",
            "Xiaqiang Tang",
            "Xinbei Ma",
            "Cunxiang Wang",
            "Jiaxin Mao",
            "Qingyao Ai",
            "Jen-Tse Huang",
            "Wenxuan Wang",
            "Yue Zhang",
            "Yiming Yang",
            "Zhaopeng Tu",
            "Zhaochun Ren"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T15:28:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02038v1"
    },
    {
        "arxiv_id": "2511.19168v1",
        "title": "RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning",
        "abstract": "Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability.",
        "authors": [
            "Deyi Ji",
            "Yuekui Yang",
            "Liqun Liu",
            "Peng Shu",
            "Haiyang Wu",
            "Shaogang Tang",
            "Xudong Chen",
            "Shaoping Ma",
            "Tianrun Chen",
            "Lanyun Zhu"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-24T14:32:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19168v1"
    },
    {
        "arxiv_id": "2511.19166v1",
        "title": "Representational Stability of Truth in Large Language Models",
        "abstract": "Large language models (LLMs) are widely used for factual tasks such as \"What treats asthma?\" or \"What is the capital of Latvia?\". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to $40\\%$ flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes ($\\leq 8.2\\%$). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.",
        "authors": [
            "Samantha Dies",
            "Courtney Maynard",
            "Germans Savcisens",
            "Tina Eliassi-Rad"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T14:28:50+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19166v1"
    },
    {
        "arxiv_id": "2511.19149v1",
        "title": "From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation",
        "abstract": "This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.",
        "authors": [
            "Moazzam Umer Gondal",
            "Hamad Ul Qudous",
            "Daniya Siddiqui",
            "Asma Ahmad Farhan"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-24T14:13:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19149v1"
    },
    {
        "arxiv_id": "2511.19131v1",
        "title": "Eliciting Chain-of-Thought in Base LLMs via Gradient-Based Representation Optimization",
        "abstract": "Chain-of-Thought (CoT) reasoning is a critical capability for large language models (LLMs), enabling them to tackle com- plex multi-step tasks. While base LLMs, pre-trained on general text corpora, often struggle with reasoning due to a lack of specialized training, recent studies reveal their latent reason- ing potential tied to hidden states. However, existing hidden state manipulation methods, such as linear activation steering, suffer from limitations due to their rigid and unconstrained nature, often leading to distribution shifts and degraded text quality. In this work, we propose a novel approach for elic- iting CoT reasoning from base LLMs through hidden state manipulation grounded in probabilistic conditional generation. By reformulating the challenge as an optimization problem with a balanced likelihood and prior regularization framework, our method guides hidden states toward reasoning-oriented trajectories while preserving linguistic coherence. Extensive evaluations across mathematical, commonsense, and logical reasoning benchmarks demonstrate that our approach con- sistently outperforms existing steering methods, offering a theoretically principled and effective solution for enhancing reasoning capabilities in base LLMs.",
        "authors": [
            "Zijian Wang",
            "Yanxiang Ma",
            "Chang Xu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T13:55:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19131v1"
    },
    {
        "arxiv_id": "2511.19122v1",
        "title": "Emotion-Enhanced Multi-Task Learning with LLMs for Aspect Category Sentiment Analysis",
        "abstract": "Aspect category sentiment analysis (ACSA) has achieved remarkable progress with large language models (LLMs), yet existing approaches primarily emphasize sentiment polarity while overlooking the underlying emotional dimensions that shape sentiment expressions. This limitation hinders the model's ability to capture fine-grained affective signals toward specific aspect categories. To address this limitation, we introduce a novel emotion-enhanced multi-task ACSA framework that jointly learns sentiment polarity and category-specific emotions grounded in Ekman's six basic emotions. Leveraging the generative capabilities of LLMs, our approach enables the model to produce emotional descriptions for each aspect category, thereby enriching sentiment representations with affective expressions. Furthermore, to ensure the accuracy and consistency of the generated emotions, we introduce an emotion refinement mechanism based on the Valence-Arousal-Dominance (VAD) dimensional framework. Specifically, emotions predicted by the LLM are projected onto a VAD space, and those inconsistent with their corresponding VAD coordinates are re-annotated using a structured LLM-based refinement strategy. Experimental results demonstrate that our approach significantly outperforms strong baselines on all benchmark datasets. This underlines the effectiveness of integrating affective dimensions into ACSA.",
        "authors": [
            "Yaping Chai",
            "Haoran Xie",
            "Joe S. Qin"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T13:52:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19122v1"
    },
    {
        "arxiv_id": "2511.19120v1",
        "title": "On the Optimality of Discrete Object Naming: a Kinship Case Study",
        "abstract": "The structure of naming systems in natural languages hinges on a trade-off between high informativeness and low complexity. Prior work capitalizes on information theory to formalize these notions; however, these studies generally rely on two simplifications: (i) optimal listeners, and (ii) universal communicative need across languages. Here, we address these limitations by introducing an information-theoretic framework for discrete object naming systems, and we use it to prove that an optimal trade-off is achievable if and only if the listener's decoder is equivalent to the Bayesian decoder of the speaker. Adopting a referential game setup from emergent communication, and focusing on the semantic domain of kinship, we show that our notion of optimality is not only theoretically achievable but also emerges empirically in learned communication systems.",
        "authors": [
            "Phong Le",
            "Mees Lindeman",
            "Raquel G. Alhama"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T13:49:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19120v1"
    },
    {
        "arxiv_id": "2511.19118v1",
        "title": "A symbolic Perl algorithm for the unification of Nahuatl word spellings",
        "abstract": "In this paper, we describe a symbolic model for the automatic orthographic unification of Nawatl text documents. Our model is based on algorithms that we have previously used to analyze sentences in Nawatl, and on the corpus called $\u03c0$-yalli, consisting of texts in several Nawatl orthographies. Our automatic unification algorithm implements linguistic rules in symbolic regular expressions. We also present a manual evaluation protocol that we have proposed and implemented to assess the quality of the unified sentences generated by our algorithm, by testing in a sentence semantic task. We have obtained encouraging results from the evaluators for most of the desired features of our artificially unified sentences",
        "authors": [
            "Juan-Jos\u00e9 Guzm\u00e1n-Landa",
            "Jes\u00fas V\u00e1zquez-Osorio",
            "Juan-Manuel Torres-Moreno",
            "Ligia Quintana Torres",
            "Miguel Figueroa-Saavedra",
            "Martha-Lorena Avenda\u00f1o-Garrido",
            "Graham Ranger",
            "Patricia Vel\u00e1zquez-Morales",
            "Gerardo Eugenio Sierra Mart\u00ednez"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T13:49:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19118v1"
    },
    {
        "arxiv_id": "2511.21759v1",
        "title": "Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models",
        "abstract": "Diffusion-based large language models (dLLMs) have recently gained significant attention for their exceptional performance and inherent potential for parallel decoding. Existing frameworks further enhance its inference efficiency by enabling KV caching. However, its bidirectional attention mechanism necessitates periodic cache refreshes that interleave prefill and decoding phases, both contributing substantial inference cost and constraining achievable speedup. Inspired by the heterogeneous arithmetic intensity of the prefill and decoding phases, we propose ODB-dLLM, a framework that orchestrates dual-boundaries to accelerate dLLM inference. In the prefill phase, we find that the predefined fixed response length introduces heavy yet redundant computational overhead, which affects efficiency. To alleviate this, ODB-dLLM incorporates an adaptive length prediction mechanism that progressively reduces prefill overhead and unnecessary computation. In the decoding phase, we analyze the computational characteristics of dLLMs and propose a dLLM-specific jump-share speculative decoding method to enhance efficiency by reducing the number of decoding iterations. Experimental results demonstrate that ODB-dLLM achieves 46-162x and 2.63-6.30x speedups over the baseline dLLM and Fast-dLLM, respectively, while simultaneously mitigating the accuracy degradation in existing acceleration frameworks.",
        "authors": [
            "Linye Wei",
            "Wenjue Chen",
            "Pingzhi Tang",
            "Xiaotian Guo",
            "Le Ye",
            "Runsheng Wang",
            "Meng Li"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T13:36:54+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21759v1"
    },
    {
        "arxiv_id": "2511.19083v1",
        "title": "A Multi-Agent LLM Framework for Multi-Domain Low-Resource In-Context NER via Knowledge Retrieval, Disambiguation and Reflective Analysis",
        "abstract": "In-context learning (ICL) with large language models (LLMs) has emerged as a promising paradigm for named entity recognition (NER) in low-resource scenarios. However, existing ICL-based NER methods suffer from three key limitations: (1) reliance on dynamic retrieval of annotated examples, which is problematic when annotated data is scarce; (2) limited generalization to unseen domains due to the LLM's insufficient internal domain knowledge; and (3) failure to incorporate external knowledge or resolve entity ambiguities. To address these challenges, we propose KDR-Agent, a novel multi-agent framework for multi-domain low-resource in-context NER that integrates Knowledge retrieval, Disambiguation, and Reflective analysis. KDR-Agent leverages natural-language type definitions and a static set of entity-level contrastive demonstrations to reduce dependency on large annotated corpora. A central planner coordinates specialized agents to (i) retrieve factual knowledge from Wikipedia for domain-specific mentions, (ii) resolve ambiguous entities via contextualized reasoning, and (iii) reflect on and correct model predictions through structured self-assessment. Experiments across ten datasets from five domains demonstrate that KDR-Agent significantly outperforms existing zero-shot and few-shot ICL baselines across multiple LLM backbones. The code and data can be found at https://github.com/MWXGOD/KDR-Agent.",
        "authors": [
            "Wenxuan Mu",
            "Jinzhong Ning",
            "Di Zhao",
            "Yijia Zhang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T13:23:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19083v1"
    },
    {
        "arxiv_id": "2511.19078v1",
        "title": "GraphMind: Theorem Selection and Conclusion Generation Framework with Dynamic GNN for LLM Reasoning",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, including multi-step reasoning such as mathematical proving. However, existing approaches often lack an explicit and dynamic mechanism to structurally represent and evolve intermediate reasoning states, which limits their ability to perform context-aware theorem selection and iterative conclusion generation. To address these challenges, we propose GraphMind, a novel dynamic graph-based framework that integrates the graph neural network (GNN) with LLMs to iteratively select theorems and generate intermediate conclusions for multi-step reasoning. Our method models the reasoning process as a heterogeneous evolving graph, where nodes represent conditions, theorems, and conclusions, while edges capture logical dependencies between nodes. By encoding the current reasoning state with GNN and leveraging semantic matching for theorem selection, our framework enables context-aware, interpretable, and structured reasoning in a closed-loop manner. Experiments on various question-answering (QA) datasets demonstrate that our proposed GraphMind method achieves consistent performance improvements and significantly outperforms existing baselines in multi-step reasoning, validating the effectiveness and generalizability of our approach.",
        "authors": [
            "Yutong Li",
            "Yitian Zhou",
            "Xudong Wang",
            "GuoChen",
            "Caiyan Qin"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T13:18:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19078v1"
    },
    {
        "arxiv_id": "2511.19063v1",
        "title": "Logic of Montage",
        "abstract": "In expressing emotions, as an expression form separate from natural language, we propose an alternative form that complements natural language, acting as a proxy or window for emotional states. First, we set up an expression form \"Effect of Contradictory Structure.\" \"Effect of Contradictory Structure\" is not static but dynamic. Effect in \"Effect of Contradictory Structure\" is unpleasant or pleasant, and the orientation to avoid that unpleasantness is considered pseudo-expression of will. Second, \"Effect of Contradictory Structure\" can be overlapped with each other. This overlapping operation is called \"montage.\" A broader \"Structure\" that includes related \"Effect of Contradictory Structure\" and \"Effect of Structure\" are set up. Montage produces \"Effect of Structure\". In montage, it is necessary to set something like \"strength,\" so we adopted Deleuze and Deleuze/Guattari's word \"intensity\" and set it as an element of our model. We set up a general theoretical framework - Word Import Between Systems (Models) and justified the import of \"intensity\" through Austin's use of the word \"force.\" \"Effect of Structure\" process is demonstrated using the example of proceeding to the next level of education.",
        "authors": [
            "Hayami Takahashi",
            "Kensuke Takahashi"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T12:55:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19063v1"
    },
    {
        "arxiv_id": "2511.21757v1",
        "title": "Medical Malice: A Dataset for Context-Aware Safety in Healthcare LLMs",
        "abstract": "The integration of Large Language Models (LLMs) into healthcare demands a safety paradigm rooted in \\textit{primum non nocere}. However, current alignment techniques rely on generic definitions of harm that fail to capture context-dependent violations, such as administrative fraud and clinical discrimination. To address this, we introduce Medical Malice: a dataset of 214,219 adversarial prompts calibrated to the regulatory and ethical complexities of the Brazilian Unified Health System (SUS). Crucially, the dataset includes the reasoning behind each violation, enabling models to internalize ethical boundaries rather than merely memorizing a fixed set of refusals. Using an unaligned agent (Grok-4) within a persona-driven pipeline, we synthesized high-fidelity threats across seven taxonomies, ranging from procurement manipulation and queue-jumping to obstetric violence. We discuss the ethical design of releasing these \"vulnerability signatures\" to correct the information asymmetry between malicious actors and AI developers. Ultimately, this work advocates for a shift from universal to context-aware safety, providing the necessary resources to immunize healthcare AI against the nuanced, systemic threats inherent to high-stakes medical environments -- vulnerabilities that represent the paramount risk to patient safety and the successful integration of AI in healthcare systems.",
        "authors": [
            "Andrew Maranh\u00e3o Ventura D'addario"
        ],
        "category": "cs.CY",
        "submitted_date": "2025-11-24T11:55:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21757v1"
    },
    {
        "arxiv_id": "2511.21756v1",
        "title": "Dissecting the Ledger: Locating and Suppressing \"Liar Circuits\" in Financial Large Language Models",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in high-stakes financial domains, yet they suffer from specific, reproducible hallucinations when performing arithmetic operations. Current mitigation strategies often treat the model as a black box. In this work, we propose a mechanistic approach to intrinsic hallucination detection. By applying Causal Tracing to the GPT-2 XL architecture on the ConvFinQA benchmark, we identify a dual-stage mechanism for arithmetic reasoning: a distributed computational scratchpad in middle layers (L12-L30) and a decisive aggregation circuit in late layers (specifically Layer 46). We verify this mechanism via an ablation study, demonstrating that suppressing Layer 46 reduces the model's confidence in hallucinatory outputs by 81.8%. Furthermore, we demonstrate that a linear probe trained on this layer generalizes to unseen financial topics with 98% accuracy, suggesting a universal geometry of arithmetic deception.",
        "authors": [
            "Soham Mirajkar"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T11:49:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21756v1"
    },
    {
        "arxiv_id": "2511.19009v1",
        "title": "Understanding and Mitigating Over-refusal for Large Language Models via Safety Representation",
        "abstract": "Large language models demonstrate powerful capabilities across various natural language processing tasks, yet they also harbor safety vulnerabilities. To enhance LLM safety, various jailbreak defense methods have been proposed to guard against harmful outputs. However, improvements in model safety often come at the cost of severe over-refusal, failing to strike a good balance between safety and usability. In this paper, we first analyze the causes of over-refusal from a representation perspective, revealing that over-refusal samples reside at the boundary between benign and malicious samples. Based on this, we propose MOSR, designed to mitigate over-refusal by intervening the safety representation of LLMs. MOSR incorporates two novel components: (1) Overlap-Aware Loss Weighting, which determines the erasure weight for malicious samples by quantifying their similarity to pseudo-malicious samples in the representation space, and (2) Context-Aware Augmentation, which supplements the necessary context for rejection decisions by adding harmful prefixes before rejection responses. Experiments demonstrate that our method outperforms existing approaches in mitigating over-refusal while largely maintaining safety. Overall, we advocate that future defense methods should strike a better balance between safety and over-refusal.",
        "authors": [
            "Junbo Zhang",
            "Ran Chen",
            "Qianli Zhou",
            "Xinyang Deng",
            "Wen Jiang"
        ],
        "category": "cs.CR",
        "submitted_date": "2025-11-24T11:38:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19009v1"
    },
    {
        "arxiv_id": "2511.18992v1",
        "title": "Classification EM-PCA for clustering and embedding",
        "abstract": "The mixture model is undoubtedly one of the greatest contributions to clustering. For continuous data, Gaussian models are often used and the Expectation-Maximization (EM) algorithm is particularly suitable for estimating parameters from which clustering is inferred. If these models are particularly popular in various domains including image clustering, they however suffer from the dimensionality and also from the slowness of convergence of the EM algorithm. However, the Classification EM (CEM) algorithm, a classifying version, offers a fast convergence solution while dimensionality reduction still remains a challenge. Thus we propose in this paper an algorithm combining simultaneously and non-sequentially the two tasks --Data embedding and Clustering-- relying on Principal Component Analysis (PCA) and CEM. We demonstrate the interest of such approach in terms of clustering and data embedding. We also establish different connections with other clustering approaches.",
        "authors": [
            "Zineddine Tighidet",
            "Lazhar Labiod",
            "Mohamed Nadif"
        ],
        "category": "stat.ML",
        "submitted_date": "2025-11-24T11:18:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18992v1"
    },
    {
        "arxiv_id": "2511.19538v1",
        "title": "Studying Maps at Scale: A Digital Investigation of Cartography and the Evolution of Figuration",
        "abstract": "This thesis presents methods and datasets to investigate cartographic heritage on a large scale and from a cultural perspective. Heritage institutions worldwide have digitized more than one million maps, and automated techniques now enable large-scale recognition and extraction of map content. Yet these methods have engaged little with the history of cartography, or the view that maps are semantic-symbolic systems, and cultural objects reflecting political and epistemic expectations. This work leverages a diverse corpus of 771,561 map records and 99,715 digitized images aggregated from 38 digital catalogs. After normalization, the dataset includes 236,925 contributors and spans six centuries, from 1492 to 1948. These data make it possible to chart geographic structures and the global chronology of map publication. The spatial focus of cartography is analyzed in relation to political dynamics, evidencing links between Atlantic maritime charting, the triangular trade, and colonial expansion. Further results document the progression of national, domestic focus and the impact of military conflicts on publication volumes. The research introduces semantic segmentation techniques and object detection models for the generic recognition of land classes and cartographic signs, trained on annotated data and synthetic images. The analysis of land classes shows that maps are designed images whose framing and composition emphasize features through centering and semantic symmetries. The study of cartographic figuration encodes 63 M signs and 25 M fragments into a latent visual space, revealing figurative shifts such as the replacement of relief hachures by terrain contours and showing that signs tend to form locally consistent systems. Analyses of collaboration and diffusion highlight the role of legitimacy, larger actors, and major cities in the spread of figurative norms and semiotic cultures.",
        "authors": [
            "Remi Petitpierre"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-24T10:35:37+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19538v1"
    },
    {
        "arxiv_id": "2511.18937v1",
        "title": "Knowledge-based Graphical Method for Safety Signal Detection in Clinical Trials",
        "abstract": "We present a graphical, knowledge-based method for reviewing treatment-emergent adverse events (AEs) in clinical trials. The approach enhances MedDRA by adding a hidden medical knowledge layer (Safeterm) that captures semantic relationships between terms in a 2-D map. Using this layer, AE Preferred Terms can be regrouped automatically into similarity clusters, and their association to the trial disease may be quantified. The Safeterm map is available online and connected to aggregated AE incidence tables from ClinicalTrials.gov. For signal detection, we compute treatment-specific disproportionality metrics using shrinkage incidence ratios. Cluster-level EBGM values are then derived through precision-weighted aggregation. Two visual outputs support interpretation: a semantic map showing AE incidence and an expectedness-versus-disproportionality plot for rapid signal detection. Applied to three legacy trials, the automated method clearly recovers all expected safety signals. Overall, augmenting MedDRA with a medical knowledge layer improves clarity, efficiency, and accuracy in AE interpretation for clinical trials.",
        "authors": [
            "Francois Vandenhende",
            "Anna Georgiou",
            "Michalis Georgiou",
            "Theodoros Psaras",
            "Ellie Karekla",
            "Elena Hadjicosta"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T09:42:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18937v1"
    },
    {
        "arxiv_id": "2511.18936v1",
        "title": "SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression",
        "abstract": "Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.",
        "authors": [
            "Santhosh G S",
            "Saurav Prakash",
            "Balaraman Ravindran"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-24T09:41:24+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18936v1"
    },
    {
        "arxiv_id": "2511.18934v1",
        "title": "Skeletons Matter: Dynamic Data Augmentation for Text-to-Query",
        "abstract": "The task of translating natural language questions into query languages has long been a central focus in semantic parsing. Recent advancements in Large Language Models (LLMs) have significantly accelerated progress in this field. However, existing studies typically focus on a single query language, resulting in methods with limited generalizability across different languages. In this paper, we formally define the Text-to-Query task paradigm, unifying semantic parsing tasks across various query languages. We identify query skeletons as a shared optimization target of Text-to-Query tasks, and propose a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons to synthesize targeted training data. Experiments on four Text-to-Query benchmarks demonstrate that our method achieves state-of-the-art performance using only a small amount of synthesized data, highlighting the efficiency and generality of our approach and laying a solid foundation for unified research on Text-to-Query tasks. We release our code at https://github.com/jjjycaptain/Skeletron.",
        "authors": [
            "Yuchen Ji",
            "Bo Xu",
            "Jie Shi",
            "Jiaqing Liang",
            "Deqing Yang",
            "Yu Mao",
            "Hai Chen",
            "Yanghua Xiao"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T09:39:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18934v1"
    },
    {
        "arxiv_id": "2511.18931v1",
        "title": "Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs",
        "abstract": "Modern large language models integrate web search to provide real-time answers, yet it remains unclear whether they are efficiently calibrated to use search when it is actually needed. We introduce a benchmark evaluating both the necessity and effectiveness of web access across commercial models with no access to internal states or parameters. The dataset includes a static split of 783 temporally anchored questions answerable from pre-cutoff knowledge, aimed at testing whether models invoke search based on low internal confidence, and a dynamic split of 288 post-cutoff queries designed to test whether models recognise when search is required and retrieve updated information. Web access substantially improves static accuracy for GPT-5-mini and Claude Haiku 4.5, though confidence calibration worsens. On dynamic queries, both models frequently invoke search yet remain below 70 percent accuracy due to weak query formulation. Costs per accuracy-improving call remain low, but returns diminish once initial retrieval fails. Selective invocation helps, but models become overconfident and inconsistent after search. Overall, built-in web search meaningfully improves factual accuracy and can be invoked selectively, yet models remain overconfident, skip retrieval when it is essential, and falter once initial search queries underperform. Taken together, internal web search works better as a good low-latency verification layer than a reliable analytical tool, with clear room for improvement.",
        "authors": [
            "Sahil Kale"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T09:37:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18931v1"
    },
    {
        "arxiv_id": "2511.18903v1",
        "title": "How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining",
        "abstract": "Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods.",
        "authors": [
            "Kairong Luo",
            "Zhenbo Sun",
            "Haodong Wen",
            "Xinyu Shi",
            "Jiarui Cui",
            "Chenyi Dang",
            "Kaifeng Lyu",
            "Wenguang Chen"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-24T09:03:49+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18903v1"
    },
    {
        "arxiv_id": "2511.18891v1",
        "title": "Reproducibility Study of Large Language Model Bayesian Optimization",
        "abstract": "In this reproducibility study, we revisit the LLAMBO framework of Daxberger et al. (2024), a prompting-based Bayesian optimization (BO) method that uses large language models as discriminative surrogates and acquisition optimizers via text-only interactions. We replicate the core Bayesmark and HPOBench experiments under the original evaluation protocol, but replace GPT-3.5 with the open-weight Llama 3.1 70B model used for all text encoding components.   Our results broadly confirm the main claims of LLAMBO. Contextual warm starting via textual problem and hyperparameter descriptions substantially improves early regret behaviour and reduces variance across runs. LLAMBO's discriminative surrogate is weaker than GP or SMAC as a pure single task regressor, yet benefits from cross task semantic priors induced by the language model. Ablations that remove textual context markedly degrade predictive accuracy and calibration, while the LLAMBO candidate sampler consistently generates higher quality and more diverse proposals than TPE or random sampling. Experiments with smaller backbones (Gemma 27B, Llama 3.1 8B) yield unstable or invalid predictions, suggesting insufficient capacity for reliable surrogate behaviour.   Overall, our study shows that the LLAMBO architecture is robust to changing the language model backbone and remains effective when instantiated with Llama 3.1 70B.",
        "authors": [
            "Adam Rychert",
            "Gasper Spagnolo",
            "Evgenii Posashkov"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T08:48:38+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18891v1"
    },
    {
        "arxiv_id": "2511.18889v1",
        "title": "CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation",
        "abstract": "Data contamination poses a significant challenge to the fairness of LLM evaluations in natural language processing tasks by inadvertently exposing models to test data during training. Current studies attempt to mitigate this issue by modifying existing datasets or generating new ones from freshly collected information. However, these methods fall short of ensuring contamination-resilient evaluation, as they fail to fully eliminate pre-existing knowledge from models or preserve the semantic complexity of the original datasets. To address these limitations, we propose \\textbf{CoreEval}, a \\textbf{Co}ntamination-\\textbf{re}silient \\textbf{Eval}uation strategy for automatically updating data with real-world knowledge. This approach begins by extracting entity relationships from the original data and leveraging the GDELT database to retrieve relevant, up-to-date knowledge. The retrieved knowledge is then recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence and enhanced task relevance. Ultimately, a robust data reflection mechanism is employed to iteratively verify and refine labels, ensuring consistency between the updated and original datasets. Extensive experiments on updated datasets validate the robustness of CoreEval, demonstrating its effectiveness in mitigating performance overestimation caused by data contamination.",
        "authors": [
            "Jingqian Zhao",
            "Bingbing Wang",
            "Geng Tu",
            "Yice Zhang",
            "Qianlong Wang",
            "Bin Liang",
            "Jing Li",
            "Ruifeng Xu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T08:44:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18889v1"
    },
    {
        "arxiv_id": "2511.18864v1",
        "title": "Think Before You Prune: Selective Self-Generated Calibration for Pruning Large Reasoning Models",
        "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning benchmarks. However, their long chain-of-thought reasoning processes incur significant inference overhead. Pruning has emerged as a promising approach to reducing computational costs. However, existing efforts have primarily focused on large language models (LLMs), while pruning LRMs remains unexplored. In this work, we conduct the first empirical study on pruning LRMs and show that directly applying existing pruning techniques fails to yield satisfactory results. Our findings indicate that using self-generated reasoning data for calibration can substantially improve pruning performance. We further investigate how the difficulty and length of reasoning data affect pruning outcomes. Our analysis reveals that challenging and moderately long self-generated reasoning data serve as ideal calibration data. Based on these insights, we propose a Selective Self-Generated Reasoning (SSGR) data construction strategy to provide effective calibration data for pruning LRMs. Experimental results on the DeepSeek-R1-Distill model series validate that our strategy improves the reasoning ability of pruned LRMs by 10%-13% compared to general pruning methods.",
        "authors": [
            "Yang Xiang",
            "Yixin Ji",
            "Juntao Li",
            "Min Zhang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T08:08:19+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18864v1"
    },
    {
        "arxiv_id": "2511.18860v1",
        "title": "Generating Reading Comprehension Exercises with Large Language Models for Educational Applications",
        "abstract": "With the rapid development of large language models (LLMs), the applications of LLMs have grown substantially. In the education domain, LLMs demonstrate significant potential, particularly in automatic text generation, which enables the creation of intelligent and adaptive learning content. This paper proposes a new LLMs framework, which is named as Reading Comprehension Exercise Generation (RCEG). It can generate high-quality and personalized English reading comprehension exercises automatically. Firstly, RCEG uses fine-tuned LLMs to generate content candidates. Then, it uses a discriminator to select the best candidate. Finally, the quality of the generated content has been improved greatly. To evaluate the performance of RCEG, a dedicated dataset for English reading comprehension is constructed to perform the experiments, and comprehensive evaluation metrics are used to analyze the experimental results. These metrics include content diversity, factual accuracy, linguistic toxicity, and pedagogical alignment. Experimental results show that RCEG significantly improves the relevance and cognitive appropriateness of the generated exercises.",
        "authors": [
            "Xingyu Huang",
            "Fei Jiang",
            "Jianli Xiao"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T08:00:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18860v1"
    },
    {
        "arxiv_id": "2511.18852v1",
        "title": "FanarGuard: A Culturally-Aware Moderation Filter for Arabic Language Models",
        "abstract": "Content moderation filters are a critical safeguard against alignment failures in language models. Yet most existing filters focus narrowly on general safety and overlook cultural context. In this work, we introduce FanarGuard, a bilingual moderation filter that evaluates both safety and cultural alignment in Arabic and English. We construct a dataset of over 468K prompt and response pairs, drawn from synthetic and public datasets, scored by a panel of LLM judges on harmlessness and cultural awareness, and use it to train two filter variants. To rigorously evaluate cultural alignment, we further develop the first benchmark targeting Arabic cultural contexts, comprising over 1k norm-sensitive prompts with LLM-generated responses annotated by human raters. Results show that FanarGuard achieves stronger agreement with human annotations than inter-annotator reliability, while matching the performance of state-of-the-art filters on safety benchmarks. These findings highlight the importance of integrating cultural awareness into moderation and establish FanarGuard as a practical step toward more context-sensitive safeguards.",
        "authors": [
            "Masoomali Fatehkia",
            "Enes Altinisik",
            "Husrev Taha Sencar"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T07:48:35+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18852v1"
    },
    {
        "arxiv_id": "2511.18850v1",
        "title": "Cognitive Alpha Mining via LLM-Driven Code-Based Evolution",
        "abstract": "Discovering effective predictive signals, or ``alphas,'' from financial data with high dimensionality and extremely low signal-to-noise ratio remains a difficult open problem. Despite progress in deep learning, genetic programming, and, more recently, large language model (LLM)--based factor generation, existing approaches still explore only a narrow region of the vast alpha search space. Neural models tend to produce opaque and fragile patterns, while symbolic or formula-based methods often yield redundant or economically ungrounded expressions that generalize poorly. Although different in form, these paradigms share a key limitation: none can conduct broad, structured, and human-like exploration that balances logical consistency with creative leaps. To address this gap, we introduce the Cognitive Alpha Mining Framework (CogAlpha), which combines code-level alpha representation with LLM-driven reasoning and evolutionary search. Treating LLMs as adaptive cognitive agents, our framework iteratively refines, mutates, and recombines alpha candidates through multi-stage prompts and financial feedback. This synergistic design enables deeper thinking, richer structural diversity, and economically interpretable alpha discovery, while greatly expanding the effective search space. Experiments on A-share equities demonstrate that CogAlpha consistently discovers alphas with superior predictive accuracy, robustness, and generalization over existing methods. Our results highlight the promise of aligning evolutionary optimization with LLM-based reasoning for automated and explainable alpha discovery. All source code will be released.",
        "authors": [
            "Fengyuan Liu",
            "Huang Yi",
            "Sichun Luo",
            "Yuqi Wang",
            "Yazheng Yang",
            "Xinye Li",
            "Zefa Hu",
            "Junlan Feng",
            "Qi Liu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T07:45:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18850v1"
    },
    {
        "arxiv_id": "2511.18848v1",
        "title": "Large Language Models for the Summarization of Czech Documents: From History to the Present",
        "abstract": "Text summarization is the task of automatically condensing longer texts into shorter, coherent summaries while preserving the original meaning and key information. Although this task has been extensively studied in English and other high-resource languages, Czech summarization, particularly in the context of historical documents, remains underexplored. This is largely due to the inherent linguistic complexity of Czech and the lack of high-quality annotated datasets.   In this work, we address this gap by leveraging the capabilities of Large Language Models (LLMs), specifically Mistral and mT5, which have demonstrated strong performance across a wide range of natural language processing tasks and multilingual settings. In addition, we also propose a translation-based approach that first translates Czech texts into English, summarizes them using an English-language model, and then translates the summaries back into Czech. Our study makes the following main contributions: We demonstrate that LLMs achieve new state-of-the-art results on the SumeCzech dataset, a benchmark for modern Czech text summarization, showing the effectiveness of multilingual LLMs even for morphologically rich, medium-resource languages like Czech. We introduce a new dataset, Posel od \u010cerchova, designed for the summarization of historical Czech texts. This dataset is derived from digitized 19th-century publications and annotated for abstractive summarization. We provide initial baselines using modern LLMs to facilitate further research in this underrepresented area.   By combining cutting-edge models with both modern and historical Czech datasets, our work lays the foundation for further progress in Czech summarization and contributes valuable resources for future research in Czech historical document processing and low-resource summarization more broadly.",
        "authors": [
            "V\u00e1clav Tran",
            "Jakub \u0160m\u00edd",
            "Ladislav Lenc",
            "Jean-Pierre Salmon",
            "Pavel Kr\u00e1l"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T07:40:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18848v1"
    },
    {
        "arxiv_id": "2511.18843v2",
        "title": "A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis",
        "abstract": "Focus group discussions generate rich qualitative data but their analysis traditionally relies on labor-intensive manual coding that limits scalability and reproducibility. We present a systematic framework for applying BERTopic to focus group transcripts using data from ten focus groups exploring HPV vaccine perceptions in Tunisia (1,075 utterances). We conducted comprehensive hyperparameter exploration across 27 configurations, evaluating each through bootstrap stability analysis, performance metrics, and comparison with LDA baseline. Bootstrap analysis revealed that stability metrics (NMI and ARI) exhibited strong disagreement (r = -0.691) and showed divergent relationships with coherence, demonstrating that stability is multifaceted rather than monolithic. Our multi-criteria selection framework yielded a 7-topic model achieving 18\\% higher coherence than optimized LDA (0.573 vs. 0.486) with interpretable topics validated through independent human evaluation (ICC = 0.700, weighted Cohen's kappa = 0.678). These findings demonstrate that transformer-based topic modeling can extract interpretable themes from small focus group transcript corpora when systematically configured and validated, while revealing that quality metrics capture distinct, sometimes conflicting constructs requiring multi-criteria evaluation. We provide complete documentation and code to support reproducibility.",
        "authors": [
            "Heger Arfaoui",
            "Mohammed Iheb Hergli",
            "Beya Benzina",
            "Slimane BenMiled"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T07:30:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18843v2"
    },
    {
        "arxiv_id": "2511.18832v1",
        "title": "Concept than Document: Context Compression via AMR-based Conceptual Entropy",
        "abstract": "Large Language Models (LLMs) face information overload when handling long contexts, particularly in Retrieval-Augmented Generation (RAG) where extensive supporting documents often introduce redundant content. This issue not only weakens reasoning accuracy but also increases computational overhead. We propose an unsupervised context compression framework that exploits Abstract Meaning Representation (AMR) graphs to preserve semantically essential information while filtering out irrelevant text. By quantifying node-level entropy within AMR graphs, our method estimates the conceptual importance of each node, enabling the retention of core semantics. Specifically, we construct AMR graphs from raw contexts, compute the conceptual entropy of each node, and screen significant informative nodes to form a condensed and semantically focused context than raw documents. Experiments on the PopQA and EntityQuestions datasets show that our method outperforms vanilla and other baselines, achieving higher accuracy while substantially reducing context length. To the best of our knowledge, this is the first work introducing AMR-based conceptual entropy for context compression, demonstrating the potential of stable linguistic features in context engineering.",
        "authors": [
            "Kaize Shi",
            "Xueyao Sun",
            "Xiaohui Tao",
            "Lin Li",
            "Qika Lin",
            "Guandong Xu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T07:08:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18832v1"
    },
    {
        "arxiv_id": "2511.18824v1",
        "title": "Assessing the alignment between infants' visual and linguistic experience using multimodal language models",
        "abstract": "Figuring out which objects or concepts words refer to is a central language learning challenge for young children. Most models of this process posit that children learn early object labels from co-occurrences of words and their referents that occur when someone around them talks about an object in the immediate physical environment. But how aligned in time are children's visual and linguistic experiences during everyday learning? To date, answers to this question have been limited by the need for labor-intensive manual annotations of vision-language co-occurrences. Here, we evaluate the use of contrastive language-image pretraining (CLIP) models to automatically characterize vision-language alignment in egocentric videos taken from the infant perspective in home environments. After validating CLIP alignment scores using human alignment judgments, we apply this metric to a large corpus of infant-perspective videos. We show that idealized aligned moments for learning (e.g., \"look at the ball\" with a ball present in the child's view) are relatively rare in children's everyday experiences compared to modern machine learning datasets, and highlight variability in alignment both within and across children. These findings suggest that infrequent alignment is a constraint for models describing early word learning and offer a new method for investigating children's multimodal environment.",
        "authors": [
            "Alvin Wei Ming Tan",
            "Jane Yang",
            "Tarun Sepuri",
            "Khai Loong Aw",
            "Robert Z. Sparks",
            "Zi Yin",
            "Virginia A. Marchman",
            "Michael C. Frank",
            "Bria Long"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-24T06:58:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18824v1"
    },
    {
        "arxiv_id": "2511.18808v2",
        "title": "HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations",
        "abstract": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to access external knowledge, helping mitigate hallucinations and enhance domain-specific expertise. Graph-based RAG enhances structural reasoning by introducing explicit relational organization that enables information propagation across semantically connected text units. However, these methods typically rely on Euclidean embeddings that capture semantic similarity but lack a geometric notion of hierarchical depth, limiting their ability to represent abstraction relationships inherent in complex knowledge graphs. To capture both fine-grained semantics and global hierarchy, we propose HyperbolicRAG, a retrieval framework that integrates hyperbolic geometry into graph-based RAG. HyperbolicRAG introduces three key designs: (1) a depth-aware representation learner that embeds nodes within a shared Poincare manifold to align semantic similarity with hierarchical containment, (2) an unsupervised contrastive regularization that enforces geometric consistency across abstraction levels, and (3) a mutual-ranking fusion mechanism that jointly exploits retrieval signals from Euclidean and hyperbolic spaces, emphasizing cross-space agreement during inference. Extensive experiments across multiple QA benchmarks demonstrate that HyperbolicRAG outperforms competitive baselines, including both standard RAG and graph-augmented baselines.",
        "authors": [
            "Linxiao Cao",
            "Ruitao Wang",
            "Jindong Li",
            "Zhipeng Zhou",
            "Menglin Yang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T06:27:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18808v2"
    },
    {
        "arxiv_id": "2511.18774v1",
        "title": "Context-Aware Whisper for Arabic ASR Under Linguistic Varieties",
        "abstract": "Low-resource ASR remains a challenging problem, especially for languages like Arabic that exhibit wide dialectal variation and limited labeled data. We propose context-aware prompting strategies to adapt OpenAI's Whisper for Arabic speech recognition without retraining. Our methods include decoder prompting with first-pass transcriptions or retrieved utterances, and encoder prefixing using speech synthesized in the target speaker's voice. We introduce techniques such as prompt reordering, speaker-aware prefix synthesis, and modality-specific retrieval (lexical, semantic, acoustic) to improve transcription in real-world, zero-shot settings. Evaluated on nine Arabic linguistic conditions, our approach reduces WER by up to 22.3% on Modern Standard Arabic and 9.2% on dialectal speech, significantly mitigating hallucinations and speaker mismatch.",
        "authors": [
            "Bashar Talafha",
            "Amin Abu Alhassan",
            "Muhammad Abdul-Mageed"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T05:16:04+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18774v1"
    },
    {
        "arxiv_id": "2511.18751v2",
        "title": "Robust Multimodal Sentiment Analysis with Distribution-Based Feature Recovery and Fusion",
        "abstract": "As posts on social media increase rapidly, analyzing the sentiments embedded in image-text pairs has become a popular research topic in recent years. Although existing works achieve impressive accomplishments in simultaneously harnessing image and text information, they lack the considerations of possible low-quality and missing modalities. In real-world applications, these issues might frequently occur, leading to urgent needs for models capable of predicting sentiment robustly. Therefore, we propose a Distribution-based feature Recovery and Fusion (DRF) method for robust multimodal sentiment analysis of image-text pairs. Specifically, we maintain a feature queue for each modality to approximate their feature distributions, through which we can simultaneously handle low-quality and missing modalities in a unified framework. For low-quality modalities, we reduce their contributions to the fusion by quantitatively estimating modality qualities based on the distributions. For missing modalities, we build inter-modal mapping relationships supervised by samples and distributions, thereby recovering the missing modalities from available ones. In experiments, two disruption strategies that corrupt and discard some modalities in samples are adopted to mimic the low-quality and missing modalities in various real-world scenarios. Through comprehensive experiments on three publicly available image-text datasets, we demonstrate the universal improvements of DRF compared to SOTA methods under both two strategies, validating its effectiveness in robust multimodal sentiment analysis.",
        "authors": [
            "Daiqing Wu",
            "Dongbao Yang",
            "Yu Zhou",
            "Can Ma"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T04:24:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18751v2"
    },
    {
        "arxiv_id": "2511.18749v1",
        "title": "Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search",
        "abstract": "Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.",
        "authors": [
            "Matthew R. DeVerna",
            "Kai-Cheng Yang",
            "Harry Yaojun Yan",
            "Filippo Menczer"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T04:22:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18749v1"
    },
    {
        "arxiv_id": "2511.18743v1",
        "title": "RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context",
        "abstract": "Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.",
        "authors": [
            "Yu Lei",
            "Shuzheng Si",
            "Wei Wang",
            "Yifei Wu",
            "Gang Chen",
            "Fanchao Qi",
            "Maosong Sun"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T04:12:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18743v1"
    },
    {
        "arxiv_id": "2511.21753v1",
        "title": "Extracting Disaster Impacts and Impact Related Locations in Social Media Posts Using Large Language Models",
        "abstract": "Large-scale disasters can often result in catastrophic consequences on people and infrastructure. Situation awareness about such disaster impacts generated by authoritative data from in-situ sensors, remote sensing imagery, and/or geographic data is often limited due to atmospheric opacity, satellite revisits, and time limitations. This often results in geo-temporal information gaps. In contrast, impact-related social media posts can act as \"geo-sensors\" during a disaster, where people describe specific impacts and locations. However, not all locations mentioned in disaster-related social media posts relate to an impact. Only the impacted locations are critical for directing resources effectively. e.g., \"The death toll from a fire which ripped through the Greek coastal town of #Mati stood at 80, with dozens of people unaccounted for as forensic experts tried to identify victims who were burned alive #Greecefires #AthensFires #Athens #Greece.\" contains impacted location \"Mati\" and non-impacted locations \"Greece\" and \"Athens\". This research uses Large Language Models (LLMs) to identify all locations, impacts and impacted locations mentioned in disaster-related social media posts. In the process, LLMs are fine-tuned to identify only impacts and impacted locations (as distinct from other, non-impacted locations), including locations mentioned in informal expressions, abbreviations, and short forms. Our fine-tuned model demonstrates efficacy, achieving an F1-score of 0.69 for impact and 0.74 for impacted location extraction, substantially outperforming the pre-trained baseline. These robust results confirm the potential of fine-tuned language models to offer a scalable solution for timely decision-making in resource allocation, situational awareness, and post-disaster recovery planning for responders.",
        "authors": [
            "Sameeah Noreen Hameed",
            "Surangika Ranathunga",
            "Raj Prasanna",
            "Kristin Stock",
            "Christopher B. Jones"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T02:55:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21753v1"
    },
    {
        "arxiv_id": "2511.18696v1",
        "title": "Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models",
        "abstract": "This report presents the Empathetic Cascading Networks (ECN) framework, a multi-stage prompting method designed to enhance the empathetic and inclusive capabilities of large language models. ECN employs four stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, to guide models toward generating emotionally resonant and contextually aware responses. Experimental results demonstrate that ECN achieves the highest Empathy Quotient (EQ) scores across GPT-3.5-turbo and GPT-4, while maintaining competitive Regard and Perplexity metrics. These findings emphasize ECN's potential for applications requiring empathy and inclusivity in conversational AI.",
        "authors": [
            "Wangjiaxuan Xin"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T02:32:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18696v1"
    },
    {
        "arxiv_id": "2511.18659v2",
        "title": "CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning",
        "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.",
        "authors": [
            "Jie He",
            "Richard He Bai",
            "Sinead Williamson",
            "Jeff Z. Pan",
            "Navdeep Jaitly",
            "Yizhe Zhang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-24T00:11:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18659v2"
    },
    {
        "arxiv_id": "2511.18649v2",
        "title": "Evaluating Large Language Models on the 2026 Korean CSAT Mathematics Exam: Measuring Mathematical Ability in a Zero-Data-Leakage Setting",
        "abstract": "This study systematically evaluated the mathematical reasoning capabilities of Large Language Models (LLMs) using the 2026 Korean College Scholastic Ability Test (CSAT) Mathematics section, ensuring a completely contamination-free evaluation environment. To address data leakage issues in existing benchmarks, we digitized all 46 questions (22 common and 24 elective) within two hours of the exam's public release, eliminating any possibility of inclusion in model training data. We conducted comprehensive evaluations of 24 state-of-the-art LLMs across varying input modalities (Text-only, Image-only, Text+Figure) and prompt languages (Korean, English). The GPT-5 family models achieved perfect scores (100 points) under a limited set of language-modality configurations, while Grok 4, Qwen 3 235B, and Gemini 2.5 pro also scored above 97 points. Notably, gpt-oss-20B achieved 95.7 points despite its relatively small size, demonstrating high cost-effectiveness. Problem-specific analysis revealed Calculus as the weakest domain with significant performance degradation on 4-point high-difficulty problems. Text input consistently outperformed image input, while prompt language effects varied by model scale. In reasoning enhancement experiments with GPT-5 series, increased reasoning intensity improved performance (82.6->100 points) but quadrupled token usage and drastically reduced efficiency, suggesting that models with minimal reasoning may be more practical. This research contributes: (1) implementation of a completely unexposed evaluation environment, (2) a standardized digitization pipeline that converts human-targeted exam materials into LLM-ready evaluation data, and (3) a practical evaluation perspective integrating performance, cost, and time considerations. Detailed results and model comparisons are available at the 2026 Korean CSAT LLM Evaluation Leaderboard; https://isoft.cnu.ac.kr/csat2026/",
        "authors": [
            "Goun Pyeon",
            "Inbum Heo",
            "Jeesu Jung",
            "Taewook Hwang",
            "Hyuk Namgoong",
            "Hyein Seo",
            "Yerim Han",
            "Eunbin Kim",
            "Hyeonseok Kang",
            "Sangkeun Jung"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T23:09:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18649v2"
    },
    {
        "arxiv_id": "2511.18635v1",
        "title": "No Free Lunch in Language Model Bias Mitigation? Targeted Bias Reduction Can Exacerbate Unmitigated LLM Biases",
        "abstract": "Large Language Models (LLMs) inherit societal biases from their training data, potentially leading to harmful or unfair outputs. While various techniques aim to mitigate these biases, their effects are often evaluated only along the dimension of the bias being targeted. This work investigates the cross-category consequences of targeted bias mitigation. We study four bias mitigation techniques applied across ten models from seven model families, and we explore racial, religious, profession- and gender-related biases. We measure the impact of debiasing on model coherence and stereotypical preference using the StereoSet benchmark. Our results consistently show that while targeted mitigation can sometimes reduce bias in the intended dimension, it frequently leads to unintended and often negative consequences in others, such as increasing model bias and decreasing general coherence. These findings underscore the critical need for robust, multi-dimensional evaluation tools when examining and developing bias mitigation strategies to avoid inadvertently shifting or worsening bias along untargeted axes.",
        "authors": [
            "Shireen Chand",
            "Faith Baca",
            "Emilio Ferrara"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T22:21:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18635v1"
    },
    {
        "arxiv_id": "2511.18630v1",
        "title": "Majority of the Bests: Improving Best-of-N via Bootstrapping",
        "abstract": "Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.",
        "authors": [
            "Amin Rakhsha",
            "Kanika Madan",
            "Tianyu Zhang",
            "Amir-massoud Farahmand",
            "Amir Khasahmadi"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-23T22:05:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18630v1"
    },
    {
        "arxiv_id": "2511.18622v1",
        "title": "OpenGloss: A Synthetic Encyclopedic Dictionary and Semantic Knowledge Graph",
        "abstract": "We present OpenGloss, a synthetic encyclopedic dictionary and semantic knowledge graph for English that integrates lexicographic definitions, encyclopedic context, etymological histories, and semantic relationships in a unified resource. OpenGloss contains 537K senses across 150K lexemes, on par with WordNet 3.1 and Open English WordNet, while providing more than four times as many sense definitions. These lexemes include 9.1M semantic edges, 1M usage examples, 3M collocations, and 60M words of encyclopedic content.   Generated through a multi-agent procedural generation pipeline with schema-validated LLM outputs and automated quality assurance, the entire resource was produced in under one week for under $1,000. This demonstrates that structured generation can create comprehensive lexical resources at cost and time scales impractical for manual curation, enabling rapid iteration as foundation models improve. The resource addresses gaps in pedagogical applications by providing integrated content -- definitions, examples, collocations, encyclopedias, etymology -- that supports both vocabulary learning and natural language processing tasks.   As a synthetically generated resource, OpenGloss reflects both the capabilities and limitations of current foundation models. The dataset is publicly available on Hugging Face under CC-BY 4.0, enabling researchers and educators to build upon and adapt this resource.",
        "authors": [
            "Michael J. Bommarito"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T21:33:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18622v1"
    },
    {
        "arxiv_id": "2511.18619v1",
        "title": "Prompt Optimization as a State-Space Search Problem",
        "abstract": "Language Models are extremely susceptible to performance collapse with even small changes to input prompt strings. Libraries such as DSpy (from Stanford NLP) avoid this problem through demonstration-based prompt optimisation. Inspired by this, I propose an alternative approach that treats prompt optimisation as a classical state-space search problem. I model the prompt space as a graph where nodes represent prompt states and edges correspond to deliberate transformations such as shortening, adding examples, or re- ordering content. Using beam search and random walk algorithms, I systematically explore this space, evaluating candidates on development sets and pruning unpromising branches. Across five NLP tasks (sentiment classification, question answering, summarisation, reason- ing, and natural language inference), I find that even shallow search configurations (beam width=2, depth=2) improve upon seed prompts on development sets. For instance, beam search achieves development accuracy gains from 0.40 to 0.80 on reasoning tasks, though test set improvements are more modest (0.20 to 0.50), indicating overfitting to the develop- ment heuristic. Analysis of successful optimisation paths reveals that transformations that make prompts concise appear most frequently, while verbosity operators are never selected. My results validate prompt optimization as a search problem and suggest that with greater computational resources and improved evaluation metrics, deeper exploration could yield more robust prompts that generalize beyond development sets. Code and implementation are available at [https://github.com/MaanasTaneja/PromptOptimiser].",
        "authors": [
            "Maanas Taneja"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T21:24:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18619v1"
    },
    {
        "arxiv_id": "2511.18618v1",
        "title": "A Unified BERT-CNN-BiLSTM Framework for Simultaneous Headline Classification and Sentiment Analysis of Bangla News",
        "abstract": "In our daily lives, newspapers are an essential information source that impacts how the public talks about present-day issues. However, effectively navigating the vast amount of news content from different newspapers and online news portals can be challenging. Newspaper headlines with sentiment analysis tell us what the news is about (e.g., politics, sports) and how the news makes us feel (positive, negative, neutral). This helps us quickly understand the emotional tone of the news. This research presents a state-of-the-art approach to Bangla news headline classification combined with sentiment analysis applying Natural Language Processing (NLP) techniques, particularly the hybrid transfer learning model BERT-CNN-BiLSTM. We have explored a dataset called BAN-ABSA of 9014 news headlines, which is the first time that has been experimented with simultaneously in the headline and sentiment categorization in Bengali newspapers. Over this imbalanced dataset, we applied two experimental strategies: technique-1, where undersampling and oversampling are applied before splitting, and technique-2, where undersampling and oversampling are applied after splitting on the In technique-1 oversampling provided the strongest performance, both headline and sentiment, that is 78.57\\% and 73.43\\% respectively, while technique-2 delivered the highest result when trained directly on the original imbalanced dataset, both headline and sentiment, that is 81.37\\% and 64.46\\% respectively. The proposed model BERT-CNN-BiLSTM significantly outperforms all baseline models in classification tasks, and achieves new state-of-the-art results for Bangla news headline classification and sentiment analysis. These results demonstrate the importance of leveraging both the headline and sentiment datasets, and provide a strong baseline for Bangla text classification in low-resource.",
        "authors": [
            "Mirza Raquib",
            "Munazer Montasir Akash",
            "Tawhid Ahmed",
            "Saydul Akbar Murad",
            "Farida Siddiqi Prity",
            "Mohammad Amzad Hossain",
            "Asif Pervez Polok",
            "Nick Rahimi"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T21:22:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18618v1"
    },
    {
        "arxiv_id": "2511.18616v1",
        "title": "A Benchmark for Zero-Shot Belief Inference in Large Language Models",
        "abstract": "Beliefs are central to how humans reason, communicate, and form social connections, yet most computational approaches to studying them remain confined to narrow sociopolitical contexts and rely on fine-tuning for optimal performance. Despite the growing use of large language models (LLMs) across disciplines, how well these systems generalize across diverse belief domains remains unclear. We introduce a systematic, reproducible benchmark that evaluates the ability of LLMs to predict individuals' stances on a wide range of topics in a zero-shot setting using data from an online debate platform. The benchmark includes multiple informational conditions that isolate the contribution of demographic context and known prior beliefs to predictive success. Across several small- to medium-sized models, we find that providing more background information about an individual improves predictive accuracy, but performance varies substantially across belief domains. These findings reveal both the capacity and limitations of current LLMs to emulate human reasoning, advancing the study of machine behavior and offering a scalable framework for modeling belief systems beyond the sociopolitical sphere.",
        "authors": [
            "Joseph Malone",
            "Rachith Aiyappa",
            "Byunghwee Lee",
            "Haewoon Kwak",
            "Jisun An",
            "Yong-Yeol Ahn"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T21:13:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18616v1"
    },
    {
        "arxiv_id": "2511.21752v1",
        "title": "Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification",
        "abstract": "Large language models are increasingly used for text classification tasks such as sentiment analysis, yet their reliance on natural language prompts exposes them to prompt injection attacks. In particular, class-directive injections exploit knowledge of the model's label set (e.g., positive vs. negative) to override its intended behavior through adversarial instructions. Existing defenses, such as detection-based filters, instruction hierarchies, and signed prompts, either require model retraining or remain vulnerable to obfuscation. This paper introduces Label Disguise Defense (LDD), a lightweight and model-agnostic strategy that conceals true labels by replacing them with semantically transformed or unrelated alias labels(e.g., blue vs. yellow). The model learns these new label mappings implicitly through few-shot demonstrations, preventing direct correspondence between injected directives and decision outputs. We evaluate LDD across nine state-of-the-art models, including GPT-5, GPT-4o, LLaMA3.2, Gemma3, and Mistral variants, under varying few-shot and an adversarial setting. Our results show that the ability of LDD to recover performance lost to the adversarial attack varies across models and alias choices. For every model evaluated, LDD is able to restore a portion of the accuracy degradation caused by the attack. Moreover, for the vast majority of models, we can identify more than one alias pair that achieves higher accuracy than the under-attack baseline, in which the model relies solely on few-shot learning without any defensive mechanism. A linguistic analysis further reveals that semantically aligned alias labels(e.g., good vs. bad) yield stronger robustness than unaligned symbols(e.g., blue vs. yellow). Overall, this study demonstrates that label semantics can serve as an effective defense layer, transforming meaning itself into a shield against prompt injection.",
        "authors": [
            "Yanxi Li",
            "Ruocheng Shan"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T20:16:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21752v1"
    },
    {
        "arxiv_id": "2511.18597v1",
        "title": "Toward Trustworthy Difficulty Assessments: Large Language Models as Judges in Programming and Synthetic Tasks",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in natural language and code generation, and are increasingly deployed as automatic judges of model outputs and learning activities. Yet, their behavior on structured tasks such as predicting the difficulty of competitive programming problems remains under-explored. We conduct a systematic comparison of GPT-4o, used purely as a natural-language difficulty assessor, against an interpretable Light-GBM ensemble trained on explicit numeric and textual features. On a dataset of 1,825 LeetCode problems labeled Easy, Medium, or Hard, LightGBM attains 86% accuracy, whereas GPT-4o reaches only 37.75%. Detailed analyses, including confusion matrices and SHAP-based interpretability, show that numeric constraints -- such as input size limits and acceptance rates -- play a crucial role in separating Hard problems from easier ones. By contrast, GPT-4o often overlooks these cues and exhibits a strong bias toward simpler categories. We further probe GPT-4o through a synthetic Hard-problem generation protocol. Surprisingly, GPT-4o labels almost all of its own synthetic Hard problems as Medium, contradicting its tendency to downgrade real Hard problems to Easy. Our findings connect to recent work on LLMs-as-judges and automatic difficulty estimation in programming and education, and highlight concrete failure modes that must be addressed before LLM-based judges can be considered trustworthy in competitive programming, educational platforms, or reinforcement-learning pipelines.",
        "authors": [
            "H. M. Shadman Tabib",
            "Jaber Ahmed Deedar"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T19:39:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18597v1"
    },
    {
        "arxiv_id": "2511.18557v1",
        "title": "Dealing with the Hard Facts of Low-Resource African NLP",
        "abstract": "Creating speech datasets, models, and evaluation frameworks for low-resource languages remains challenging given the lack of a broad base of pertinent experience to draw from. This paper reports on the field collection of 612 hours of spontaneous speech in Bambara, a low-resource West African language; the semi-automated annotation of that dataset with transcriptions; the creation of several monolingual ultra-compact and small models using the dataset; and the automatic and human evaluation of their output. We offer practical suggestions for data collection protocols, annotation, and model design, as well as evidence for the importance of performing human evaluation. In addition to the main dataset, multiple evaluation datasets, models, and code are made publicly available.",
        "authors": [
            "Yacouba Diarra",
            "Nouhoum Souleymane Coulibaly",
            "Panga Azazia Kamat\u00e9",
            "Madani Amadou Tall",
            "Emmanuel \u00c9lis\u00e9 Kon\u00e9",
            "Aymane Demb\u00e9l\u00e9",
            "Michael Leventhal"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T18:08:36+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18557v1"
    },
    {
        "arxiv_id": "2511.18538v3",
        "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence",
        "abstract": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.",
        "authors": [
            "Jian Yang",
            "Xianglong Liu",
            "Weifeng Lv",
            "Ken Deng",
            "Shawn Guo",
            "Lin Jing",
            "Yizhi Li",
            "Shark Liu",
            "Xianzhen Luo",
            "Yuyu Luo",
            "Changzai Pan",
            "Ensheng Shi",
            "Yingshui Tan",
            "Renshuai Tao",
            "Jiajun Wu",
            "Xianjie Wu",
            "Zhenhe Wu",
            "Daoguang Zan",
            "Chenchen Zhang",
            "Wei Zhang",
            "He Zhu",
            "Terry Yue Zhuo",
            "Kerui Cao",
            "Xianfu Cheng",
            "Jun Dong",
            "Shengjie Fang",
            "Zhiwei Fei",
            "Xiangyuan Guan",
            "Qipeng Guo",
            "Zhiguang Han",
            "Joseph James",
            "Tianqi Luo",
            "Renyuan Li",
            "Yuhang Li",
            "Yiming Liang",
            "Congnan Liu",
            "Jiaheng Liu",
            "Qian Liu",
            "Ruitong Liu",
            "Tyler Loakman",
            "Xiangxin Meng",
            "Chuang Peng",
            "Tianhao Peng",
            "Jiajun Shi",
            "Mingjie Tang",
            "Boyang Wang",
            "Haowen Wang",
            "Yunli Wang",
            "Fanglin Xu",
            "Zihan Xu",
            "Fei Yuan",
            "Ge Zhang",
            "Jiayi Zhang",
            "Xinhao Zhang",
            "Wangchunshu Zhou",
            "Hualei Zhu",
            "King Zhu",
            "Bryan Dai",
            "Aishan Liu",
            "Zhoujun Li",
            "Chenghua Lin",
            "Tianyu Liu",
            "Chao Peng",
            "Kai Shen",
            "Libo Qin",
            "Shuangyong Song",
            "Zizheng Zhan",
            "Jiajun Zhang",
            "Jie Zhang",
            "Zhaoxiang Zhang",
            "Bo Zheng"
        ],
        "category": "cs.SE",
        "submitted_date": "2025-11-23T17:09:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18538v3"
    },
    {
        "arxiv_id": "2511.21750v1",
        "title": "SO-Bench: A Structural Output Evaluation of Multimodal LLMs",
        "abstract": "Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.",
        "authors": [
            "Di Feng",
            "Kaixin Ma",
            "Feng Nan",
            "Haofeng Chen",
            "Bohan Zhai",
            "David Griffiths",
            "Mingfei Gao",
            "Zhe Gan",
            "Eshan Verma",
            "Yinfei Yang",
            "Zhifeng Chen",
            "Afshin Dehghan"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-23T16:53:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21750v1"
    },
    {
        "arxiv_id": "2511.18499v1",
        "title": "For Those Who May Find Themselves on the Red Team",
        "abstract": "This position paper argues that literary scholars must engage with large language model (LLM) interpretability research. While doing so will involve ideological struggle, if not out-right complicity, the necessity of this engagement is clear: the abiding instrumentality of current approaches to interpretability cannot be the only standard by which we measure interpretation with LLMs. One site at which this struggle could take place, I suggest, is the red team.",
        "authors": [
            "Tyler Shoemaker"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T15:31:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18499v1"
    },
    {
        "arxiv_id": "2511.18491v2",
        "title": "MindEval: Benchmarking Language Models on Multi-turn Mental Health Support",
        "abstract": "Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data.",
        "authors": [
            "Jos\u00e9 Pombal",
            "Maya D'Eon",
            "Nuno M. Guerreiro",
            "Pedro Henrique Martins",
            "Ant\u00f3nio Farinhas",
            "Ricardo Rei"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T15:19:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18491v2"
    },
    {
        "arxiv_id": "2511.18487v1",
        "title": "InstructAudio: Unified speech and music generation with natural language instruction",
        "abstract": "Text-to-speech (TTS) and text-to-music (TTM) models face significant limitations in instruction-based control. TTS systems usually depend on reference audio for timbre, offer only limited text-level attribute control, and rarely support dialogue generation. TTM systems are constrained by input conditioning requirements that depend on expert knowledge annotations. The high heterogeneity of these input control conditions makes them difficult to joint modeling with speech synthesis. Despite sharing common acoustic modeling characteristics, these two tasks have long been developed independently, leaving open the challenge of achieving unified modeling through natural language instructions. We introduce InstructAudio, a unified framework that enables instruction-based (natural language descriptions) control of acoustic attributes including timbre (gender, age), paralinguistic (emotion, style, accent), and musical (genre, instrument, rhythm, atmosphere). It supports expressive speech, music, and dialogue generation in English and Chinese. The model employs joint and single diffusion transformer layers with a standardized instruction-phoneme input format, trained on 50K hours of speech and 20K hours of music data, enabling multi-task learning and cross-modal alignment. Fig. 1 visualizes performance comparisons with mainstream TTS and TTM models, demonstrating that InstructAudio achieves optimal results on most metrics. To our best knowledge, InstructAudio represents the first instruction-controlled framework unifying speech and music generation. Audio samples are available at: https://qiangchunyu.github.io/InstructAudio/",
        "authors": [
            "Chunyu Qiang",
            "Kang Yin",
            "Xiaopeng Wang",
            "Yuzhe Liang",
            "Jiahui Zhao",
            "Ruibo Fu",
            "Tianrui Wang",
            "Cheng Gong",
            "Chen Zhang",
            "Longbiao Wang",
            "Jianwu Dang"
        ],
        "category": "eess.AS",
        "submitted_date": "2025-11-23T15:15:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18487v1"
    },
    {
        "arxiv_id": "2511.18467v1",
        "title": "Shadows in the Code: Exploring the Risks and Defenses of LLM-based Multi-Agent Software Development Systems",
        "abstract": "The rapid advancement of Large Language Model (LLM)-driven multi-agent systems has significantly streamlined software developing tasks, enabling users with little technical expertise to develop executable applications. While these systems democratize software creation through natural language requirements, they introduce significant security risks that remain largely unexplored. We identify two risky scenarios: Malicious User with Benign Agents (MU-BA) and Benign User with Malicious Agents (BU-MA). We introduce the Implicit Malicious Behavior Injection Attack (IMBIA), demonstrating how multi-agent systems can be manipulated to generate software with concealed malicious capabilities beneath seemingly benign applications, and propose Adv-IMBIA as a defense mechanism. Evaluations across ChatDev, MetaGPT, and AgentVerse frameworks reveal varying vulnerability patterns, with IMBIA achieving attack success rates of 93%, 45%, and 71% in MU-BA scenarios, and 71%, 84%, and 45% in BU-MA scenarios. Our defense mechanism reduced attack success rates significantly, particularly in the MU-BA scenario. Further analysis reveals that compromised agents in the coding and testing phases pose significantly greater security risks, while also identifying critical agents that require protection against malicious user exploitation. Our findings highlight the urgent need for robust security measures in multi-agent software development systems and provide practical guidelines for implementing targeted, resource-efficient defensive strategies.",
        "authors": [
            "Xiaoqing Wang",
            "Keman Huang",
            "Bin Liang",
            "Hongyu Li",
            "Xiaoyong Du"
        ],
        "category": "cs.CR",
        "submitted_date": "2025-11-23T14:26:35+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18467v1"
    },
    {
        "arxiv_id": "2511.18423v1",
        "title": "General Agentic Memory Via Deep Research",
        "abstract": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \\textbf{general agentic memory (GAM)}. GAM follows the principle of \"\\textbf{just-in time (JIT) compilation}\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \\textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \\textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.",
        "authors": [
            "B. Y. Yan",
            "Chaofan Li",
            "Hongjin Qian",
            "Shuqi Lu",
            "Zheng Liu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T12:29:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18423v1"
    },
    {
        "arxiv_id": "2511.18413v1",
        "title": "Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations",
        "abstract": "Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.",
        "authors": [
            "Yu Xia",
            "Sungchul Kim",
            "Tong Yu",
            "Ryan A. Rossi",
            "Julian McAuely"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T11:57:10+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18413v1"
    },
    {
        "arxiv_id": "2511.18411v1",
        "title": "SmolKalam: Ensemble Quality-Filtered Translation at Scale for High Quality Arabic Post-Training Data",
        "abstract": "Although the community has tackled the acquisition of high-quality Arabic pretraining data, we still lack large-scale, multi-turn Arabic datasets that include reasoning and tool calling. Naive translation can work at the pretraining scale, but post-training demands much higher quality, which requires a stricter approach to dataset curation. In this work, we introduce SmolKalam, a translation of Smoltalk2 that uses a multi-model ensemble translation pipeline, applies quality filtering, and examines effective translation techniques for traditional decoder-only models through ablations.",
        "authors": [
            "Sultan Alrashed",
            "Chadi Helwe",
            "Francesco Orabona"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T11:53:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18411v1"
    },
    {
        "arxiv_id": "2511.18409v1",
        "title": "Findings of the BlackboxNLP 2025 Shared Task: Localizing Circuits and Causal Variables in Language Models",
        "abstract": "Mechanistic interpretability (MI) seeks to uncover how language models (LMs) implement specific behaviors, yet measuring progress in MI remains challenging. The recently released Mechanistic Interpretability Benchmark (MIB; Mueller et al., 2025) provides a standardized framework for evaluating circuit and causal variable localization. Building on this foundation, the BlackboxNLP 2025 Shared Task extends MIB into a community-wide reproducible comparison of MI techniques. The shared task features two tracks: circuit localization, which assesses methods that identify causally influential components and interactions driving model behavior, and causal variable localization, which evaluates approaches that map activations into interpretable features. With three teams spanning eight different methods, participants achieved notable gains in circuit localization using ensemble and regularization strategies for circuit discovery. With one team spanning two methods, participants achieved significant gains in causal variable localization using low-dimensional and non-linear projections to featurize activation vectors. The MIB leaderboard remains open; we encourage continued work in this standard evaluation framework to measure progress in MI research going forward.",
        "authors": [
            "Dana Arad",
            "Yonatan Belinkov",
            "Hanjie Chen",
            "Najoung Kim",
            "Hosein Mohebbi",
            "Aaron Mueller",
            "Gabriele Sarti",
            "Martin Tutek"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T11:33:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18409v1"
    },
    {
        "arxiv_id": "2511.18393v1",
        "title": "Towards Robust and Fair Next Visit Diagnosis Prediction under Noisy Clinical Notes with Large Language Models",
        "abstract": "A decade of rapid advances in artificial intelligence (AI) has opened new opportunities for clinical decision support systems (CDSS), with large language models (LLMs) demonstrating strong reasoning abilities on timely medical tasks. However, clinical texts are often degraded by human errors or failures in automated pipelines, raising concerns about the reliability and fairness of AI-assisted decision-making. Yet the impact of such degradations remains under-investigated, particularly regarding how noise-induced shifts can heighten predictive uncertainty and unevenly affect demographic subgroups. We present a systematic study of state-of-the-art LLMs under diverse text corruption scenarios, focusing on robustness and equity in next-visit diagnosis prediction. To address the challenge posed by the large diagnostic label space, we introduce a clinically grounded label-reduction scheme and a hierarchical chain-of-thought (CoT) strategy that emulates clinicians' reasoning. Our approach improves robustness and reduces subgroup instability under degraded inputs, advancing the reliable use of LLMs in CDSS. We release code at https://github.com/heejkoo9/NECHOv3.",
        "authors": [
            "Heejoon Koo"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T10:40:36+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18393v1"
    },
    {
        "arxiv_id": "2511.18369v1",
        "title": "Tu crois que c'est vrai ? Diversite des regimes d'enonciation face aux fake news et mecanismes d'autoregulation conversationnelle",
        "abstract": "This thesis addresses two paradoxes: (1) why empirical studies find that fake news represent only a small share of the information consulted and shared on social media despite the absence of editorial control or journalistic norms, and (2) how political polarization has intensified even though users do not appear especially receptive to fake news. To investigate these issues, two complementary studies were carried out on Twitter and Facebook, combining quantitative analyses of digital traces with online observation and interviews. This mixed-methods design avoids reducing users to single reactions to identified fake items and instead examines the variety of practices across different interactional situations, online and offline, while recording socio-demographic traits. The first study mapped users who shared at least one item labeled fake by fact-checkers in the French Twittersphere. The second used a corpus of items flagged by Facebook users to study reactions to statements whose epistemic status is uncertain. Three main findings emerge. First, sharing fake news is concentrated among a limited group of users who are not less educated or cognitively disadvantaged but are more politicized and critical of institutions; owing to their high activity and prolific sharing, they can help set the agenda for their political camp. Second, exposed users can deploy varying forms of critical distance depending on their social position and the interactional norms of the situations they inhabit: either discursive caution (prudence \u00e9nonciative) or interventions ('points d'arr\u00eat') that express disagreement or corrections. Third, these forms of critical distance seldom yield genuine deliberative debates or agonistic pluralism; rather, they often produce dialogues of the deaf among a small, particularly active minority.",
        "authors": [
            "Manon Berriche"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T09:28:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18369v1"
    },
    {
        "arxiv_id": "2511.18335v1",
        "title": "OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas",
        "abstract": "The ability of Large Language Models (LLMs) to generate structured outputs that follow arbitrary schemas is crucial to a wide range of downstream tasks that require diverse structured representations of results such as information extraction, table generation, and function calling. While modern LLMs excel in generating unstructured responses in natural language, whether this advancement translates to a strong performance on text-to-structure tasks remains unclear. To bridge this gap, we first introduce OmniStruct, a comprehensive benchmark for assessing LLMs' capabilities on diverse text-to-structure tasks such as information extraction, table generation, and function calling. We build OmniStruct by identifying existing datasets across a wide range of tasks that are suitable for a structured answer format, and adapting them under a unified text-to-structure problem setting. To facilitate the development of efficient text-to-structure models, we collect high-quality training data via synthetic task generation. Without using any supervised data for OmniStruct tasks, our experiments demonstrate the possibility of fine-tuning much smaller models on synthetic data into universal structured generation models that can rival the performance of GPT-4o.",
        "authors": [
            "James Y. Huang",
            "Wenxuan Zhou",
            "Nan Xu",
            "Fei Wang",
            "Qin Liu",
            "Sheng Zhang",
            "Hoifung Poon",
            "Muhao Chen"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T08:18:12+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18335v1"
    },
    {
        "arxiv_id": "2511.21749v1",
        "title": "Proactive Defense: Compound AI for Detecting Persuasion Attacks and Measuring Inoculation Effectiveness",
        "abstract": "This paper introduces BRIES, a novel compound AI architecture designed to detect and measure the effectiveness of persuasion attacks across information environments. We present a system with specialized agents: a Twister that generates adversarial content employing targeted persuasion tactics, a Detector that identifies attack types with configurable parameters, a Defender that creates resilient content through content inoculation, and an Assessor that employs causal inference to evaluate inoculation effectiveness. Experimenting with the SemEval 2023 Task 3 taxonomy across the synthetic persuasion dataset, we demonstrate significant variations in detection performance across language agents. Our comparative analysis reveals significant performance disparities with GPT-4 achieving superior detection accuracy on complex persuasion techniques, while open-source models like Llama3 and Mistral demonstrated notable weaknesses in identifying subtle rhetorical, suggesting that different architectures encode and process persuasive language patterns in fundamentally different ways. We show that prompt engineering dramatically affects detection efficacy, with temperature settings and confidence scoring producing model-specific variations; Gemma and GPT-4 perform optimally at lower temperatures while Llama3 and Mistral show improved capabilities at higher temperatures. Our causal analysis provides novel insights into socio-emotional-cognitive signatures of persuasion attacks, revealing that different attack types target specific cognitive dimensions. This research advances generative AI safety and cognitive security by quantifying LLM-specific vulnerabilities to persuasion attacks and delivers a framework for enhancing human cognitive resilience through structured interventions before exposure to harmful content.",
        "authors": [
            "Svitlana Volkova",
            "Will Dupree",
            "Hsien-Te Kao",
            "Peter Bautista",
            "Gabe Ganberg",
            "Jeff Beaubien",
            "Laura Cassani"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T07:49:05+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21749v1"
    },
    {
        "arxiv_id": "2511.18324v1",
        "title": "Gradient Masters at BLP-2025 Task 1: Advancing Low-Resource NLP for Bengali using Ensemble-Based Adversarial Training for Hate Speech Detection",
        "abstract": "This paper introduces the approach of \"Gradient Masters\" for BLP-2025 Task 1: \"Bangla Multitask Hate Speech Identification Shared Task\". We present an ensemble-based fine-tuning strategy for addressing subtasks 1A (hate-type classification) and 1B (target group classification) in YouTube comments. We propose a hybrid approach on a Bangla Language Model, which outperformed the baseline models and secured the 6th position in subtask 1A with a micro F1 score of 73.23% and the third position in subtask 1B with 73.28%. We conducted extensive experiments that evaluated the robustness of the model throughout the development and evaluation phases, including comparisons with other Language Model variants, to measure generalization in low-resource Bangla hate speech scenarios and data set coverage. In addition, we provide a detailed analysis of our findings, exploring misclassification patterns in the detection of hate speech.",
        "authors": [
            "Syed Mohaiminul Hoque",
            "Naimur Rahman",
            "Md Sakhawat Hossain"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T07:29:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18324v1"
    },
    {
        "arxiv_id": "2511.21748v1",
        "title": "Building Domain-Specific Small Language Models via Guided Data Generation",
        "abstract": "Large Language Models (LLMs) have shown remarkable success in supporting a wide range of knowledge-intensive tasks. In specialized domains, there is growing interest in leveraging LLMs to assist subject matter experts with domain-specific challenges. However, deploying LLMs as SaaS solutions raises data privacy concerns, while many open-source models demand significant computational resources for effective domain adaptation and deployment. A promising alternative is to develop smaller, domain-specialized LLMs, though this approach is often constrained by the lack of high-quality domain-specific training data. In this work, we address these limitations by presenting a cost-efficient and scalable training pipeline that combines guided synthetic data generation from a small seed corpus with bottom-up domain data curation. Our pipeline integrates Domain-Adaptive Pretraining (DAPT), Domain-specific Supervised Fine-tuning (DSFT), and Direct Preference Optimization (DPO) to train effective small-scale models for specialized use cases. We demonstrate this approach through DiagnosticSLM, a 3B-parameter domain-specific model tailored for fault diagnosis, root cause analysis, and repair recommendation in industrial settings. To evaluate model performance, we introduce four domain-specific benchmarks: multiple-choice questions (DiagnosticMCQ), question answering (DiagnosticQA), sentence completion (DiagnosticComp), and summarization (DiagnosticSum). DiagnosticSLM achieves up to 25% accuracy improvement over open-source models of comparable or larger size (2B-9B) on the MCQ task, while also outperforming or matching them in other tasks, demonstrating effective domain-specific reasoning and generalization capabilities.",
        "authors": [
            "Aman Kumar",
            "Ekant Muljibhai Amin",
            "Xian Yeow Lee",
            "Lasitha Vidyaratne",
            "Ahmed K. Farahat",
            "Dipanjan D. Ghosh",
            "Yuta Koreeda",
            "Chetan Gupta"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T07:19:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21748v1"
    },
    {
        "arxiv_id": "2511.18313v1",
        "title": "Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search",
        "abstract": "Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.",
        "authors": [
            "Joseph Oladokun"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T06:50:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18313v1"
    },
    {
        "arxiv_id": "2511.18306v1",
        "title": "Table Comprehension in Building Codes using Vision Language Models and Domain-Specific Fine-Tuning",
        "abstract": "Building codes contain critical information for ensuring safety, regulatory compliance, and informed decision-making in construction and engineering. Automated question answering systems over such codes enable quick and accurate access to specific regulatory clauses, improving efficiency and reducing errors. Retrieval-Augmented Generation (RAG) systems are essential for this task as they combine the precision of information retrieval with the generative capabilities of language models. However, tabular data are challenging to extract as they often involve complex layouts, merged cells, multi-row headers, and embedded semantic relationships that are not easily captured by traditional natural language processing techniques and Vision Language Models (VLMs). This paper explores and compares two methods for extracting information from tabular data in building codes using several pre-trained VLMs. First, a direct input method is used, where the image of the page is input directly into the VLMs, which are then tasked with answering questions based on the image. Second, an indirect input method is introduced, which involves converting an image of a page containing tables into the LaTeX code and then answering inquires based on the LaTeX-based input. The experiments find that the direct input method generally resulted in higher accuracy than the indirect input method. To further improve the performance, we fine-tuned each VLM using Low Rank Adaptation (LoRA) on a domain-specific tabular dataset. The fine-tuned models exhibited substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%. Our results highlight the potential of parameter-efficient fine-tuning methods to adapt powerful VLMs for understanding complex structured data in specialized fields, such as building code interpretation and regulatory compliance.",
        "authors": [
            "Mohammad Aqib",
            "Mohd Hamza",
            "Ying Hei Chui",
            "Qipei Mei"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T06:34:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18306v1"
    },
    {
        "arxiv_id": "2511.18301v1",
        "title": "\"AGI\" team at SHROOM-CAP: Data-Centric Approach to Multilingual Hallucination Detection using XLM-RoBERTa",
        "abstract": "The detection of hallucinations in multilingual scientific text generated by Large Language Models (LLMs) presents significant challenges for reliable AI systems. This paper describes our submission to the SHROOM-CAP 2025 shared task on scientific hallucination detection across 9 languages. Unlike most approaches that focus primarily on model architecture, we adopted a data-centric strategy that addressed the critical issue of training data scarcity and imbalance. We unify and balance five existing datasets to create a comprehensive training corpus of 124,821 samples (50% correct, 50% hallucinated), representing a 172x increase over the original SHROOM training data. Our approach fine-tuned XLM-RoBERTa-Large with 560 million parameters on this enhanced dataset, achieves competitive performance across all languages, including \\textbf{2nd place in Gujarati} (zero-shot language) with Factuality F1 of 0.5107, and rankings between 4th-6th place across the remaining 8 languages. Our results demonstrate that systematic data curation can significantly outperform architectural innovations alone, particularly for low-resource languages in zero-shot settings.",
        "authors": [
            "Harsh Rathva",
            "Pruthwik Mishra",
            "Shrikant Malviya"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T05:48:27+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18301v1"
    },
    {
        "arxiv_id": "2511.18259v1",
        "title": "From Archives to Decisions: Multi-Agent Pharmaceutical Co-Scientist for Traceable Drug Discovery and Reverse Translation",
        "abstract": "Pharmaceutical research and development has accumulated vast, heterogeneous archives of data. Much of this knowledge stems from discontinued programs, and reusing these archives is invaluable for reverse translation. However, in practice, such reuse is often infeasible. In this work, we introduce DiscoVerse, a multi-agent co-scientist designed to support pharmaceutical research and development. The system implements semantic retrieval, cross-document linking, and auditable synthesis on a large historical corpus from Roche. To validate our approach at real-world scale, we selected a subset of 180 molecules from the Roche research repositories, covering over 0.87 billion BPE tokens and more than four decades of research. Given that automated evaluation metrics are poorly aligned with scientific utility, we evaluate the performance of DiscoVerse using blinded expert evaluation of source-linked outputs. To our knowledge, this is the first agentic framework systematically assessed on real pharmaceutical data for reverse translation, enabled by authorized access to confidential, end-to-end drug-development archives. Our contributions include role-specialized agent designs aligned with scientist workflows; human-in-the-loop support for reverse translation; expert evaluation; and a large-scale demonstration showing promising answer accuracy and decision-making insights. In brief, across seven benchmark queries covering 180 molecules, DiscoVerse achieved near-perfect recall ($\\geq 0.99$) with moderate precision ($0.71-0.91$), while qualitative assessments of discontinuation rationale and organ-specific toxicity showed faithful, source-linked synthesis across preclinical and clinical evidence.",
        "authors": [
            "Xiaochen Zheng",
            "Alvaro Serra",
            "Ilya Schneider Chernov",
            "Maddalena Marchesi",
            "Eunice Musvasva",
            "Tatyana Y. Doktorova"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-23T03:17:26+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18259v1"
    },
    {
        "arxiv_id": "2511.18194v1",
        "title": "Agent-as-a-Graph: Knowledge Graph-Based Tool and Agent Retrieval for LLM Multi-Agent Systems",
        "abstract": "Recent advances in Large Language Model Multi-Agent Systems enable scalable orchestration and retrieval of specialized, parallelized subagents, each equipped with hundreds or thousands of Model Context Protocol (MCP) servers and tools. However, existing agent, MCP, and retrieval methods typically match queries against a single agent description, obscuring fine-grained tool capabilities of each agent, resulting in suboptimal agent selection. We introduce Agent-as-a-Graph retrieval, a knowledge graph retrieval augmented generation approach that represents both tools and their parent agents as nodes and edges in a knowledge graph. During retrieval, i) relevant agents and tool nodes are first retrieved through vector search, ii) we apply a type-specific weighted reciprocal rank fusion (wRRF) for reranking tools and agents, and iii) parent agents are traversed in the knowledge graph for the final set of agents. We evaluate Agent-as-a-Graph on the LiveMCPBenchmark, achieving 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers, and 2.4% improvements in wRRF optimizations.",
        "authors": [
            "Faheem Nizar",
            "Elias Lumer",
            "Anmol Gulati",
            "Pradeep Honaganahalli Basavaraju",
            "Vamse Kumar Subbiah"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T21:24:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18194v1"
    },
    {
        "arxiv_id": "2511.18177v1",
        "title": "Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models",
        "abstract": "Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.",
        "authors": [
            "Elias Lumer",
            "Matt Melich",
            "Olivia Zino",
            "Elena Kim",
            "Sara Dieter",
            "Pradeep Honaganahalli Basavaraju",
            "Vamse Kumar Subbiah",
            "James A. Burke",
            "Roberto Hernandez"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T20:06:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18177v1"
    },
    {
        "arxiv_id": "2511.18162v1",
        "title": "Vector Arithmetic in Concept and Token Subspaces",
        "abstract": "In order to predict the next token, LLMs must represent semantic and surface-level information about the current word. Previous work identified two types of attention heads that disentangle this information: (i) Concept induction heads, which copy word meanings, and (ii) Token induction heads, which copy literal token representations (Feucht et al., 2025). We show that these heads can be used to identify subspaces of model activations that exhibit coherent semantic structure in Llama-2-7b. Specifically, when we transform hidden states using the attention weights of concept heads, we are able to more accurately perform parallelogram arithmetic (Mikolov et al., 2013) on the resulting hidden states, e.g., showing that \"Athens\" - \"Greece\" + \"China\" = \"Beijing\". This transformation allows for much higher nearest-neighbor accuracy (80%) than direct use of raw hidden states (47%). Analogously, we show that token heads allow for transformations that reveal surface-level word information in hidden states, allowing for operations like \"coding\" - \"code\" + \"dance\" = \"dancing\".",
        "authors": [
            "Sheridan Feucht",
            "Byron Wallace",
            "David Bau"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T19:21:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18162v1"
    },
    {
        "arxiv_id": "2511.18146v1",
        "title": "GeeSanBhava: Sentiment Tagged Sinhala Music Video Comment Data Set",
        "abstract": "This study introduce GeeSanBhava, a high-quality data set of Sinhala song comments extracted from YouTube manually tagged using Russells Valence-Arousal model by three independent human annotators. The human annotators achieve a substantial inter-annotator agreement (Fleiss kappa = 84.96%). The analysis revealed distinct emotional profiles for different songs, highlighting the importance of comment based emotion mapping. The study also addressed the challenges of comparing comment-based and song-based emotions, mitigating biases inherent in user-generated content. A number of Machine learning and deep learning models were pre-trained on a related large data set of Sinhala News comments in order to report the zero-shot result of our Sinhala YouTube comment data set. An optimized Multi-Layer Perceptron model, after extensive hyperparameter tuning, achieved a ROC-AUC score of 0.887. The model is a three-layer MLP with a configuration of 256, 128, and 64 neurons. This research contributes a valuable annotated dataset and provides insights for future work in Sinhala Natural Language Processing and music emotion recognition.",
        "authors": [
            "Yomal De Mel",
            "Nisansa de Silva"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T18:15:06+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18146v1"
    },
    {
        "arxiv_id": "2511.18123v1",
        "title": "Bias Is a Subspace, Not a Coordinate: A Geometric Rethinking of Post-hoc Debiasing in Vision-Language Models",
        "abstract": "Vision-Language Models (VLMs) have become indispensable for multimodal reasoning, yet their representations often encode and amplify demographic biases, resulting in biased associations and misaligned predictions in downstream tasks. Such behavior undermines fairness and distorts the intended alignment between vision and language. Recent post-hoc approaches attempt to mitigate bias by replacing the most attribute-correlated embedding coordinates with neutral values. However, our systematic analysis reveals three critical failures of this coordinate-wise approach: feature entanglement, poor cross-dataset generalization, and incomplete bias removal. We find that bias is not localized to a few coordinates but is instead distributed across a few linear subspaces. To address these limitations, we propose $\\textbf{S}$ubspace $\\textbf{P}$rojection $\\textbf{D}$ebiasing ($\\textbf{SPD}$), a geometrically principled framework that identifies and removes the entire subspace of linearly decodable bias while reinserting a neutral mean component to preserve semantic fidelity. Extensive experiments across zero-shot classification, text-to-image retrieval, and image generation validate the effectiveness of SPD: our method achieves more robust debiasing with an average improvement of $18.5\\%$ across four fairness metrics, while maintaining minimal loss in task performance compared to the best debiasing baseline.",
        "authors": [
            "Dachuan Zhao",
            "Weiyue Li",
            "Zhenda Shen",
            "Yushu Qiu",
            "Bowen Xu",
            "Haoyu Chen",
            "Yongchao Chen"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-22T17:04:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18123v1"
    },
    {
        "arxiv_id": "2511.18103v1",
        "title": "Comparing Labeled Markov Chains: A Cantor-Kantorovich Approach",
        "abstract": "Labeled Markov Chains (or LMCs for short) are useful mathematical objects to model complex probabilistic languages. A central challenge is to compare two LMCs, for example to assess the accuracy of an abstraction or to quantify the effect of model perturbations. In this work, we study the recently introduced Cantor-Kantorovich (or CK) distance. In particular we show that the latter can be framed as a discounted sum of finite-horizon Total Variation distances, making it an instance of discounted linear distance, but arising from the natural Cantor topology. Building on the latter observation, we analyze the properties of the CK distance along three dimensions: computational complexity, continuity properties and approximation. More precisely, we show that the exact computation of the CK distance is #P-hard. We also provide an upper bound on the CK distance as a function of the approximation relation between the two LMCs, and show that a bounded CK distance implies a bounded error between probabilities of finite-horizon traces. Finally, we provide a computable approximation scheme, and show that the latter is also #P-hard. Altogether, our results provide a rigorous theoretical foundation for the CK distance and clarify its relationship with existing distances.",
        "authors": [
            "Adrien Banse",
            "Alessandro Abate",
            "Rapha\u00ebl M. Jungers"
        ],
        "category": "cs.LO",
        "submitted_date": "2025-11-22T16:02:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18103v1"
    },
    {
        "arxiv_id": "2511.18055v1",
        "title": "IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment",
        "abstract": "Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not well aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding edited results from different editing methods, and nearly 4,000 samples with corresponding Mean Opinion Scores (MOS) provided by 15 human subjects. Furthermore, we introduce IE-Critic-R1, which, benefiting from Reinforcement Learning from Verifiable Rewards (RLVR), provides more comprehensive and explainable quality assessment for text-driven image editing that aligns with human perception. Extensive experiments demonstrate IE-Critic-R1's superior subjective-alignments on the text-driven image editing task compared with previous metrics. Related data and codes are available to the public.",
        "authors": [
            "Bowen Qu",
            "Shangkun Sun",
            "Xiaoyu Liang",
            "Wei Gao"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-22T13:16:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18055v1"
    },
    {
        "arxiv_id": "2511.18054v1",
        "title": "Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets",
        "abstract": "High-quality training data is fundamental to large language model (LLM) performance, yet existing preprocessing pipelines often struggle to effectively remove noise and unstructured content from web-scale corpora. This paper presents Blu-WERP, a novel data preprocessing pipeline designed to optimize the quality of Common Crawl WARC files for LLM training. We demonstrate that Blu-WERP significantly outperforms established baselines including DCLM across multiple model scales and evaluation benchmarks. Our pipeline processes CC WARC dumps, implementing advanced filtering and quality assessment mechanisms. We conducted comprehensive evaluations using models with 150M, 400M, 530M, 750M, and 1B parameters, testing against nine standard benchmarks categorized as World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning. Results show Blu-WERP consistently achieved superior performance across all model scales. At the 1B parameter scale, Relatively Blu-WERP demonstrates a 4.0% and 9.5% aggregate improvement over DCLM and Fineweb respectively, while achieving quality-per-token efficiency gain. Categorical analysis reveals 2.4% improvement in World Knowledge & Reasoning, 6.2% improvement in Language Understanding, and 4.2% improvement in Commonsense Reasoning. These results establish Blu-WERP as a state-of-the-art preprocessing pipeline that substantially improves LLM training data quality and downstream model performance with reduced computational cost. Our findings contribute to the growing body of research on data-centric AI, demonstrating that preprocessing pipeline design significantly impacts LLM capabilities. The Blu-WERP pipeline represents a practical advancement in data quality optimization, offering researchers and practitioners an effective solution for improving LLM training efficiency and model performance.",
        "authors": [
            "Gowtham",
            "Sai Rupesh",
            "Sanjay Kumar",
            "Saravanan",
            "Venkata Chaithanya"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T13:14:07+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18054v1"
    },
    {
        "arxiv_id": "2511.18036v1",
        "title": "Paper2SysArch: Structure-Constrained System Architecture Generation from Scientific Papers",
        "abstract": "The manual creation of system architecture diagrams for scientific papers is a time-consuming and subjective process, while existing generative models lack the necessary structural control and semantic understanding for this task. A primary obstacle hindering research and development in this domain has been the profound lack of a standardized benchmark to quantitatively evaluate the automated generation of diagrams from text. To address this critical gap, we introduce a novel and comprehensive benchmark, the first of its kind, designed to catalyze progress in automated scientific visualization. It consists of 3,000 research papers paired with their corresponding high-quality ground-truth diagrams and is accompanied by a three-tiered evaluation metric assessing semantic accuracy, layout coherence, and visual quality. Furthermore, to establish a strong baseline on this new benchmark, we propose Paper2SysArch, an end-to-end system that leverages multi-agent collaboration to convert papers into structured, editable diagrams. To validate its performance on complex cases, the system was evaluated on a manually curated and more challenging subset of these papers, where it achieves a composite score of 69.0. This work's principal contribution is the establishment of a large-scale, foundational benchmark to enable reproducible research and fair comparison. Meanwhile, our proposed system serves as a viable proof-of-concept, demonstrating a promising path forward for this complex task.",
        "authors": [
            "Ziyi Guo",
            "Zhou Liu",
            "Wentao Zhang"
        ],
        "category": "cs.AI",
        "submitted_date": "2025-11-22T12:24:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18036v1"
    },
    {
        "arxiv_id": "2511.21746v1",
        "title": "DELTA: Language Diffusion-based EEG-to-Text Architecture",
        "abstract": "Electroencephalogram (EEG)-to-text remains challenging due to high-dimensional noise, subject variability, and error accumulation in autoregressive decoding. We introduce DELTA, which pairs a Residual Vector Quantization (RVQ) EEG tokenizer with a masked language diffusion model (LLaDA). RVQ discretizes continuous EEG into multi-layer tokens to reduce noise and individual differences, while LLaDA reconstructs sentences via non-sequential denoising. On ZuCo, DELTA improves semantic alignment by up to 5.37 points over autoregressive baselines, achieving BLEU-1 21.9 and ROUGE-1 F 17.2 under word-level conditions. These results enable reliable text generation from small EEG-text datasets and point toward scalable multimodal EEG-language models.",
        "authors": [
            "Mingyu Jeon",
            "Hyobin Kim"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T10:30:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21746v1"
    },
    {
        "arxiv_id": "2511.21744v1",
        "title": "A Lightweight Approach to Detection of AI-Generated Texts Using Stylometric Features",
        "abstract": "A growing number of AI-generated texts raise serious concerns. Most existing approaches to AI-generated text detection rely on fine-tuning large transformer models or building ensembles, which are computationally expensive and often provide limited generalization across domains. Existing lightweight alternatives achieved significantly lower accuracy on large datasets. We introduce NEULIF, a lightweight approach that achieves best performance in the lightweight detector class, that does not require extensive computational power and provides high detection accuracy. In our approach, a text is first decomposed into stylometric and readability features which are then used for classification by a compact Convolutional Neural Network (CNN) or Random Forest (RF). Evaluated and tested on the Kaggle AI vs. Human corpus, our models achieve 97% accuracy (~ 0.95 F1) for CNN and 95% accuracy (~ 0.94 F1) for the Random Forest, demonstrating high precision and recall, with ROC-AUC scores of 99.5% and 95%, respectively. The CNN (~ 25 MB) and Random Forest (~ 10.6 MB) models are orders of magnitude smaller than transformer-based ensembles and can be run efficiently on standard CPU devices, without sacrificing accuracy. This study also highlights the potential of such models for broader applications across languages, domains, and streaming contexts, showing that simplicity, when guided by structural insights, can rival complexity in AI-generated content detection.",
        "authors": [
            "Sergey K. Aityan",
            "William Claster",
            "Karthik Sai Emani",
            "Sohni Rais",
            "Thy Tran"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T08:08:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21744v1"
    },
    {
        "arxiv_id": "2511.17955v1",
        "title": "MTikGuard System: A Transformer-Based Multimodal System for Child-Safe Content Moderation on TikTok",
        "abstract": "With the rapid rise of short-form videos, TikTok has become one of the most influential platforms among children and teenagers, but also a source of harmful content that can affect their perception and behavior. Such content, often subtle or deceptive, challenges traditional moderation methods due to the massive volume and real-time nature of uploads. This paper presents MTikGuard, a real-time multimodal harmful content detection system for TikTok, with three key contributions: (1) an extended TikHarm dataset expanded to 4,723 labeled videos by adding diverse real-world samples, (2) a multimodal classification framework integrating visual, audio, and textual features to achieve state-of-the-art performance with 89.37% accuracy and 89.45% F1-score, and (3) a scalable streaming architecture built on Apache Kafka and Apache Spark for real-time deployment. The results demonstrate the effectiveness of combining dataset expansion, advanced multimodal fusion, and robust deployment for practical large-scale social media content moderation. The dataset is available at https://github.com/ntdat-8324/MTikGuard-System.git.",
        "authors": [
            "Dat Thanh Nguyen",
            "Nguyen Hung Lam",
            "Anh Hoang-Thi Nguyen",
            "Trong-Hop Do"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T07:41:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17955v1"
    },
    {
        "arxiv_id": "2511.17946v1",
        "title": "Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models",
        "abstract": "Hallucination in large language models (LLMs) is a fundamental challenge, particularly in open-domain question answering. Prior work attempts to detect hallucination with model-internal signals such as token-level entropy or generation consistency, while the connection between pretraining data exposure and hallucination is underexplored. Existing studies show that LLMs underperform on long-tail knowledge, i.e., the accuracy of the generated answer drops for the ground-truth entities that are rare in pretraining. However, examining whether data coverage itself can serve as a detection signal is overlooked. We propose a complementary question: Does lexical training-data coverage of the question and/or generated answer provide additional signal for hallucination detection? To investigate this, we construct scalable suffix arrays over RedPajama's 1.3-trillion-token pretraining corpus to retrieve $n$-gram statistics for both prompts and model generations. We evaluate their effectiveness for hallucination detection across three QA benchmarks. Our observations show that while occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty. These findings suggest that lexical coverage features provide a complementary signal for hallucination detection. All code and suffix-array infrastructure are provided at https://github.com/WWWonderer/ostd.",
        "authors": [
            "Shuo Zhang",
            "Fabrizio Gotti",
            "Fengran Mo",
            "Jian-Yun Nie"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T06:59:55+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17946v1"
    },
    {
        "arxiv_id": "2511.17938v1",
        "title": "SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization",
        "abstract": "Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.",
        "authors": [
            "Jianghao Wu",
            "Yasmeen George",
            "Jin Ye",
            "Yicheng Wu",
            "Daniel F. Schmidt",
            "Jianfei Cai"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T06:32:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17938v1"
    },
    {
        "arxiv_id": "2511.17923v1",
        "title": "Towards Efficient LLM-aware Heterogeneous Graph Learning",
        "abstract": "Heterogeneous graphs are widely present in real-world complex networks, where the diversity of node and relation types leads to complex and rich semantics. Efforts for modeling complex relation semantics in heterogeneous graphs are restricted by the limitations of predefined semantic dependencies and the scarcity of supervised signals. The advanced pre-training and fine-tuning paradigm leverages graph structure to provide rich self-supervised signals, but introduces semantic gaps between tasks. Large Language Models (LLMs) offer significant potential to address the semantic issues of relations and tasks in heterogeneous graphs through their strong reasoning capabilities in textual modality, but their incorporation into heterogeneous graphs is largely limited by computational complexity. Therefore, in this paper, we propose an Efficient LLM-Aware (ELLA) framework for heterogeneous graphs, addressing the above issues. To capture complex relation semantics, we propose an LLM-aware Relation Tokenizer that leverages LLM to encode multi-hop, multi-type relations. To reduce computational complexity, we further employ a Hop-level Relation Graph Transformer, which help reduces the complexity of LLM-aware relation reasoning from exponential to linear. To bridge semantic gaps between pre-training and fine-tuning tasks, we introduce the fine-grained task-aware textual Chain-of-Thought (CoT) prompts. Extensive experiments on four heterogeneous graphs show that our proposed ELLA outperforms state-of-the-art methods in the performance and efficiency. In particular, ELLA scales up to 13b-parameter LLMs and achieves up to a 4x speedup compared with existing LLM-based methods. Our code is publicly available at https://github.com/l-wd/ELLA.",
        "authors": [
            "Wenda Li",
            "Tongya Zheng",
            "Shunyu Liu",
            "Yu Wang",
            "Kaixuan Chen",
            "Hanyang Yuan",
            "Bingde Hu",
            "Zujie Ren",
            "Mingli Song",
            "Gang Chen"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T05:38:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17923v1"
    },
    {
        "arxiv_id": "2511.19470v1",
        "title": "Quantifying Modality Contributions via Disentangling Multimodal Representations",
        "abstract": "Quantifying modality contributions in multimodal models remains a challenge, as existing approaches conflate the notion of contribution itself. Prior work relies on accuracy-based approaches, interpreting performance drops after removing a modality as indicative of its influence. However, such outcome-driven metrics fail to distinguish whether a modality is inherently informative or whether its value arises only through interaction with other modalities. This distinction is particularly important in cross-attention architectures, where modalities influence each other's representations. In this work, we propose a framework based on Partial Information Decomposition (PID) that quantifies modality contributions by decomposing predictive information in internal embeddings into unique, redundant, and synergistic components. To enable scalable, inference-only analysis, we develop an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that computes layer and dataset-level contributions without retraining. This provides a principled, representation-level view of multimodal behavior, offering clearer and more interpretable insights than outcome-based metrics.",
        "authors": [
            "Padegal Amit",
            "Omkar Mahesh Kashyap",
            "Namitha Rayasam",
            "Nidhi Shekhar",
            "Surabhi Narayan"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-22T05:02:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.19470v1"
    },
    {
        "arxiv_id": "2511.17910v1",
        "title": "L2V-CoT: Cross-Modal Transfer of Chain-of-Thought Reasoning via Latent Intervention",
        "abstract": "Recently, Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs), but Vision-Language Models (VLMs) still struggle with multi-step reasoning tasks due to limited multimodal reasoning data. To bridge this gap, researchers have explored methods to transfer CoT reasoning from LLMs to VLMs. However, existing approaches either need high training costs or require architectural alignment. In this paper, we use Linear Artificial Tomography (LAT) to empirically show that LLMs and VLMs share similar low-frequency latent representations of CoT reasoning despite architectural differences. Based on this insight, we propose L2V-CoT, a novel training-free latent intervention approach that transfers CoT reasoning from LLMs to VLMs. L2V-CoT extracts and resamples low-frequency CoT representations from LLMs in the frequency domain, enabling dimension matching and latent injection into VLMs during inference to enhance reasoning capabilities. Extensive experiments demonstrate that our approach consistently outperforms training-free baselines and even surpasses supervised methods.",
        "authors": [
            "Yuliang Zhan",
            "Xinyu Tang",
            "Han Wan",
            "Jian Li",
            "Ji-Rong Wen",
            "Hao Sun"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T04:25:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17910v1"
    },
    {
        "arxiv_id": "2511.17908v1",
        "title": "Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances factual grounding in large language models (LLMs) by incorporating retrieved evidence, but LLM accuracy declines when long or noisy contexts exceed the model's effective attention span. Existing pre-generation filters rely on heuristics or uncalibrated LLM confidence scores, offering no statistical control over retained evidence. We evaluate and demonstrate context engineering through conformal prediction, a coverage-controlled filtering framework that removes irrelevant content while preserving recall of supporting evidence. Using both embedding- and LLM-based scoring functions, we test this approach on the NeuCLIR and RAGTIME collections. Conformal filtering consistently meets its target coverage, ensuring that a specified fraction of relevant snippets are retained, and reduces retained context by 2-3x relative to unfiltered retrieval. On NeuCLIR, downstream factual accuracy measured by ARGUE F1 improves under strict filtering and remains stable at moderate coverage, indicating that most discarded material is redundant or irrelevant. These results demonstrate that conformal prediction enables reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.",
        "authors": [
            "Debashish Chakraborty",
            "Eugene Yang",
            "Daniel Khashabi",
            "Dawn Lawrie",
            "Kevin Duh"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T04:17:06+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17908v1"
    },
    {
        "arxiv_id": "2511.20691v1",
        "title": "LLMs-Powered Accurate Extraction, Querying and Intelligent Management of Literature derived 2D Materials Data",
        "abstract": "Two-dimensional (2D) materials have showed widespread applications in energy storage and conversion owning to their unique physicochemical, and electronic properties. Most of the valuable information for the materials, such as their properties and preparation methods, is included in the published research papers. However, due to the dispersion of synthe",
        "authors": [
            "Lijun Shang",
            "Yadong Yu",
            "Wenqiang Kang",
            "Jian Zhou",
            "Dongyue Gao",
            "Pan Xiang",
            "Zhe Liu",
            "Mengyan Dai",
            "Zhonglu Guo",
            "Zhimei Sun"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T04:09:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.20691v1"
    },
    {
        "arxiv_id": "2511.17886v1",
        "title": "When Better Teachers Don't Make Better Students: Revisiting Knowledge Distillation for CLIP Models in VQA",
        "abstract": "Vision-language models (VLMs) have achieved remarkable success across multimodal tasks, yet their substantial computational demands hinder efficient deployment. Knowledge distillation (KD) has emerged as a powerful approach for building lightweight but competitive models, with strong evidence from both language and vision domains. However, its application to VLMs, particularly CLIP-style models, remains limited, often constrained to small-scale teachers and narrow evaluation tasks such as classification or retrieval. In this work, we present the first systematic study of distillation across a range of CLIP-style teacher models, ranging from standard baselines to large-scale state-of-the-art models. Contrary to trends observed in NLP and vision, we find that stronger teachers do not consistently yield better students; in fact, existing distillation frameworks often fail to scale, leading to degraded performance in downstream multimodal tasks such as visual question answering. Our findings challenge prevailing assumptions in KD and point toward new directions for designing parameter-efficient multimodal models.",
        "authors": [
            "Pume Tuchinda",
            "Parinthapat Pengpun",
            "Romrawin Chumpu",
            "Sarana Nutanong",
            "Peerat Limkonchotiwat"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-22T02:30:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17886v1"
    },
    {
        "arxiv_id": "2511.21743v1",
        "title": "Scaling Competence, Shrinking Reasoning: Cognitive Signatures in Language Model Learning",
        "abstract": "We analyze reasoning in language models during task-specific fine-tuning and draws parallel between reasoning tokens--intermediate steps generated while solving problem and the human working memory. Drawing from cognitive science, we align training dynamics with the Four Stages of Competence: models initially produce incorrect outputs without reasoning, then begin reasoning (but still fail), eventually reason effectively, and finally solve tasks without explicit reasoning. We find that reasoning token length expands as performance improves, peaks at the stage of conscious competence, then declines as the model internalizes the task. Notably, after training, models retain performance even when reasoning is removed--suggesting it scaffolded learning but is no longer needed. This progression offers actionable insights: reasoning token dynamics can serve as a signal for diagnosing training stage, identifying convergence, and guiding early stopping. We propose metrics to track this trajectory and argue that reasoning behavior is valuable for understanding and optimizing reasoning model training.",
        "authors": [
            "Mukul Singh",
            "Ananya Singha",
            "Arjun Radhakrishna",
            "Sumit Gulwani"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T01:58:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21743v1"
    },
    {
        "arxiv_id": "2511.17854v1",
        "title": "A superpersuasive autonomous policy debating system",
        "abstract": "The capacity for highly complex, evidence-based, and strategically adaptive persuasion remains a formidable great challenge for artificial intelligence. Previous work, like IBM Project Debater, focused on generating persuasive speeches in simplified and shortened debate formats intended for relatively lay audiences. We introduce DeepDebater, a novel autonomous system capable of participating in and winning a full, unmodified, two-team competitive policy debate. Our system employs a hierarchical architecture of specialized multi-agent workflows, where teams of LLM-powered agents collaborate and critique one another to perform discrete argumentative tasks. Each workflow utilizes iterative retrieval, synthesis, and self-correction using a massive corpus of policy debate evidence (OpenDebateEvidence) and produces complete speech transcripts, cross-examinations, and rebuttals. We introduce a live, interactive end-to-end presentation pipeline that renders debates with AI speech and animation: transcripts are surface-realized and synthesized to audio with OpenAI TTS, and then displayed as talking-head portrait videos with EchoMimic V1. Beyond fully autonomous matches (AI vs AI), DeepDebater supports hybrid human-AI operation: human debaters can intervene at any stage, and humans can optionally serve as opponents against AI in any speech, allowing AI-human and AI-AI rounds. In preliminary evaluations against human-authored cases, DeepDebater produces qualitatively superior argumentative components and consistently wins simulated rounds as adjudicated by an independent autonomous judge. Expert human debate coaches also prefer the arguments, evidence, and cases constructed by DeepDebater. We open source all code, generated speech transcripts, audio and talking head video here: https://github.com/Hellisotherpeople/DeepDebater/tree/main",
        "authors": [
            "Allen Roush",
            "Devin Gonier",
            "John Hines",
            "Judah Goldfeder",
            "Philippe Martin Wyder",
            "Sanjay Basu",
            "Ravid Shwartz Ziv"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-22T00:45:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17854v1"
    },
    {
        "arxiv_id": "2511.21742v1",
        "title": "EduMod-LLM: A Modular Approach for Designing Flexible and Transparent Educational Assistants",
        "abstract": "With the growing use of Large Language Model (LLM)-based Question-Answering (QA) systems in education, it is critical to evaluate their performance across individual pipeline components. In this work, we introduce {\\model}, a modular function-calling LLM pipeline, and present a comprehensive evaluation along three key axes: function calling strategies, retrieval methods, and generative language models. Our framework enables fine-grained analysis by isolating and assessing each component. We benchmark function-calling performance across LLMs, compare our novel structure-aware retrieval method to vector-based and LLM-scoring baselines, and evaluate various LLMs for response synthesis. This modular approach reveals specific failure modes and performance patterns, supporting the development of interpretable and effective educational QA systems. Our findings demonstrate the value of modular function calling in improving system transparency and pedagogical alignment. Website and Supplementary Material: https://chancharikmitra.github.io/EduMod-LLM-website/",
        "authors": [
            "Meenakshi Mittal",
            "Rishi Khare",
            "Mihran Miroyan",
            "Chancharik Mitra",
            "Narges Norouzi"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T23:05:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21742v1"
    },
    {
        "arxiv_id": "2511.21741v1",
        "title": "A Multiscale Geometric Method for Capturing Relational Topic Alignment",
        "abstract": "Interpretable topic modeling is essential for tracking how research interests evolve within co-author communities. In scientific corpora, where novelty is prized, identifying underrepresented niche topics is particularly important. However, contemporary models built from dense transformer embeddings tend to miss rare topics and therefore also fail to capture smooth temporal alignment. We propose a geometric method that integrates multimodal text and co-author network data, using Hellinger distances and Ward's linkage to construct a hierarchical topic dendrogram. This approach captures both local and global structure, supporting multiscale learning across semantic and temporal dimensions. Our method effectively identifies rare-topic structure and visualizes smooth topic drift over time. Experiments highlight the strength of interpretable bag-of-words models when paired with principled geometric alignment.",
        "authors": [
            "Conrad D. Hougen",
            "Karl T. Pazdernik",
            "Alfred O. Hero"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T22:45:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21741v1"
    },
    {
        "arxiv_id": "2511.17826v1",
        "title": "Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch",
        "abstract": "Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outputs when system configurations (e.g., tensor parallel (TP) size, batch size) vary, even under greedy decoding. This arises from the non-associativity of floating-point arithmetic and inconsistent reduction orders across GPUs. While prior work has addressed batch-size-related nondeterminism through batch-invariant kernels, determinism across different TP sizes remains an open problem, particularly in RL settings, where the training engine typically uses Fully Sharded Data Parallel (i.e., TP = 1) while the rollout engine relies on multi-GPU TP to maximize the inference throughput, creating a natural mismatch between the two. This precision mismatch problem may lead to suboptimal performance or even collapse for RL training. We identify and analyze the root causes of TP-induced inconsistency and propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Our key insight is to align intra- and inter-GPU reduction orders through a unified hierarchical binary tree structure. We implement these kernels in Triton and integrate them into vLLM and FSDP. Experiments confirm zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes. Also, we achieve bit-wise identical results between vLLM and FSDP in RL training pipelines with different parallel strategy. Code is available at https://github.com/nanomaoli/llm_reproducibility.",
        "authors": [
            "Ziyang Zhang",
            "Xinheng Ding",
            "Jiayi Yuan",
            "Rixin Liu",
            "Huizi Mao",
            "Jiarong Xing",
            "Zirui Liu"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-21T22:40:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17826v1"
    },
    {
        "arxiv_id": "2511.17813v1",
        "title": "Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation",
        "abstract": "Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this \"action-aware\" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.",
        "authors": [
            "Scott Merrill",
            "Shashank Srivastava"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T22:07:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17813v1"
    },
    {
        "arxiv_id": "2511.17808v2",
        "title": "PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese",
        "abstract": "Large Language Models (LLMs) exhibit significant variations in performance across linguistic and cultural contexts, underscoring the need for systematic evaluation in diverse languages. In this work, we present the most extensive evaluation of LLMs for the Portuguese language to date. Leveraging our newly introduced PoETa v2 benchmark -- a comprehensive suite of over 40 tasks in Portuguese -- we assess more than 20 models covering a broad spectrum of training scales and computational resources. Our study reveals how computational investment and language-specific adaptation impact performance in Portuguese, while also analyzing performance gaps in comparison to equivalent tasks in English. Through this benchmark and analysis, PoETa v2 lays the groundwork for future research on Portuguese language modeling and evaluation. The benchmark is available at https://github.com/PoETaV2/PoETaV2.",
        "authors": [
            "Thales Sales Almeida",
            "Ramon Pires",
            "Hugo Abonizio",
            "Rodrigo Nogueira",
            "H\u00e9lio Pedrini"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T22:01:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17808v2"
    },
    {
        "arxiv_id": "2511.21740v1",
        "title": "Decoding inner speech with an end-to-end brain-to-text neural interface",
        "abstract": "Speech brain-computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end Brain-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text '24 and '25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization.",
        "authors": [
            "Yizi Zhang",
            "Linyang He",
            "Chaofei Fan",
            "Tingkai Liu",
            "Han Yu",
            "Trung Le",
            "Jingyuan Li",
            "Scott Linderman",
            "Lea Duncker",
            "Francis R Willett",
            "Nima Mesgarani",
            "Liam Paninski"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T21:25:54+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21740v1"
    },
    {
        "arxiv_id": "2511.17746v1",
        "title": "Computational frame analysis revisited: On LLMs for studying news coverage",
        "abstract": "Computational approaches have previously shown various promises and pitfalls when it comes to the reliable identification of media frames. Generative LLMs like GPT and Claude are increasingly being used as content analytical tools, but how effective are they for frame analysis? We address this question by systematically evaluating them against their computational predecessors: bag-of-words models and encoder-only transformers; and traditional manual coding procedures. Our analysis rests on a novel gold standard dataset that we inductively and iteratively developed through the study, investigating six months of news coverage of the US Mpox epidemic of 2022. While we discover some potential applications for generative LLMs, we demonstrate that they were consistently outperformed by manual coders, and in some instances, by smaller language models. Some form of human validation was always necessary to determine appropriate model choice. Additionally, by examining how the suitability of various approaches depended on the nature of different tasks that were part of our frame analytical workflow, we provide insights as to how researchers may leverage the complementarity of these approaches to use them in tandem. We conclude by endorsing a methodologically pluralistic approach and put forth a roadmap for computational frame analysis for researchers going forward.",
        "authors": [
            "Sharaj Kunjar",
            "Alyssa Hasegawa Smith",
            "Tyler R Mckenzie",
            "Rushali Mohbe",
            "Samuel V Scarpino",
            "Brooke Foucault Welles"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T19:52:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17746v1"
    },
    {
        "arxiv_id": "2511.17473v1",
        "title": "Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards",
        "abstract": "Test-time scaling has been shown to substantially improve large language models' (LLMs) mathematical reasoning. However, for a large portion of mathematical corpora, especially theorem proving, RLVR's scalability is limited: intermediate reasoning is crucial, while final answers are difficult to directly and reliably verify. Meanwhile, token-level SFT often degenerates into rote memorization rather than inducing longer chains of thought. Inspired by BERT's self-supervised tasks, we propose MR-RLVR (Masked-and-Reordered RLVR), which constructs process-level self-supervised rewards via \"masked-then-fill\" and \"step reordering\" to extract learnable signals from intermediate reasoning. Our training pipeline comprises two stages: we first perform self-supervised training on sampled mathematical calculation and proof data; we then conduct RLVR fine-tuning on mathematical calculation datasets where only outcomes are verifiable. We implement MR-RLVR on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B, and evaluate on AIME24, AIME25, AMC23, and MATH500. Under a fixed sampling and decoding budget, MR-RLVR achieves average relative gains over the original RLVR of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results indicate that incorporating process-aware self-supervised signals can effectively enhance RLVR's scalability and performance in only outcome-verifiable settings.",
        "authors": [
            "Zhen Wang",
            "Zhifeng Gao",
            "Guolin Ke"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T18:23:04+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17473v1"
    },
    {
        "arxiv_id": "2511.17450v1",
        "title": "Planning with Sketch-Guided Verification for Physics-Aware Video Generation",
        "abstract": "Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.",
        "authors": [
            "Yidong Huang",
            "Zun Wang",
            "Han Lin",
            "Dong-Ki Kim",
            "Shayegan Omidshafiei",
            "Jaehong Yoon",
            "Yue Zhang",
            "Mohit Bansal"
        ],
        "category": "cs.CV",
        "submitted_date": "2025-11-21T17:48:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17450v1"
    },
    {
        "arxiv_id": "2511.17432v1",
        "title": "SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation",
        "abstract": "Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.",
        "authors": [
            "Shrikant Kendre",
            "Austin Xu",
            "Honglu Zhou",
            "Michael Ryoo",
            "Shafiq Joty",
            "Juan Carlos Niebles"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T17:30:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17432v1"
    },
    {
        "arxiv_id": "2511.17405v2",
        "title": "Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT",
        "abstract": "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.",
        "authors": [
            "Yesheng Liu",
            "Hao Li",
            "Haiyu Xu",
            "Baoqi Pei",
            "Jiahao Wang",
            "Mingxuan Zhao",
            "Jingshu Zheng",
            "Zheqi He",
            "JG Yao",
            "Bowen Qin",
            "Xi Yang",
            "Jiajun Zhang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T17:06:37+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17405v2"
    },
    {
        "arxiv_id": "2511.17402v1",
        "title": "PUCP-Metrix: A Comprehensive Open-Source Repository of Linguistic Metrics for Spanish",
        "abstract": "Linguistic features remain essential for interpretability and tasks involving style, structure, and readability, but existing Spanish tools offer limited coverage. We present PUCP-Metrix, an open-source repository of 182 linguistic metrics spanning lexical diversity, syntactic and semantic complexity, cohesion, psycholinguistics, and readability. PUCP-Metrix enables fine-grained, interpretable text analysis. We evaluate its usefulness on Automated Readability Assessment and Machine-Generated Text Detection, showing competitive performance compared to an existing repository and strong neural baselines. PUCP-Metrix offers a comprehensive, extensible resource for Spanish, supporting diverse NLP applications.",
        "authors": [
            "Javier Alonso Villegas Luis",
            "Marco Antonio Sobrevilla Cabezudo"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T17:03:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17402v1"
    },
    {
        "arxiv_id": "2511.17388v1",
        "title": "Selective Rotary Position Embedding",
        "abstract": "Position information is essential for language modeling. In softmax transformers, Rotary Position Embeddings (\\textit{RoPE}) encode positions through \\textit{fixed-angle} rotations, while in linear transformers, order is handled via input-dependent (selective) gating that decays past key-value associations. Selectivity has generally been shown to improve language-related tasks. Inspired by this, we introduce \\textit{Selective RoPE}, an \\textit{input-dependent} rotary embedding mechanism, that generalizes \\textit{RoPE}, and enables rotation in \\textit{arbitrary angles} for both linear and softmax transformers. We show that softmax attention already performs a hidden form of these rotations on query-key pairs, uncovering an implicit positional structure. We further show that in state-space models and gated linear transformers, the real part manages forgetting while the imaginary part encodes positions through rotations. We validate our method by equipping gated transformers with \\textit{Selective RoPE}, demonstrating that its input-dependent rotations improve performance in language modeling and on difficult sequence tasks like copying, state tracking, and retrieval.",
        "authors": [
            "Sajad Movahedi",
            "Timur Carstensen",
            "Arshia Afzal",
            "Frank Hutter",
            "Antonio Orvieto",
            "Volkan Cevher"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T16:50:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17388v1"
    },
    {
        "arxiv_id": "2511.17358v1",
        "title": "Don't Learn, Ground: A Case for Natural Language Inference with Visual Grounding",
        "abstract": "We propose a zero-shot method for Natural Language Inference (NLI) that leverages multimodal representations by grounding language in visual contexts. Our approach generates visual representations of premises using text-to-image models and performs inference by comparing these representations with textual hypotheses. We evaluate two inference techniques: cosine similarity and visual question answering. Our method achieves high accuracy without task-specific fine-tuning, demonstrating robustness against textual biases and surface heuristics. Additionally, we design a controlled adversarial dataset to validate the robustness of our approach. Our findings suggest that leveraging visual modality as a meaning representation provides a promising direction for robust natural language understanding.",
        "authors": [
            "Daniil Ignatev",
            "Ayman Santeer",
            "Albert Gatt",
            "Denis Paperno"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T16:23:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17358v1"
    },
    {
        "arxiv_id": "2511.17693v1",
        "title": "DeepCoT: Deep Continual Transformers for Real-Time Inference on Data Streams",
        "abstract": "Transformer-based models have dramatically increased their size and parameter count to tackle increasingly complex tasks. At the same time, there is a growing demand for low-latency inference on resource-constrained devices that achieves high performance. In particular, stream data inference is typically performed over a sliding temporal window, leading to highly redundant computations. The recent Continual Transformers have addressed this issue, but they can only be effectively used in shallow models, which limits their scope and generalization power. In this paper, we propose the Deep Continual Transformer (DeepCoT), a redundancy-free encoder-only model that can be applied over existing deep encoder architectures with minimal changes. In our experiments over audio, video, and text streams, we show that DeepCoTs retain comparative performance to their non-continual baselines while offering a linear computational cost for all Transformer layers, which reduces up to two orders of magnitude in the running time compared to previous efficient models.",
        "authors": [
            "Gin\u00e9s Carreto Pic\u00f3n",
            "Peng Yuan Zhou",
            "Qi Zhang",
            "Alexandros Iosifidis"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-21T16:15:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17693v1"
    },
    {
        "arxiv_id": "2511.17337v1",
        "title": "A new kid on the block: Distributional semantics predicts the word-specific tone signatures of monosyllabic words in conversational Taiwan Mandarin",
        "abstract": "We present a corpus-based investigation of how the pitch contours of monosyllabic words are realized in spontaneous conversational Mandarin, focusing on the effects of words' meanings. We used the generalized additive model to decompose a given observed pitch contour into a set of component pitch contours that are tied to different control variables and semantic predictors. Even when variables such as word duration, gender, speaker identity, tonal context, vowel height, and utterance position are controlled for, the effect of word remains a strong predictor of tonal realization. We present evidence that this effect of word is a semantic effect: word sense is shown to be a better predictor than word, and heterographic homophones are shown to have different pitch contours. The strongest evidence for the importance of semantics is that the pitch contours of individual word tokens can be predicted from their contextualized embeddings with an accuracy that substantially exceeds a permutation baseline. For phonetics, distributional semantics is a new kid on the block. Although our findings challenge standard theories of Mandarin tone, they fit well within the theoretical framework of the Discriminative Lexicon Model.",
        "authors": [
            "Xiaoyun Jin",
            "Mirjam Ernestus",
            "R. Harald Baayen"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T15:56:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17337v1"
    },
    {
        "arxiv_id": "2511.17335v1",
        "title": "Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM",
        "abstract": "Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.",
        "authors": [
            "Chiori Hori",
            "Yoshiki Masuyama",
            "Siddarth Jain",
            "Radu Corcodel",
            "Devesh Jha",
            "Diego Romeres",
            "Jonathan Le Roux"
        ],
        "category": "cs.RO",
        "submitted_date": "2025-11-21T15:55:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17335v1"
    },
    {
        "arxiv_id": "2511.17323v1",
        "title": "MusicAIR: A Multimodal AI Music Generation Framework Powered by an Algorithm-Driven Core",
        "abstract": "Recent advances in generative AI have made music generation a prominent research focus. However, many neural-based models rely on large datasets, raising concerns about copyright infringement and high-performance costs. In contrast, we propose MusicAIR, an innovative multimodal AI music generation framework powered by a novel algorithm-driven symbolic music core, effectively mitigating copyright infringement risks. The music core algorithms connect critical lyrical and rhythmic information to automatically derive musical features, creating a complete, coherent melodic score solely from the lyrics. The MusicAIR framework facilitates music generation from lyrics, text, and images. The generated score adheres to established principles of music theory, lyrical structure, and rhythmic conventions. We developed Generate AI Music (GenAIM), a web tool using MusicAIR for lyric-to-song, text-to-music, and image-to-music generation. In our experiments, we evaluated AI-generated music scores produced by the system using both standard music metrics and innovative analysis that compares these compositions with original works. The system achieves an average key confidence of 85%, outperforming human composers at 79%, and aligns closely with established music theory standards, demonstrating its ability to generate diverse, human-like compositions. As a co-pilot tool, GenAIM can serve as a reliable music composition assistant and a possible educational composition tutor while simultaneously lowering the entry barrier for all aspiring musicians, which is innovative and significantly contributes to AI for music generation.",
        "authors": [
            "Callie C. Liao",
            "Duoduo Liao",
            "Ellie L. Zhang"
        ],
        "category": "cs.SD",
        "submitted_date": "2025-11-21T15:43:27+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17323v1"
    },
    {
        "arxiv_id": "2511.17315v1",
        "title": "Humanlike Multi-user Agent (HUMA): Designing a Deceptively Human AI Facilitator for Group Chats",
        "abstract": "Conversational agents built on large language models (LLMs) are becoming increasingly prevalent, yet most systems are designed for one-on-one, turn-based exchanges rather than natural, asynchronous group chats. As AI assistants become widespread throughout digital platforms, from virtual assistants to customer service, developing natural and humanlike interaction patterns seems crucial for maintaining user trust and engagement. We present the Humanlike Multi-user Agent (HUMA), an LLM-based facilitator that participates in multi-party conversations using human-like strategies and timing. HUMA extends prior multi-user chatbot work with an event-driven architecture that handles messages, replies, reactions and introduces realistic response-time simulation. HUMA comprises three components-Router, Action Agent, and Reflection-which together adapt LLMs to group conversation dynamics.   We evaluate HUMA in a controlled study with 97 participants in four-person role-play chats, comparing AI and human community managers (CMs). Participants classified CMs as human at near-chance rates in both conditions, indicating they could not reliably distinguish HUMA agents from humans. Subjective experience was comparable across conditions: community-manager effectiveness, social presence, and engagement/satisfaction differed only modestly with small effect sizes. Our results suggest that, in natural group chat settings, an AI facilitator can match human quality while remaining difficult to identify as nonhuman.",
        "authors": [
            "Mateusz Jacniacki",
            "Mart\u00ed Carmona Serrat"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T15:34:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17315v1"
    },
    {
        "arxiv_id": "2511.17301v1",
        "title": "Large Language Models for Sentiment Analysis to Detect Social Challenges: A Use Case with South African Languages",
        "abstract": "Sentiment analysis can aid in understanding people's opinions and emotions on social issues. In multilingual communities sentiment analysis systems can be used to quickly identify social challenges in social media posts, enabling government departments to detect and address these issues more precisely and effectively. Recently, large-language models (LLMs) have become available to the wide public and initial analyses have shown that they exhibit magnificent zero-shot sentiment analysis abilities in English. However, there is no work that has investigated to leverage LLMs for sentiment analysis on social media posts in South African languages and detect social challenges. Consequently, in this work, we analyse the zero-shot performance of the state-of-the-art LLMs GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2 to investigate the sentiment polarities of the 10 most emerging topics in English, Sepedi and Setswana social media posts that fall within the jurisdictional areas of 10 South African government departments. Our results demonstrate that there are big differences between the various LLMs, topics, and languages. In addition, we show that a fusion of the outcomes of different LLMs provides large gains in sentiment classification performance with sentiment classification errors below 1%. Consequently, it is now feasible to provide systems that generate reliable information about sentiment analysis to detect social challenges and draw conclusions about possible needs for actions on specific topics and within different language groups.",
        "authors": [
            "Koena Ronny Mabokela",
            "Tim Schlippe",
            "Matthias W\u00f6lfel"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T15:14:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17301v1"
    },
    {
        "arxiv_id": "2511.17290v1",
        "title": "Estonian WinoGrande Dataset: Comparative Analysis of LLM Performance on Human and Machine Translation",
        "abstract": "In this paper, we present a localized and culturally adapted Estonian translation of the test set from the widely used commonsense reasoning benchmark, WinoGrande. We detail the translation and adaptation process carried out by translation specialists and evaluate the performance of both proprietary and open source models on the human translated benchmark. Additionally, we explore the feasibility of achieving high-quality machine translation by incorporating insights from the manual translation process into the design of a detailed prompt. This prompt is specifically tailored to address both the linguistic characteristics of Estonian and the unique translation challenges posed by the WinoGrande dataset. Our findings show that model performance on the human translated Estonian dataset is slightly lower than on the original English test set, while performance on machine-translated data is notably worse. Additionally, our experiments indicate that prompt engineering offers limited improvement in translation quality or model accuracy, and highlight the importance of involving language specialists in dataset translation and adaptation to ensure reliable and interpretable evaluations of language competency and reasoning in large language models.",
        "authors": [
            "Marii Ojastu",
            "Hele-Andra Kuulmets",
            "Aleksei Dorkin",
            "Marika Borovikova",
            "Dage S\u00e4rg",
            "Kairit Sirts"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T15:01:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17290v1"
    },
    {
        "arxiv_id": "2511.21737v1",
        "title": "Polarity-Aware Probing for Quantifying Latent Alignment in Language Models",
        "abstract": "Advances in unsupervised probes such as Contrast-Consistent Search (CCS), which reveal latent beliefs without relying on token outputs, raise the question of whether these methods can reliably assess model alignment. We investigate this by examining the sensitivity of CCS to harmful vs. safe statements and by introducing Polarity-Aware CCS (PA-CCS), a method for evaluating whether a model's internal representations remain consistent under polarity inversion. We propose two alignment-oriented metrics, Polar-Consistency and the Contradiction Index, to quantify the semantic robustness of a model's latent knowledge. To validate PA-CCS, we curate two main datasets and one control dataset containing matched harmful-safe sentence pairs constructed using different methodologies (concurrent and antagonistic statements). We apply PA-CCS to 16 language models. Our results show that PA-CCS identifies both architectural and layer-specific differences in the encoding of latent harmful knowledge. Notably, replacing the negation token with a meaningless marker degrades PA-CCS scores for models with well-aligned internal representations, while models lacking robust internal calibration do not exhibit this degradation. Our findings highlight the potential of unsupervised probing for alignment evaluation and emphasize the need to incorporate structural robustness checks into interpretability benchmarks. Code and datasets are available at: https://github.com/SadSabrina/polarity-probing. WARNING: This paper contains potentially sensitive, harmful, and offensive content.",
        "authors": [
            "Sabrina Sadiekh",
            "Elena Ericheva",
            "Chirag Agarwal"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T14:58:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21737v1"
    },
    {
        "arxiv_id": "2511.17256v1",
        "title": "Cross-cultural value alignment frameworks for responsible AI governance: Evidence from China-West comparative analysis",
        "abstract": "As Large Language Models (LLMs) increasingly influence high-stakes decision-making across global contexts, ensuring their alignment with diverse cultural values has become a critical governance challenge. This study presents a Multi-Layered Auditing Platform for Responsible AI that systematically evaluates cross-cultural value alignment in China-origin and Western-origin LLMs through four integrated methodologies: Ethical Dilemma Corpus for assessing temporal stability, Diversity-Enhanced Framework (DEF) for quantifying cultural fidelity, First-Token Probability Alignment for distributional accuracy, and Multi-stAge Reasoning frameworK (MARK) for interpretable decision-making. Our comparative analysis of 20+ leading models, such as Qwen, GPT-4o, Claude, LLaMA, and DeepSeek, reveals universal challenges-fundamental instability in value systems, systematic under-representation of younger demographics, and non-linear relationships between model scale and alignment quality-alongside divergent regional development trajectories. While China-origin models increasingly emphasize multilingual data integration for context-specific optimization, Western models demonstrate greater architectural experimentation but persistent U.S.-centric biases. Neither paradigm achieves robust cross-cultural generalization. We establish that Mistral-series architectures significantly outperform LLaMA3-series in cross-cultural alignment, and that Full-Parameter Fine-Tuning on diverse datasets surpasses Reinforcement Learning from Human Feedback in preserving cultural variation...",
        "authors": [
            "Haijiang Liu",
            "Jinguang Gu",
            "Xun Wu",
            "Daniel Hershcovich",
            "Qiaoling Xiao"
        ],
        "category": "cs.CY",
        "submitted_date": "2025-11-21T14:02:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17256v1"
    },
    {
        "arxiv_id": "2511.17241v1",
        "title": "Social-Media Based Personas Challenge: Hybrid Prediction of Common and Rare User Actions on Bluesky",
        "abstract": "Understanding and predicting user behavior on social media platforms is crucial for content recommendation and platform design. While existing approaches focus primarily on common actions like retweeting and liking, the prediction of rare but significant behaviors remains largely unexplored. This paper presents a hybrid methodology for social media user behavior prediction that addresses both frequent and infrequent actions across a diverse action vocabulary. We evaluate our approach on a large-scale Bluesky dataset containing 6.4 million conversation threads spanning 12 distinct user actions across 25 persona clusters. Our methodology combines four complementary approaches: (i) a lookup database system based on historical response patterns; (ii) persona-specific LightGBM models with engineered temporal and semantic features for common actions; (iii) a specialized hybrid neural architecture fusing textual and temporal representations for rare action classification; and (iv) generation of text replies. Our persona-specific models achieve an average macro F1-score of 0.64 for common action prediction, while our rare action classifier achieves 0.56 macro F1-score across 10 rare actions. These results demonstrate that effective social media behavior prediction requires tailored modeling strategies recognizing fundamental differences between action types. Our approach achieved first place in the SocialSim: Social-Media Based Personas challenge organized at the Social Simulation with LLMs workshop at COLM 2025.",
        "authors": [
            "Benjamin White",
            "Anastasia Shimorina"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T13:40:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17241v1"
    },
    {
        "arxiv_id": "2511.17238v1",
        "title": "Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables",
        "abstract": "The impressive performance of VLMs is largely measured on benchmarks that fail to capture the complexities of real-world scenarios. Existing datasets for tabular QA, such as WikiTableQuestions and FinQA, are overwhelmingly monolingual (English) and present tables in a digitally perfect, clean format. This creates a significant gap between research and practice. To address this, we present \\textbf{MirageTVQA}, a new benchmark designed to evaluate VLMs on these exact dimensions. Featuring nearly 60,000 QA pairs across 24 languages, MirageTVQA challenges models with tables that are not only multilingual but also visually imperfect, incorporating realistic noise to mimic scanned documents. Our evaluation of the leading VLMs reveals two primary failure points: a severe degradation in performance (over 35\\% drop for the best models) when faced with visual noise and a consistent English-first bias where reasoning abilities fail to transfer to other languages. MirageTVQA provides a benchmark for measuring and driving progress towards more robust VLM models for table reasoning. The dataset and the code are available at: https://github.com/anshulsc/MirageTVQA.",
        "authors": [
            "Anshul Singh",
            "Rohan Chaudhary",
            "Gagneet Singh",
            "Abhay Kumary"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T13:32:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17238v1"
    },
    {
        "arxiv_id": "2511.17220v2",
        "title": "PARROT: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs",
        "abstract": "This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low \"follow rates\" ($\\leq 11\\%$, GPT-5: 4\\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\\%, Qwen 2.5-1.5B: 94\\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of \"resistance to overfitting pressure\" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.",
        "authors": [
            "Yusuf \u00c7elebi",
            "\u00d6zay Ezerceli",
            "Mahmoud El Hussieni"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T13:01:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17220v2"
    },
    {
        "arxiv_id": "2511.17208v1",
        "title": "A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents",
        "abstract": "LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem.",
        "authors": [
            "Sizhe Zhou"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T12:41:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17208v1"
    },
    {
        "arxiv_id": "2511.21736v1",
        "title": "R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization",
        "abstract": "The rapid progress of Large Language Models (LLMs) has brought substantial computational and memory demands, spurring the adoption of low-bit quantization. While 8-bit and 4-bit formats have become prevalent, extending quantization to 2 bits remains challenging due to severe accuracy degradation. To address this, we propose Residual Refinement Quantization (R2Q)-a novel 2-bit quantization framework that decomposes the process into two sequential 1-bit sub-quantizations, forming an adaptive quantization lattice. Extensive evaluations on Llama, OPT, and Qwen across diverse benchmarks-covering question answering, commonsense reasoning, and language modeling-demonstrate that R2Q consistently outperforms existing 2-bit quantization methods in both fine-grained and coarse-grained settings. By refining quantization through a residual learning mechanism, R2Q enhances performance, improves training stability, and accelerates convergence under extreme compression. Furthermore, its modular design enables seamless integration with existing quantization-aware training (QAT) frameworks.",
        "authors": [
            "Jiayi Chen",
            "Jieqi Shi",
            "Jing Huo",
            "Chen Wu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T12:39:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21736v1"
    },
    {
        "arxiv_id": "2511.17205v1",
        "title": "E$^3$-Pruner: Towards Efficient, Economical, and Effective Layer Pruning for Large Language Models",
        "abstract": "With the increasing size of large language models, layer pruning has gained increased attention as a hardware-friendly approach for model compression. However, existing layer pruning methods struggle to simultaneously address key practical deployment challenges, including performance degradation, high training costs, and limited acceleration. To overcome these limitations, we propose \\name, a task-\\underline{E}ffective, training-\\underline{E}conomical and inference-\\underline{E}fficient layer pruning framework. \\namespace introduces two key innovations: (1) a differentiable mask optimization method using a Gumbel-TopK sampler, enabling efficient and precise pruning mask search; and (2) an entropy-aware adaptive knowledge distillation strategy that enhances task performance. Extensive experiments over diverse model architectures and benchmarks demonstrate the superiority of our method over state-of-the-art approaches. Notably, \\namespace achieves 96\\% accuracy, a mere 0.8\\% drop from the original model (96.8\\%) on MATH-500 when pruning 25\\% layers of Qwen3-32B, outperforming existing SOTA (95\\%), with a 1.33$\\times$ inference speedup by consuming merely 0.5B tokens (0.5\\% of the post-training data volume).",
        "authors": [
            "Tao Yuan",
            "Haoli Bai",
            "Yinfei Pan",
            "Xuyang Cao",
            "Tianyu Zhang",
            "Lu Hou",
            "Ting Hu",
            "Xianzhi Yu"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T12:32:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17205v1"
    },
    {
        "arxiv_id": "2511.17190v1",
        "title": "AutoLink: Autonomous Schema Exploration and Expansion for Scalable Schema Linking in Text-to-SQL at Scale",
        "abstract": "For industrial-scale text-to-SQL, supplying the entire database schema to Large Language Models (LLMs) is impractical due to context window limits and irrelevant noise. Schema linking, which filters the schema to a relevant subset, is therefore critical. However, existing methods incur prohibitive costs, struggle to trade off recall and noise, and scale poorly to large databases. We present \\textbf{AutoLink}, an autonomous agent framework that reformulates schema linking as an iterative, agent-driven process. Guided by an LLM, AutoLink dynamically explores and expands the linked schema subset, progressively identifying necessary schema components without inputting the full database schema. Our experiments demonstrate AutoLink's superior performance, achieving state-of-the-art strict schema linking recall of \\textbf{97.4\\%} on Bird-Dev and \\textbf{91.2\\%} on Spider-2.0-Lite, with competitive execution accuracy, i.e., \\textbf{68.7\\%} EX on Bird-Dev (better than CHESS) and \\textbf{34.9\\%} EX on Spider-2.0-Lite (ranking 2nd on the official leaderboard). Crucially, AutoLink exhibits \\textbf{exceptional scalability}, \\textbf{maintaining high recall}, \\textbf{efficient token consumption}, and \\textbf{robust execution accuracy} on large schemas (e.g., over 3,000 columns) where existing methods severely degrade-making it a highly scalable, high-recall schema-linking solution for industrial text-to-SQL systems.",
        "authors": [
            "Ziyang Wang",
            "Yuanlei Zheng",
            "Zhenbiao Cao",
            "Xiaojin Zhang",
            "Zhongyu Wei",
            "Pei Fu",
            "Zhenbo Luo",
            "Wei Chen",
            "Xiang Bai"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T12:12:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17190v1"
    },
    {
        "arxiv_id": "2511.17184v1",
        "title": "Attention-Guided Feature Fusion (AGFF) Model for Integrating Statistical and Semantic Features in News Text Classification",
        "abstract": "News text classification is a crucial task in natural language processing, essential for organizing and filtering the massive volume of digital content. Traditional methods typically rely on statistical features like term frequencies or TF-IDF values, which are effective at capturing word-level importance but often fail to reflect contextual meaning. In contrast, modern deep learning approaches utilize semantic features to understand word usage within context, yet they may overlook simple, high-impact statistical indicators. This paper introduces an Attention-Guided Feature Fusion (AGFF) model that combines statistical and semantic features in a unified framework. The model applies an attention-based mechanism to dynamically determine the relative importance of each feature type, enabling more informed classification decisions. Through evaluation on benchmark news datasets, the AGFF model demonstrates superior performance compared to both traditional statistical models and purely semantic deep learning models. The results confirm that strategic integration of diverse feature types can significantly enhance classification accuracy. Additionally, ablation studies validate the contribution of each component in the fusion process. The findings highlight the model's ability to balance and exploit the complementary strengths of statistical and semantic representations, making it a practical and effective solution for real-world news classification tasks.",
        "authors": [
            "Mohammad Zare"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T12:05:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17184v1"
    },
    {
        "arxiv_id": "2511.17170v1",
        "title": "Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models",
        "abstract": "Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as \"I don't know\", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions.",
        "authors": [
            "Vy Nguyen",
            "Ziqi Xu",
            "Jeffrey Chan",
            "Estrid He",
            "Feng Xia",
            "Xiuzhen Zhang"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T11:42:49+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17170v1"
    },
    {
        "arxiv_id": "2511.17161v1",
        "title": "The PLLuM Instruction Corpus",
        "abstract": "This paper describes the instruction dataset used to fine-tune a set of transformer-based large language models (LLMs) developed in the PLLuM (Polish Large Language Model) project. We present a functional typology of the organic, converted, and synthetic instructions used in PLLuM and share some observations about the implications of using human-authored versus synthetic instruction datasets in the linguistic adaptation of base LLMs. Additionally, we release the first representative subset of the PLLuM instruction corpus (PLLuMIC), which we believe to be useful in guiding and planning the development of similar datasets for other LLMs.",
        "authors": [
            "Piotr P\u0119zik",
            "Filip \u017barnecki",
            "Konrad Kaczy\u0144ski",
            "Anna Cichosz",
            "Zuzanna Deckert",
            "Monika Garnys",
            "Izabela Grabarczyk",
            "Wojciech Janowski",
            "Sylwia Karasi\u0144ska",
            "Aleksandra Kujawiak",
            "Piotr Misztela",
            "Maria Szyma\u0144ska",
            "Karolina Walkusz",
            "Igor Siek",
            "Maciej Chrab\u0105szcz",
            "Anna Ko\u0142os",
            "Agnieszka Karli\u0144ska",
            "Karolina Seweryn",
            "Aleksandra Krasnod\u0119bska",
            "Paula Betscher",
            "Zofia Cie\u015bli\u0144ska",
            "Katarzyna Kowol",
            "Artur Wilczek",
            "Maciej Trzci\u0144ski",
            "Katarzyna Dziewulska",
            "Roman Roszko",
            "Tomasz Berna\u015b",
            "Jurgita Vai\u010denonien\u0117",
            "Danuta Roszko",
            "Pawe\u0142 Levchuk",
            "Pawe\u0142 Kowalski",
            "Irena Prawdzic-Jankowska",
            "Marek Koz\u0142owski",
            "S\u0142awomir Dadas",
            "Rafa\u0142 Po\u015bwiata",
            "Alina Wr\u00f3blewska",
            "Katarzyna Krasnowska-Kiera\u015b",
            "Maciej Ogrodniczuk",
            "Micha\u0142 Rudolf",
            "Piotr Rybak",
            "Karolina Saputa",
            "Joanna Wo\u0142oszyn",
            "Marcin Oleksy",
            "Bart\u0142omiej Koptyra",
            "Teddy Ferdinan",
            "Stanis\u0142aw Wo\u017aniak",
            "Maciej Piasecki",
            "Pawe\u0142 Walkowiak",
            "Konrad Wojtasik",
            "Arkadiusz Janz",
            "Przemys\u0142aw Kazienko",
            "Julia Moska",
            "Jan Koco\u0144"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T11:28:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17161v1"
    },
    {
        "arxiv_id": "2511.17153v1",
        "title": "LangMark: A Multilingual Dataset for Automatic Post-Editing",
        "abstract": "Automatic post-editing (APE) aims to correct errors in machine-translated text, enhancing translation quality, while reducing the need for human intervention. Despite advances in neural machine translation (NMT), the development of effective APE systems has been hindered by the lack of large-scale multilingual datasets specifically tailored to NMT outputs. To address this gap, we present and release LangMark, a new human-annotated multilingual APE dataset for English translation to seven languages: Brazilian Portuguese, French, German, Italian, Japanese, Russian, and Spanish. The dataset has 206,983 triplets, with each triplet consisting of a source segment, its NMT output, and a human post-edited translation. Annotated by expert human linguists, our dataset offers both linguistic diversity and scale. Leveraging this dataset, we empirically show that Large Language Models (LLMs) with few-shot prompting can effectively perform APE, improving upon leading commercial and even proprietary machine translation systems. We believe that this new resource will facilitate the future development and evaluation of APE systems.",
        "authors": [
            "Diego Velazquez",
            "Mikaela Grace",
            "Konstantinos Karageorgos",
            "Lawrence Carin",
            "Aaron Schliem",
            "Dimitrios Zaikis",
            "Roger Wechsler"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T11:18:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17153v1"
    },
    {
        "arxiv_id": "2511.21735v1",
        "title": "Closing the Performance Gap Between AI and Radiologists in Chest X-Ray Reporting",
        "abstract": "AI-assisted report generation offers the opportunity to reduce radiologists' workload stemming from expanded screening guidelines, complex cases and workforce shortages, while maintaining diagnostic accuracy. In addition to describing pathological findings in chest X-ray reports, interpreting lines and tubes (L&T) is demanding and repetitive for radiologists, especially with high patient volumes. We introduce MAIRA-X, a clinically evaluated multimodal AI model for longitudinal chest X-ray (CXR) report generation, that encompasses both clinical findings and L&T reporting. Developed using a large-scale, multi-site, longitudinal dataset of 3.1 million studies (comprising 6 million images from 806k patients) from Mayo Clinic, MAIRA-X was evaluated on three holdout datasets and the public MIMIC-CXR dataset, where it significantly improved AI-generated reports over the state of the art on lexical quality, clinical correctness, and L&T-related elements. A novel L&T-specific metrics framework was developed to assess accuracy in reporting attributes such as type, longitudinal change and placement. A first-of-its-kind retrospective user evaluation study was conducted with nine radiologists of varying experience, who blindly reviewed 600 studies from distinct subjects. The user study found comparable rates of critical errors (3.0% for original vs. 4.6% for AI-generated reports) and a similar rate of acceptable sentences (97.8% for original vs. 97.4% for AI-generated reports), marking a significant improvement over prior user studies with larger gaps and higher error rates. Our results suggest that MAIRA-X can effectively assist radiologists, particularly in high-volume clinical settings.",
        "authors": [
            "Harshita Sharma",
            "Maxwell C. Reynolds",
            "Valentina Salvatelli",
            "Anne-Marie G. Sykes",
            "Kelly K. Horst",
            "Anton Schwaighofer",
            "Maximilian Ilse",
            "Olesya Melnichenko",
            "Sam Bond-Taylor",
            "Fernando P\u00e9rez-Garc\u00eda",
            "Vamshi K. Mugu",
            "Alex Chan",
            "Ceylan Colak",
            "Shelby A. Swartz",
            "Motassem B. Nashawaty",
            "Austin J. Gonzalez",
            "Heather A. Ouellette",
            "Selnur B. Erdal",
            "Beth A. Schueler",
            "Maria T. Wetscherek",
            "Noel Codella",
            "Mohit Jain",
            "Shruthi Bannur",
            "Kenza Bouzid",
            "Daniel C. Castro",
            "Stephanie Hyland",
            "Panos Korfiatis",
            "Ashish Khandelwal",
            "Javier Alvarez-Valle"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T10:53:26+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21735v1"
    },
    {
        "arxiv_id": "2511.17129v1",
        "title": "Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation",
        "abstract": "Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data.",
        "authors": [
            "Yeqin Zhang",
            "Yizheng Zhao",
            "Chen Hu",
            "Binxing Jiao",
            "Daxin Jiang",
            "Ruihang Miao",
            "Cam-Tu Nguyen"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T10:45:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17129v1"
    },
    {
        "arxiv_id": "2511.17127v1",
        "title": "Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design",
        "abstract": "We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs with Pollara interconnect. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts on Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.",
        "authors": [
            "Quentin Anthony",
            "Yury Tokpanov",
            "Skyler Szot",
            "Srivatsan Rajagopal",
            "Praneeth Medepalli",
            "Rishi Iyer",
            "Vasu Shyam",
            "Anna Golubeva",
            "Ansh Chaurasia",
            "Xiao Yang",
            "Tomas Figliolia",
            "Robert Washbourne",
            "Drew Thorstensen",
            "Amartey Pearson",
            "Zack Grossbart",
            "Jason van Patten",
            "Emad Barsoum",
            "Zhenyu Gu",
            "Yao Fu",
            "Beren Millidge"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T10:44:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17127v1"
    },
    {
        "arxiv_id": "2511.17100v1",
        "title": "Geometric-Disentangelment Unlearning",
        "abstract": "Machine unlearning, the removal of a training subset's influence from a deployed model, is critical for privacy preservation and model reliability, yet gradient ascent on forget samples often harms retained knowledge. Existing approaches face a persistent tradeoff between effective forgetting and preservation on the retain set. While previous methods provide useful heuristics, they often lack a formal analysis on how exactly forgetting updates harm retained knowledge, and whether the side effects can be removed with theoretical guarantees. To explore a theoretically sound and simple solution, we start from the first principle on how performance on the retain set is actually affected: a first-order analysis of the local change of the retain loss under small parameter updates during model training. We start from a crisp equivalence: the retain loss is unchanged to first order iff the update direction is orthogonal to the subspace spanned by retain gradients (\"retain-invariant\"). This identifies the entangled component as the tangential part of forget update within the retain-gradient subspace, and characterizes disentanglement as orthogonality. Guided by this, we propose the Geometric-disentanglement Unlearning (GU) that decomposes any candidate forget gradient update into tangential and normal components to retain space and executes only the normal component. Under a standard trust-region budget, the projected direction aligned with the raw forget gradient is optimal among all first-order retain-invariant moves, and we also derive the optimal projected direction for joint forget-retain updating objectives. Our method is plug-and-play and can be attached to existing gradient-based unlearning procedures to mitigate side effects. GU achieves consistent improvement on various methods across three benchmarks TOFU, MUSE, and WMDP.",
        "authors": [
            "Duo Zhou",
            "Yuji Zhang",
            "Tianxin Wei",
            "Ruizhong Qiu",
            "Ke Yang",
            "Xiao Lin",
            "Cheng Qian",
            "Jingrui He",
            "Hanghang Tong",
            "Heng Ji",
            "Huan Zhang"
        ],
        "category": "cs.LG",
        "submitted_date": "2025-11-21T09:58:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.17100v1"
    },
    {
        "arxiv_id": "2511.21734v1",
        "title": "Asking LLMs to Verify First is Almost Free Lunch",
        "abstract": "To enhance the reasoning capabilities of Large Language Models (LLMs) without high costs of training, nor extensive test-time sampling, we introduce Verification-First (VF), a strategy that prompts models to verify a provided candidate answer, even a trivial or random one, before generating a solution. This approach triggers a \"reverse reasoning\" process that is cognitively easier and complementary to standard forward Chain-of-Thought (CoT), effectively invoking the model's critical thinking to reduce logical errors. We further generalize the VF strategy to Iter-VF, a sequential test-time scaling (TTS) method that iteratively cycles the verification-generation process using the model's previous answer. Extensive experiments across various benchmarks (from mathematical reasoning to coding and agentic tasks) and various LLMs (from open-source 1B to cutting-edge commercial ones) confirm that VF with random answer consistently outperforms standard CoT with minimal computational overhead, and Iter-VF outperforms existing TTS strategies.",
        "authors": [
            "Shiguang Wu",
            "Quanming Yao"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T09:55:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21734v1"
    },
    {
        "arxiv_id": "2511.21733v1",
        "title": "RoSA: Enhancing Parameter-Efficient Fine-Tuning via RoPE-aware Selective Adaptation in Large Language Models",
        "abstract": "Fine-tuning large language models is essential for task-specific adaptation, yet it remains computationally prohibitive. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a solution, but current approaches typically ignore the distinct roles of model components and the heterogeneous importance across layers, thereby limiting adaptation efficiency. Motivated by the observation that Rotary Position Embeddings (RoPE) induce critical activations in the low-frequency dimensions of attention states, we propose RoPE-aware Selective Adaptation (RoSA), a novel PEFT framework that allocates trainable parameters in a more targeted and effective manner. RoSA comprises a RoPE-aware Attention Enhancement (RoAE) module, which selectively enhances the low-frequency components of RoPE-influenced attention states, and a Dynamic Layer Selection (DLS) strategy that adaptively identifies and updates the most critical layers based on LayerNorm gradient norms. By combining dimension-wise enhancement with layer-wise adaptation, RoSA achieves more targeted and efficient fine-tuning. Extensive experiments on fifteen commonsense and arithmetic benchmarks demonstrate that RoSA outperforms existing mainstream PEFT methods under comparable trainable parameters. The code is available to ease reproducibility at https://github.com/Applied-Machine-Learning-Lab/RoSA.",
        "authors": [
            "Dayan Pan",
            "Jingyuan Wang",
            "Yilong Zhou",
            "Jiawei Cheng",
            "Pengyue Jia",
            "Xiangyu Zhao"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T09:55:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21733v1"
    },
    {
        "arxiv_id": "2511.21732v1",
        "title": "HUMORCHAIN: Theory-Guided Multi-Stage Reasoning for Interpretable Multimodal Humor Generation",
        "abstract": "Humor, as both a creative human activity and a social binding mechanism, has long posed a major challenge for AI generation. Although producing humor requires complex cognitive reasoning and social understanding, theories of humor suggest that it follows learnable patterns and structures, making it theoretically possible for generative models to acquire them implicitly. In recent years, multimodal humor has become a prevalent form of online communication, especially among Gen Z, highlighting the need for AI systems capable of integrating visual understanding with humorous language generation. However, existing data-driven approaches lack explicit modeling or theoretical grounding of humor, often producing literal descriptions that fail to capture its underlying cognitive mechanisms, resulting in the generated image descriptions that are fluent but lack genuine humor or cognitive depth. To address this limitation, we propose HUMORCHAIN (HUmor-guided Multi-step Orchestrated Reasoning Chain for Image Captioning), a theory-guided multi-stage reasoning framework. It integrates visual semantic parsing, humor- and psychology-based reasoning, and a fine-tuned discriminator for humor evaluation, forming an interpretable and controllable cognitive reasoning chain. To the best of our knowledge, this is the first work to explicitly embed cognitive structures from humor theories into multimodal humor generation, enabling a structured reasoning process from visual understanding to humor creation. Experiments on Meme-Image-No-Text, Oogiri-GO, and OxfordTVG-HIC datasets show that HUMORCHAIN outperforms state-of-the-art baselines in human humor preference, Elo/BT scores, and semantic diversity, demonstrating that theory-driven structured reasoning enables large language models to generate humor aligned with human perception.",
        "authors": [
            "Jiajun Zhang",
            "Shijia Luo",
            "Ruikang Zhang",
            "Qi Su"
        ],
        "category": "cs.CL",
        "submitted_date": "2025-11-21T09:52:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.21732v1"
    }
]